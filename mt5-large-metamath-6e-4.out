[2024-11-12 10:02:53,608] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
[2024-11-12 10:02:55,161] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-11-12 10:02:55,162] [INFO] [runner.py:568:main] cmd = /root/work/huangxin/envs/hs/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=9901 --enable_each_rank_log=None /root/work/huangxin/nanda/ImplicitTransBridge-master/train.py --deepspeed /root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json --dataset_name /root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json --preprocessing_num_workers 64 --llm_path /root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct --slm_path_a /root/work/huangxin/nanda/models/google/mt5-large --slm_path_b /root/work/huangxin/nanda/models/google/mt5-large --stage 0 --output_dir /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5large-qwe2.5-math7b-metamath --do_train --max_seq_length 1024 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --learning_rate 6e-4 --num_train_epochs 1 --save_only_model --logging_steps 10 --save_steps 2000 --seed 42 --overwrite_output_dir --bf16
[2024-11-12 10:02:56,553] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[2024-11-12 10:02:57,922] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-11-12 10:02:57,922] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-11-12 10:02:57,922] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-11-12 10:02:57,922] [INFO] [launch.py:164:main] dist_world_size=8
[2024-11-12 10:02:57,922] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-11-12 10:02:57,930] [INFO] [launch.py:256:main] process 245 spawned with command: ['/root/work/huangxin/envs/hs/bin/python', '-u', '/root/work/huangxin/nanda/ImplicitTransBridge-master/train.py', '--local_rank=0', '--deepspeed', '/root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json', '--dataset_name', '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json', '--preprocessing_num_workers', '64', '--llm_path', '/root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct', '--slm_path_a', '/root/work/huangxin/nanda/models/google/mt5-large', '--slm_path_b', '/root/work/huangxin/nanda/models/google/mt5-large', '--stage', '0', '--output_dir', '/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5large-qwe2.5-math7b-metamath', '--do_train', '--max_seq_length', '1024', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '6e-4', '--num_train_epochs', '1', '--save_only_model', '--logging_steps', '10', '--save_steps', '2000', '--seed', '42', '--overwrite_output_dir', '--bf16']
[2024-11-12 10:02:57,938] [INFO] [launch.py:256:main] process 246 spawned with command: ['/root/work/huangxin/envs/hs/bin/python', '-u', '/root/work/huangxin/nanda/ImplicitTransBridge-master/train.py', '--local_rank=1', '--deepspeed', '/root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json', '--dataset_name', '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json', '--preprocessing_num_workers', '64', '--llm_path', '/root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct', '--slm_path_a', '/root/work/huangxin/nanda/models/google/mt5-large', '--slm_path_b', '/root/work/huangxin/nanda/models/google/mt5-large', '--stage', '0', '--output_dir', '/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5large-qwe2.5-math7b-metamath', '--do_train', '--max_seq_length', '1024', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '6e-4', '--num_train_epochs', '1', '--save_only_model', '--logging_steps', '10', '--save_steps', '2000', '--seed', '42', '--overwrite_output_dir', '--bf16']
[2024-11-12 10:02:57,944] [INFO] [launch.py:256:main] process 247 spawned with command: ['/root/work/huangxin/envs/hs/bin/python', '-u', '/root/work/huangxin/nanda/ImplicitTransBridge-master/train.py', '--local_rank=2', '--deepspeed', '/root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json', '--dataset_name', '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json', '--preprocessing_num_workers', '64', '--llm_path', '/root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct', '--slm_path_a', '/root/work/huangxin/nanda/models/google/mt5-large', '--slm_path_b', '/root/work/huangxin/nanda/models/google/mt5-large', '--stage', '0', '--output_dir', '/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5large-qwe2.5-math7b-metamath', '--do_train', '--max_seq_length', '1024', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '6e-4', '--num_train_epochs', '1', '--save_only_model', '--logging_steps', '10', '--save_steps', '2000', '--seed', '42', '--overwrite_output_dir', '--bf16']
[2024-11-12 10:02:57,951] [INFO] [launch.py:256:main] process 248 spawned with command: ['/root/work/huangxin/envs/hs/bin/python', '-u', '/root/work/huangxin/nanda/ImplicitTransBridge-master/train.py', '--local_rank=3', '--deepspeed', '/root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json', '--dataset_name', '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json', '--preprocessing_num_workers', '64', '--llm_path', '/root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct', '--slm_path_a', '/root/work/huangxin/nanda/models/google/mt5-large', '--slm_path_b', '/root/work/huangxin/nanda/models/google/mt5-large', '--stage', '0', '--output_dir', '/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5large-qwe2.5-math7b-metamath', '--do_train', '--max_seq_length', '1024', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '6e-4', '--num_train_epochs', '1', '--save_only_model', '--logging_steps', '10', '--save_steps', '2000', '--seed', '42', '--overwrite_output_dir', '--bf16']
[2024-11-12 10:02:57,958] [INFO] [launch.py:256:main] process 249 spawned with command: ['/root/work/huangxin/envs/hs/bin/python', '-u', '/root/work/huangxin/nanda/ImplicitTransBridge-master/train.py', '--local_rank=4', '--deepspeed', '/root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json', '--dataset_name', '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json', '--preprocessing_num_workers', '64', '--llm_path', '/root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct', '--slm_path_a', '/root/work/huangxin/nanda/models/google/mt5-large', '--slm_path_b', '/root/work/huangxin/nanda/models/google/mt5-large', '--stage', '0', '--output_dir', '/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5large-qwe2.5-math7b-metamath', '--do_train', '--max_seq_length', '1024', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '6e-4', '--num_train_epochs', '1', '--save_only_model', '--logging_steps', '10', '--save_steps', '2000', '--seed', '42', '--overwrite_output_dir', '--bf16']
[2024-11-12 10:02:57,963] [INFO] [launch.py:256:main] process 250 spawned with command: ['/root/work/huangxin/envs/hs/bin/python', '-u', '/root/work/huangxin/nanda/ImplicitTransBridge-master/train.py', '--local_rank=5', '--deepspeed', '/root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json', '--dataset_name', '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json', '--preprocessing_num_workers', '64', '--llm_path', '/root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct', '--slm_path_a', '/root/work/huangxin/nanda/models/google/mt5-large', '--slm_path_b', '/root/work/huangxin/nanda/models/google/mt5-large', '--stage', '0', '--output_dir', '/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5large-qwe2.5-math7b-metamath', '--do_train', '--max_seq_length', '1024', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '6e-4', '--num_train_epochs', '1', '--save_only_model', '--logging_steps', '10', '--save_steps', '2000', '--seed', '42', '--overwrite_output_dir', '--bf16']
[2024-11-12 10:02:57,968] [INFO] [launch.py:256:main] process 251 spawned with command: ['/root/work/huangxin/envs/hs/bin/python', '-u', '/root/work/huangxin/nanda/ImplicitTransBridge-master/train.py', '--local_rank=6', '--deepspeed', '/root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json', '--dataset_name', '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json', '--preprocessing_num_workers', '64', '--llm_path', '/root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct', '--slm_path_a', '/root/work/huangxin/nanda/models/google/mt5-large', '--slm_path_b', '/root/work/huangxin/nanda/models/google/mt5-large', '--stage', '0', '--output_dir', '/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5large-qwe2.5-math7b-metamath', '--do_train', '--max_seq_length', '1024', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '6e-4', '--num_train_epochs', '1', '--save_only_model', '--logging_steps', '10', '--save_steps', '2000', '--seed', '42', '--overwrite_output_dir', '--bf16']
[2024-11-12 10:02:57,973] [INFO] [launch.py:256:main] process 252 spawned with command: ['/root/work/huangxin/envs/hs/bin/python', '-u', '/root/work/huangxin/nanda/ImplicitTransBridge-master/train.py', '--local_rank=7', '--deepspeed', '/root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json', '--dataset_name', '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json', '--preprocessing_num_workers', '64', '--llm_path', '/root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct', '--slm_path_a', '/root/work/huangxin/nanda/models/google/mt5-large', '--slm_path_b', '/root/work/huangxin/nanda/models/google/mt5-large', '--stage', '0', '--output_dir', '/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5large-qwe2.5-math7b-metamath', '--do_train', '--max_seq_length', '1024', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '6e-4', '--num_train_epochs', '1', '--save_only_model', '--logging_steps', '10', '--save_steps', '2000', '--seed', '42', '--overwrite_output_dir', '--bf16']
[2024-11-12 10:03:02,087] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-12 10:03:02,093] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-12 10:03:02,095] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-12 10:03:02,096] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-12 10:03:02,097] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-12 10:03:02,098] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-12 10:03:02,098] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-12 10:03:02,102] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[2024-11-12 10:03:02,695] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-12 10:03:02,696] [INFO] [comm.py:637:init_distributed] cdb=None
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[2024-11-12 10:03:02,701] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-12 10:03:02,710] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-12 10:03:02,710] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-11-12 10:03:02,714] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-12 10:03:02,721] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-12 10:03:02,749] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-12 10:03:02,797] [INFO] [comm.py:637:init_distributed] cdb=None
11/12/2024 10:03:02 - WARNING - __main__ - Process rank: 7, device: cuda:7, n_gpu: 1, distributed training: True, 16-bits training: False
11/12/2024 10:03:02 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
[WARNING|logging.py:328] 2024-11-12 10:03:03,501 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[WARNING|logging.py:328] 2024-11-12 10:03:03,504 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
/root/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
11/12/2024 10:03:04 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
11/12/2024 10:03:04 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=no,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0006,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5large-qwe2.5-math7b-metamath/runs/Nov12_10-03-01_dt-3f438f9c1ce34127b03dfa1fd050d2d6-master-e5d6642230f0-0,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5large-qwe2.5-math7b-metamath,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5large-qwe2.5-math7b-metamath,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=2000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|tokenization_utils_base.py:2267] 2024-11-12 10:03:04,024 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2267] 2024-11-12 10:03:04,024 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2267] 2024-11-12 10:03:04,024 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2267] 2024-11-12 10:03:04,024 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2267] 2024-11-12 10:03:04,024 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2267] 2024-11-12 10:03:04,024 >> loading file tokenizer_config.json
11/12/2024 10:03:04 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1, distributed training: True, 16-bits training: False
11/12/2024 10:03:04 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, 16-bits training: False
11/12/2024 10:03:04 - WARNING - __main__ - Process rank: 6, device: cuda:6, n_gpu: 1, distributed training: True, 16-bits training: False
11/12/2024 10:03:04 - WARNING - __main__ - Process rank: 5, device: cuda:5, n_gpu: 1, distributed training: True, 16-bits training: False
11/12/2024 10:03:04 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, 16-bits training: False
[INFO|tokenization_utils_base.py:2513] 2024-11-12 10:03:04,196 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:731] 2024-11-12 10:03:04,198 >> loading configuration file /root/work/huangxin/nanda/models/google/mt5-large/config.json
[INFO|configuration_utils.py:800] 2024-11-12 10:03:04,199 >> Model config MT5Config {
  "_name_or_path": "/root/work/huangxin/nanda/models/google/mt5-large",
  "architectures": [
    "MT5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 2816,
  "d_kv": 64,
  "d_model": 1024,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "mt5",
  "num_decoder_layers": 24,
  "num_heads": 16,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "tokenizer_class": "T5Tokenizer",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 250112
}

[INFO|tokenization_utils_base.py:2267] 2024-11-12 10:03:04,200 >> loading file spiece.model
[INFO|tokenization_utils_base.py:2267] 2024-11-12 10:03:04,200 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2267] 2024-11-12 10:03:04,200 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2267] 2024-11-12 10:03:04,200 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2267] 2024-11-12 10:03:04,200 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:731] 2024-11-12 10:03:04,200 >> loading configuration file /root/work/huangxin/nanda/models/google/mt5-large/config.json
[INFO|configuration_utils.py:800] 2024-11-12 10:03:04,201 >> Model config MT5Config {
  "_name_or_path": "/root/work/huangxin/nanda/models/google/mt5-large",
  "architectures": [
    "MT5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 2816,
  "d_kv": 64,
  "d_model": 1024,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "mt5",
  "num_decoder_layers": 24,
  "num_heads": 16,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "tokenizer_class": "T5Tokenizer",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 250112
}

[WARNING|logging.py:328] 2024-11-12 10:03:04,538 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
/root/.local/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
[WARNING|logging.py:328] 2024-11-12 10:03:04,673 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[WARNING|logging.py:328] 2024-11-12 10:03:04,684 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[WARNING|logging.py:328] 2024-11-12 10:03:04,694 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[WARNING|logging.py:328] 2024-11-12 10:03:04,696 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[WARNING|logging.py:328] 2024-11-12 10:03:04,700 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
/root/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]/root/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][INFO|configuration_utils.py:731] 2024-11-12 10:03:05,613 >> loading configuration file /root/work/huangxin/nanda/models/google/mt5-large/config.json
[INFO|configuration_utils.py:800] 2024-11-12 10:03:05,614 >> Model config MT5Config {
  "_name_or_path": "/root/work/huangxin/nanda/models/google/mt5-large",
  "architectures": [
    "MT5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 2816,
  "d_kv": 64,
  "d_model": 1024,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "mt5",
  "num_decoder_layers": 24,
  "num_heads": 16,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "tokenizer_class": "T5Tokenizer",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 250112
}

/root/.local/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
[INFO|configuration_utils.py:731] 2024-11-12 10:03:06,290 >> loading configuration file /root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct/config.json
[INFO|configuration_utils.py:800] 2024-11-12 10:03:06,290 >> Model config Qwen2Config {
  "_name_or_path": "/root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 4096,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_theta": 10000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|modeling_utils.py:3675] 2024-11-12 10:03:06,315 >> loading weights file /root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct/model.safetensors.index.json
[INFO|configuration_utils.py:1038] 2024-11-12 10:03:06,347 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.21s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.88s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:16,  5.64s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.83s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.89s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.86s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.98s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.92s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:12<00:12,  6.08s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.33s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.76s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.75s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.58s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.77s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:06,  6.21s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:16<00:05,  5.89s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:17<00:06,  6.12s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:06,  6.57s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:06,  6.31s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:06,  6.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:17<00:06,  6.13s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:17<00:06,  6.15s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:24<00:00,  5.98s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:24<00:00,  6.02s/it]
[INFO|modeling_utils.py:4507] 2024-11-12 10:03:30,487 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4515] 2024-11-12 10:03:30,487 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:991] 2024-11-12 10:03:30,491 >> loading configuration file /root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct/generation_config.json
[INFO|configuration_utils.py:1038] 2024-11-12 10:03:30,491 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643
}

[INFO|configuration_utils.py:731] 2024-11-12 10:03:30,494 >> loading configuration file /root/work/huangxin/nanda/models/google/mt5-large/config.json
[INFO|configuration_utils.py:800] 2024-11-12 10:03:30,495 >> Model config MT5Config {
  "_name_or_path": "/root/work/huangxin/nanda/models/google/mt5-large",
  "architectures": [
    "MT5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 2816,
  "d_kv": 64,
  "d_model": 1024,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "mt5",
  "num_decoder_layers": 24,
  "num_heads": 16,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "tokenizer_class": "T5Tokenizer",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 250112
}

[INFO|modeling_utils.py:3675] 2024-11-12 10:03:30,534 >> loading weights file /root/work/huangxin/nanda/models/google/mt5-large/pytorch_model.bin
Loading checkpoint shards: 100%|██████████| 4/4 [00:24<00:00,  6.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:24<00:00,  6.09s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:23<00:00,  6.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:23<00:00,  5.93s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:23<00:00,  6.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:23<00:00,  5.92s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:23<00:00,  6.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:23<00:00,  5.93s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:25<00:00,  6.30s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:25<00:00,  6.43s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:25<00:00,  6.32s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:25<00:00,  6.45s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.91s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.68s/it]
11/12/2024 10:03:34 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
11/12/2024 10:03:34 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
11/12/2024 10:03:34 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
11/12/2024 10:03:34 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
11/12/2024 10:03:34 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
11/12/2024 10:03:34 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
11/12/2024 10:03:34 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
11/12/2024 10:03:34 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
[INFO|modeling_utils.py:4497] 2024-11-12 10:03:34,568 >> Some weights of the model checkpoint at /root/work/huangxin/nanda/models/google/mt5-large were not used when initializing MT5Model: ['lm_head.weight']
- This IS expected if you are initializing MT5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MT5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:4515] 2024-11-12 10:03:34,569 >> All the weights of MT5Model were initialized from the model checkpoint at /root/work/huangxin/nanda/models/google/mt5-large.
If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5Model for predictions without further training.
11/12/2024 10:03:34 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
11/12/2024 10:03:34 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
11/12/2024 10:03:34 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
11/12/2024 10:03:34 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
11/12/2024 10:03:34 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
11/12/2024 10:03:34 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
11/12/2024 10:03:34 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
11/12/2024 10:03:34 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
11/12/2024 10:03:37 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
11/12/2024 10:03:37 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
11/12/2024 10:03:37 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
11/12/2024 10:03:37 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
11/12/2024 10:03:37 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
11/12/2024 10:03:37 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
11/12/2024 10:03:37 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
11/12/2024 10:03:37 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
11/12/2024 10:03:37 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
11/12/2024 10:03:37 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
11/12/2024 10:03:37 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
11/12/2024 10:03:37 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
11/12/2024 10:03:37 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
11/12/2024 10:03:37 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
11/12/2024 10:03:37 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
11/12/2024 10:03:37 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
Using custom data configuration default-5110b371feb00e53
11/12/2024 10:03:37 - INFO - datasets.builder - Using custom data configuration default-5110b371feb00e53
Loading Dataset Infos from /root/.local/lib/python3.9/site-packages/datasets/packaged_modules/json
11/12/2024 10:03:37 - INFO - datasets.info - Loading Dataset Infos from /root/.local/lib/python3.9/site-packages/datasets/packaged_modules/json
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 395000 examples [00:09, 39965.41 examples/s]Generating train split: 395000 examples [00:09, 39927.37 examples/s]
Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5110b371feb00e53/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
11/12/2024 10:03:47 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5110b371feb00e53/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5110b371feb00e53/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
11/12/2024 10:03:47 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5110b371feb00e53/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
Map:   0%|          | 0/395000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-5110b371feb00e53/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d9c32650a4c419a6.arrow
11/12/2024 10:03:47 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-5110b371feb00e53/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d9c32650a4c419a6.arrow
Map:   0%|          | 1000/395000 [00:00<00:56, 6979.78 examples/s]Map:   1%|          | 2000/395000 [00:00<00:50, 7786.23 examples/s]Map:   1%|          | 3000/395000 [00:00<00:48, 8082.95 examples/s]Map:   1%|          | 4000/395000 [00:00<00:47, 8301.79 examples/s]Map:   1%|▏         | 5000/395000 [00:00<00:45, 8493.81 examples/s]Map:   2%|▏         | 6000/395000 [00:00<00:45, 8561.51 examples/s]Map:   2%|▏         | 7000/395000 [00:00<00:44, 8647.29 examples/s]Map:   2%|▏         | 8000/395000 [00:00<00:44, 8665.84 examples/s]Map:   2%|▏         | 9000/395000 [00:01<00:44, 8588.49 examples/s]Map:   3%|▎         | 10000/395000 [00:01<00:44, 8668.63 examples/s]Map:   3%|▎         | 11000/395000 [00:01<00:44, 8685.62 examples/s]Map:   3%|▎         | 12000/395000 [00:01<00:43, 8778.64 examples/s]Map:   3%|▎         | 13000/395000 [00:01<00:43, 8849.06 examples/s]Map:   4%|▎         | 14000/395000 [00:01<00:42, 8921.40 examples/s]Map:   4%|▍         | 15000/395000 [00:01<00:42, 8933.55 examples/s]Map:   4%|▍         | 16000/395000 [00:01<00:42, 8900.26 examples/s]Map:   4%|▍         | 17000/395000 [00:01<00:42, 8958.54 examples/s]Map:   5%|▍         | 18000/395000 [00:02<00:41, 8998.98 examples/s]Map:   5%|▍         | 19000/395000 [00:02<00:41, 9006.52 examples/s]Map:   5%|▌         | 20000/395000 [00:02<00:41, 9082.32 examples/s]Map:   5%|▌         | 21000/395000 [00:02<00:41, 9112.30 examples/s]Map:   6%|▌         | 22000/395000 [00:02<00:40, 9150.14 examples/s]Map:   6%|▌         | 23000/395000 [00:02<00:40, 9179.65 examples/s]Map:   6%|▌         | 24000/395000 [00:02<00:57, 6414.27 examples/s]Map:   6%|▋         | 25000/395000 [00:02<00:52, 6984.55 examples/s]Map:   7%|▋         | 26000/395000 [00:03<00:49, 7497.03 examples/s]Map:   7%|▋         | 27000/395000 [00:03<00:46, 7963.87 examples/s]Map:   7%|▋         | 28000/395000 [00:03<00:44, 8339.59 examples/s]Map:   7%|▋         | 29000/395000 [00:03<00:42, 8649.02 examples/s]Map:   8%|▊         | 30000/395000 [00:03<00:41, 8845.90 examples/s]Map:   8%|▊         | 31000/395000 [00:03<00:40, 8903.19 examples/s]Map:   8%|▊         | 32000/395000 [00:03<00:40, 8993.42 examples/s]Map:   8%|▊         | 33000/395000 [00:03<00:39, 9093.75 examples/s]Map:   9%|▊         | 34000/395000 [00:03<00:39, 9114.33 examples/s]Map:   9%|▉         | 35000/395000 [00:04<00:39, 9189.31 examples/s]Map:   9%|▉         | 36000/395000 [00:04<00:38, 9226.71 examples/s]Map:   9%|▉         | 37000/395000 [00:04<00:38, 9246.16 examples/s]Map:  10%|▉         | 38000/395000 [00:04<00:38, 9355.30 examples/s]Map:  10%|▉         | 39000/395000 [00:04<00:38, 9345.38 examples/s]Map:  10%|█         | 40000/395000 [00:04<00:38, 9326.28 examples/s]Map:  10%|█         | 41000/395000 [00:04<00:38, 9303.30 examples/s]Map:  11%|█         | 42000/395000 [00:04<00:37, 9334.11 examples/s]Map:  11%|█         | 43000/395000 [00:04<00:37, 9363.22 examples/s]Map:  11%|█         | 44000/395000 [00:05<00:37, 9393.31 examples/s]Map:  11%|█▏        | 45000/395000 [00:05<00:37, 9382.30 examples/s]Map:  12%|█▏        | 46000/395000 [00:05<00:37, 9327.29 examples/s]Map:  12%|█▏        | 47000/395000 [00:05<00:37, 9258.17 examples/s]Map:  12%|█▏        | 48000/395000 [00:05<00:37, 9305.72 examples/s]Map:  12%|█▏        | 49000/395000 [00:05<00:37, 9319.86 examples/s]Map:  13%|█▎        | 50000/395000 [00:05<00:37, 9245.04 examples/s]Map:  13%|█▎        | 51000/395000 [00:05<00:37, 9211.93 examples/s]Map:  13%|█▎        | 52000/395000 [00:06<00:49, 6981.87 examples/s]Map:  13%|█▎        | 53000/395000 [00:06<00:45, 7574.19 examples/s]Map:  14%|█▎        | 54000/395000 [00:06<00:42, 8057.12 examples/s]Map:  14%|█▍        | 55000/395000 [00:06<00:40, 8359.62 examples/s]Map:  14%|█▍        | 56000/395000 [00:06<00:39, 8560.30 examples/s]Map:  14%|█▍        | 57000/395000 [00:06<00:38, 8802.54 examples/s]Map:  15%|█▍        | 58000/395000 [00:06<00:37, 8932.67 examples/s]Map:  15%|█▍        | 59000/395000 [00:06<00:37, 8996.87 examples/s]Map:  15%|█▌        | 60000/395000 [00:06<00:36, 9089.52 examples/s]Map:  15%|█▌        | 61000/395000 [00:06<00:36, 9157.34 examples/s]Map:  16%|█▌        | 62000/395000 [00:07<00:35, 9351.18 examples/s]Map:  16%|█▌        | 63000/395000 [00:07<00:35, 9356.37 examples/s]Map:  16%|█▌        | 64000/395000 [00:07<00:35, 9341.33 examples/s]Map:  16%|█▋        | 65000/395000 [00:07<00:35, 9294.46 examples/s]Map:  17%|█▋        | 66000/395000 [00:07<00:35, 9254.60 examples/s]Map:  17%|█▋        | 67000/395000 [00:07<00:35, 9270.30 examples/s]Map:  17%|█▋        | 68000/395000 [00:07<00:35, 9289.70 examples/s]Map:  17%|█▋        | 69000/395000 [00:07<00:34, 9315.45 examples/s]Map:  18%|█▊        | 70000/395000 [00:07<00:34, 9426.35 examples/s]Map:  18%|█▊        | 71000/395000 [00:08<00:34, 9443.03 examples/s]Map:  18%|█▊        | 72000/395000 [00:08<00:33, 9542.48 examples/s]Map:  18%|█▊        | 73000/395000 [00:08<00:34, 9456.65 examples/s]Map:  19%|█▊        | 74000/395000 [00:08<00:33, 9447.54 examples/s]Map:  19%|█▉        | 75000/395000 [00:08<00:34, 9267.26 examples/s]Map:  19%|█▉        | 76000/395000 [00:08<00:34, 9250.53 examples/s]Map:  19%|█▉        | 77000/395000 [00:08<00:34, 9310.73 examples/s]Map:  20%|█▉        | 78000/395000 [00:08<00:33, 9377.37 examples/s]Map:  20%|██        | 79000/395000 [00:09<00:49, 6405.60 examples/s]Map:  20%|██        | 80000/395000 [00:09<00:44, 7097.61 examples/s]Map:  21%|██        | 81000/395000 [00:09<00:40, 7667.60 examples/s]Map:  21%|██        | 82000/395000 [00:09<00:39, 7998.39 examples/s]Map:  21%|██        | 83000/395000 [00:09<00:37, 8408.44 examples/s]Map:  21%|██▏       | 84000/395000 [00:09<00:35, 8693.39 examples/s]Map:  22%|██▏       | 85000/395000 [00:09<00:34, 8886.81 examples/s]Map:  22%|██▏       | 86000/395000 [00:09<00:34, 8981.62 examples/s]Map:  22%|██▏       | 87000/395000 [00:09<00:33, 9070.75 examples/s]Map:  22%|██▏       | 88000/395000 [00:10<00:33, 9192.43 examples/s]Map:  23%|██▎       | 89000/395000 [00:10<00:32, 9342.70 examples/s]Map:  23%|██▎       | 90000/395000 [00:10<00:32, 9378.09 examples/s]Map:  23%|██▎       | 91000/395000 [00:10<00:33, 9206.03 examples/s]Map:  23%|██▎       | 92000/395000 [00:10<00:32, 9262.81 examples/s]Map:  24%|██▎       | 93000/395000 [00:10<00:32, 9286.41 examples/s]Map:  24%|██▍       | 94000/395000 [00:10<00:32, 9280.13 examples/s]Map:  24%|██▍       | 95000/395000 [00:10<00:32, 9304.37 examples/s]Map:  24%|██▍       | 96000/395000 [00:10<00:31, 9359.21 examples/s]Map:  25%|██▍       | 97000/395000 [00:10<00:32, 9292.05 examples/s]Map:  25%|██▍       | 98000/395000 [00:11<00:31, 9389.23 examples/s]Map:  25%|██▌       | 99000/395000 [00:11<00:31, 9313.66 examples/s]Map:  25%|██▌       | 100000/395000 [00:11<00:31, 9251.35 examples/s]Map:  26%|██▌       | 101000/395000 [00:11<00:31, 9286.95 examples/s]Map:  26%|██▌       | 102000/395000 [00:11<00:31, 9342.13 examples/s]Map:  26%|██▌       | 103000/395000 [00:11<00:44, 6547.71 examples/s]Map:  26%|██▋       | 104000/395000 [00:11<00:40, 7243.19 examples/s]Map:  27%|██▋       | 105000/395000 [00:11<00:36, 7875.79 examples/s]Map:  27%|██▋       | 106000/395000 [00:12<00:34, 8258.83 examples/s]Map:  27%|██▋       | 107000/395000 [00:12<00:33, 8575.04 examples/s]Map:  27%|██▋       | 108000/395000 [00:12<00:32, 8836.41 examples/s]Map:  28%|██▊       | 109000/395000 [00:12<00:31, 9012.65 examples/s]Map:  28%|██▊       | 110000/395000 [00:12<00:31, 9165.14 examples/s]Map:  28%|██▊       | 111000/395000 [00:12<00:30, 9219.23 examples/s]Map:  28%|██▊       | 112000/395000 [00:12<00:30, 9274.12 examples/s]Map:  29%|██▊       | 113000/395000 [00:12<00:30, 9396.79 examples/s]Map:  29%|██▉       | 114000/395000 [00:12<00:30, 9286.97 examples/s]Map:  29%|██▉       | 115000/395000 [00:13<00:30, 9322.48 examples/s]Map:  29%|██▉       | 116000/395000 [00:13<00:30, 9297.20 examples/s]Map:  30%|██▉       | 117000/395000 [00:13<00:29, 9411.70 examples/s]Map:  30%|██▉       | 118000/395000 [00:13<00:29, 9482.38 examples/s]Map:  30%|███       | 119000/395000 [00:13<00:29, 9483.18 examples/s]Map:  30%|███       | 120000/395000 [00:13<00:29, 9482.15 examples/s]Map:  31%|███       | 121000/395000 [00:13<00:29, 9431.80 examples/s]Map:  31%|███       | 122000/395000 [00:13<00:29, 9386.62 examples/s]Map:  31%|███       | 123000/395000 [00:13<00:28, 9389.40 examples/s]Map:  31%|███▏      | 124000/395000 [00:14<00:28, 9416.08 examples/s]Map:  32%|███▏      | 125000/395000 [00:14<00:38, 7010.83 examples/s]Map:  32%|███▏      | 126000/395000 [00:14<00:35, 7631.23 examples/s]Map:  32%|███▏      | 127000/395000 [00:14<00:33, 7995.73 examples/s]Map:  32%|███▏      | 128000/395000 [00:14<00:32, 8252.39 examples/s]Map:  33%|███▎      | 129000/395000 [00:14<00:31, 8566.91 examples/s]Map:  33%|███▎      | 130000/395000 [00:14<00:30, 8795.33 examples/s]Map:  33%|███▎      | 131000/395000 [00:14<00:29, 9006.07 examples/s]Map:  33%|███▎      | 132000/395000 [00:14<00:28, 9175.24 examples/s]Map:  34%|███▎      | 133000/395000 [00:15<00:28, 9228.96 examples/s]Map:  34%|███▍      | 134000/395000 [00:15<00:28, 9286.38 examples/s]Map:  34%|███▍      | 135000/395000 [00:15<00:27, 9393.68 examples/s]Map:  34%|███▍      | 136000/395000 [00:15<00:27, 9421.43 examples/s]Map:  35%|███▍      | 137000/395000 [00:15<00:27, 9404.87 examples/s]Map:  35%|███▍      | 138000/395000 [00:15<00:26, 9522.44 examples/s]Map:  35%|███▌      | 139000/395000 [00:15<00:27, 9353.16 examples/s]Map:  35%|███▌      | 140000/395000 [00:15<00:27, 9401.77 examples/s]Map:  36%|███▌      | 141000/395000 [00:15<00:26, 9490.66 examples/s]Map:  36%|███▌      | 142000/395000 [00:16<00:26, 9537.10 examples/s]Map:  36%|███▌      | 143000/395000 [00:16<00:26, 9577.70 examples/s]Map:  36%|███▋      | 144000/395000 [00:16<00:26, 9537.03 examples/s]Map:  37%|███▋      | 145000/395000 [00:16<00:26, 9481.68 examples/s]Map:  37%|███▋      | 146000/395000 [00:16<00:26, 9455.48 examples/s]Map:  37%|███▋      | 147000/395000 [00:16<00:26, 9418.57 examples/s]Map:  37%|███▋      | 148000/395000 [00:16<00:37, 6586.94 examples/s]Map:  38%|███▊      | 149000/395000 [00:16<00:34, 7233.27 examples/s]Map:  38%|███▊      | 150000/395000 [00:17<00:31, 7808.85 examples/s]Map:  38%|███▊      | 151000/395000 [00:17<00:29, 8159.28 examples/s]Map:  38%|███▊      | 152000/395000 [00:17<00:28, 8583.78 examples/s]Map:  39%|███▊      | 153000/395000 [00:17<00:27, 8869.24 examples/s]Map:  39%|███▉      | 154000/395000 [00:17<00:26, 9103.66 examples/s]Map:  39%|███▉      | 155000/395000 [00:17<00:25, 9284.35 examples/s]Map:  39%|███▉      | 156000/395000 [00:17<00:25, 9405.49 examples/s]Map:  40%|███▉      | 157000/395000 [00:17<00:25, 9358.62 examples/s]Map:  40%|████      | 158000/395000 [00:17<00:25, 9360.97 examples/s]Map:  40%|████      | 159000/395000 [00:17<00:25, 9362.60 examples/s]Map:  41%|████      | 160000/395000 [00:18<00:25, 9376.35 examples/s]Map:  41%|████      | 161000/395000 [00:18<00:24, 9431.55 examples/s]Map:  41%|████      | 162000/395000 [00:18<00:24, 9500.57 examples/s]Map:  41%|████▏     | 163000/395000 [00:18<00:24, 9462.63 examples/s]Map:  42%|████▏     | 164000/395000 [00:18<00:24, 9550.94 examples/s]Map:  42%|████▏     | 165000/395000 [00:18<00:24, 9482.53 examples/s]Map:  42%|████▏     | 166000/395000 [00:18<00:24, 9441.34 examples/s]Map:  42%|████▏     | 167000/395000 [00:18<00:24, 9407.34 examples/s]Map:  43%|████▎     | 168000/395000 [00:19<00:32, 7062.22 examples/s]Map:  43%|████▎     | 169000/395000 [00:19<00:29, 7641.97 examples/s]Map:  43%|████▎     | 170000/395000 [00:19<00:27, 8134.40 examples/s]Map:  43%|████▎     | 171000/395000 [00:19<00:26, 8544.73 examples/s]Map:  44%|████▎     | 172000/395000 [00:19<00:25, 8824.75 examples/s]Map:  44%|████▍     | 173000/395000 [00:19<00:24, 9069.84 examples/s]Map:  44%|████▍     | 174000/395000 [00:19<00:24, 9186.28 examples/s]Map:  44%|████▍     | 175000/395000 [00:19<00:23, 9213.57 examples/s]Map:  45%|████▍     | 176000/395000 [00:19<00:23, 9382.29 examples/s]Map:  45%|████▍     | 177000/395000 [00:19<00:22, 9541.67 examples/s]Map:  45%|████▌     | 178000/395000 [00:20<00:22, 9442.01 examples/s]Map:  45%|████▌     | 179000/395000 [00:20<00:22, 9602.22 examples/s]Map:  46%|████▌     | 180000/395000 [00:20<00:22, 9599.02 examples/s]Map:  46%|████▌     | 181000/395000 [00:20<00:22, 9659.05 examples/s]Map:  46%|████▌     | 182000/395000 [00:20<00:22, 9603.49 examples/s]Map:  46%|████▋     | 183000/395000 [00:20<00:22, 9574.42 examples/s]Map:  47%|████▋     | 184000/395000 [00:20<00:22, 9506.23 examples/s]Map:  47%|████▋     | 185000/395000 [00:20<00:22, 9504.34 examples/s]Map:  47%|████▋     | 186000/395000 [00:20<00:21, 9559.19 examples/s]Map:  47%|████▋     | 187000/395000 [00:21<00:21, 9500.86 examples/s]Map:  48%|████▊     | 188000/395000 [00:21<00:28, 7186.43 examples/s]Map:  48%|████▊     | 189000/395000 [00:21<00:26, 7795.22 examples/s]Map:  48%|████▊     | 190000/395000 [00:21<00:24, 8275.01 examples/s]Map:  48%|████▊     | 191000/395000 [00:21<00:23, 8615.33 examples/s]Map:  49%|████▊     | 192000/395000 [00:21<00:22, 8905.14 examples/s]Map:  49%|████▉     | 193000/395000 [00:21<00:22, 9121.00 examples/s]Map:  49%|████▉     | 194000/395000 [00:21<00:21, 9262.21 examples/s]Map:  49%|████▉     | 195000/395000 [00:21<00:21, 9401.32 examples/s]Map:  50%|████▉     | 196000/395000 [00:22<00:21, 9442.16 examples/s]Map:  50%|████▉     | 197000/395000 [00:22<00:20, 9482.59 examples/s]Map:  50%|█████     | 198000/395000 [00:22<00:20, 9519.48 examples/s]Map:  50%|█████     | 199000/395000 [00:22<00:20, 9611.42 examples/s]Map:  51%|█████     | 200000/395000 [00:22<00:20, 9601.69 examples/s]Map:  51%|█████     | 201000/395000 [00:22<00:20, 9623.66 examples/s]Map:  51%|█████     | 202000/395000 [00:22<00:20, 9629.32 examples/s]Map:  51%|█████▏    | 203000/395000 [00:22<00:19, 9624.31 examples/s]Map:  52%|█████▏    | 204000/395000 [00:22<00:19, 9567.07 examples/s]Map:  52%|█████▏    | 205000/395000 [00:23<00:19, 9586.17 examples/s]Map:  52%|█████▏    | 206000/395000 [00:23<00:19, 9622.10 examples/s]Map:  52%|█████▏    | 207000/395000 [00:23<00:26, 7180.20 examples/s]Map:  53%|█████▎    | 208000/395000 [00:23<00:24, 7713.37 examples/s]Map:  53%|█████▎    | 209000/395000 [00:23<00:22, 8236.04 examples/s]Map:  53%|█████▎    | 210000/395000 [00:23<00:21, 8523.16 examples/s]Map:  53%|█████▎    | 211000/395000 [00:23<00:20, 8785.75 examples/s]Map:  54%|█████▎    | 212000/395000 [00:23<00:20, 9075.32 examples/s]Map:  54%|█████▍    | 213000/395000 [00:23<00:19, 9263.32 examples/s]Map:  54%|█████▍    | 214000/395000 [00:24<00:19, 9420.85 examples/s]Map:  54%|█████▍    | 215000/395000 [00:24<00:18, 9561.48 examples/s]Map:  55%|█████▍    | 216000/395000 [00:24<00:18, 9545.86 examples/s]Map:  55%|█████▍    | 217000/395000 [00:24<00:18, 9540.83 examples/s]Map:  55%|█████▌    | 218000/395000 [00:24<00:18, 9598.58 examples/s]Map:  55%|█████▌    | 219000/395000 [00:24<00:18, 9616.47 examples/s]Map:  56%|█████▌    | 220000/395000 [00:24<00:18, 9595.05 examples/s]Map:  56%|█████▌    | 221000/395000 [00:24<00:18, 9616.14 examples/s]Map:  56%|█████▌    | 222000/395000 [00:24<00:17, 9632.43 examples/s]Map:  56%|█████▋    | 223000/395000 [00:25<00:17, 9647.04 examples/s]Map:  57%|█████▋    | 224000/395000 [00:25<00:17, 9649.39 examples/s]Map:  57%|█████▋    | 225000/395000 [00:25<00:17, 9644.95 examples/s]Map:  57%|█████▋    | 226000/395000 [00:25<00:17, 9562.97 examples/s]Map:  57%|█████▋    | 227000/395000 [00:25<00:23, 7159.18 examples/s]Map:  58%|█████▊    | 228000/395000 [00:25<00:21, 7797.01 examples/s]Map:  58%|█████▊    | 229000/395000 [00:25<00:20, 8252.64 examples/s]Map:  58%|█████▊    | 230000/395000 [00:25<00:19, 8659.87 examples/s]Map:  58%|█████▊    | 231000/395000 [00:25<00:18, 8948.64 examples/s]Map:  59%|█████▊    | 232000/395000 [00:26<00:17, 9138.61 examples/s]Map:  59%|█████▉    | 233000/395000 [00:26<00:17, 9344.90 examples/s]Map:  59%|█████▉    | 234000/395000 [00:26<00:17, 9391.04 examples/s]Map:  59%|█████▉    | 235000/395000 [00:26<00:16, 9418.79 examples/s]Map:  60%|█████▉    | 236000/395000 [00:26<00:16, 9460.84 examples/s]Map:  60%|██████    | 237000/395000 [00:26<00:16, 9506.60 examples/s]Map:  60%|██████    | 238000/395000 [00:26<00:16, 9534.34 examples/s]Map:  61%|██████    | 239000/395000 [00:26<00:16, 9341.25 examples/s]Map:  61%|██████    | 240000/395000 [00:26<00:16, 9346.02 examples/s]Map:  61%|██████    | 241000/395000 [00:27<00:16, 9437.08 examples/s]Map:  61%|██████▏   | 242000/395000 [00:27<00:16, 9491.36 examples/s]Map:  62%|██████▏   | 243000/395000 [00:27<00:15, 9550.92 examples/s]Map:  62%|██████▏   | 244000/395000 [00:27<00:15, 9508.71 examples/s]Map:  62%|██████▏   | 245000/395000 [00:27<00:15, 9532.18 examples/s]Map:  62%|██████▏   | 246000/395000 [00:27<00:21, 6922.88 examples/s]Map:  63%|██████▎   | 247000/395000 [00:27<00:19, 7529.20 examples/s]Map:  63%|██████▎   | 248000/395000 [00:27<00:18, 8025.61 examples/s]Map:  63%|██████▎   | 250000/395000 [00:28<00:16, 8751.41 examples/s]Map:  64%|██████▎   | 251000/395000 [00:28<00:16, 8964.16 examples/s]Map:  64%|██████▍   | 252000/395000 [00:28<00:15, 9166.22 examples/s]Map:  64%|██████▍   | 253000/395000 [00:28<00:15, 9320.48 examples/s]Map:  64%|██████▍   | 254000/395000 [00:28<00:14, 9405.68 examples/s]Map:  65%|██████▍   | 255000/395000 [00:28<00:14, 9366.14 examples/s]Map:  65%|██████▍   | 256000/395000 [00:28<00:14, 9393.77 examples/s]Map:  65%|██████▌   | 257000/395000 [00:28<00:14, 9457.14 examples/s]Map:  65%|██████▌   | 258000/395000 [00:28<00:14, 9519.62 examples/s]Map:  66%|██████▌   | 259000/395000 [00:29<00:14, 9546.85 examples/s]Map:  66%|██████▌   | 260000/395000 [00:29<00:14, 9575.19 examples/s]Map:  66%|██████▌   | 261000/395000 [00:29<00:13, 9579.61 examples/s]Map:  66%|██████▋   | 262000/395000 [00:29<00:13, 9599.82 examples/s]Map:  67%|██████▋   | 263000/395000 [00:29<00:13, 9646.85 examples/s]Map:  67%|██████▋   | 264000/395000 [00:29<00:13, 9638.85 examples/s]Map:  67%|██████▋   | 265000/395000 [00:29<00:13, 9544.32 examples/s]Map:  67%|██████▋   | 266000/395000 [00:29<00:18, 7081.59 examples/s]Map:  68%|██████▊   | 267000/395000 [00:29<00:16, 7749.76 examples/s]Map:  68%|██████▊   | 268000/395000 [00:30<00:15, 8064.91 examples/s]Map:  68%|██████▊   | 269000/395000 [00:30<00:14, 8466.03 examples/s]Map:  68%|██████▊   | 270000/395000 [00:30<00:14, 8712.48 examples/s]Map:  69%|██████▊   | 271000/395000 [00:30<00:13, 8974.02 examples/s]Map:  69%|██████▉   | 272000/395000 [00:30<00:13, 9067.49 examples/s]Map:  69%|██████▉   | 273000/395000 [00:30<00:13, 9267.24 examples/s]Map:  69%|██████▉   | 274000/395000 [00:30<00:12, 9419.10 examples/s]Map:  70%|██████▉   | 275000/395000 [00:30<00:12, 9405.15 examples/s]Map:  70%|██████▉   | 276000/395000 [00:30<00:12, 9448.49 examples/s]Map:  70%|███████   | 277000/395000 [00:31<00:12, 9471.22 examples/s]Map:  70%|███████   | 278000/395000 [00:31<00:12, 9505.61 examples/s]Map:  71%|███████   | 279000/395000 [00:31<00:12, 9526.19 examples/s]Map:  71%|███████   | 280000/395000 [00:31<00:11, 9588.86 examples/s]Map:  71%|███████   | 281000/395000 [00:31<00:11, 9597.85 examples/s]Map:  71%|███████▏  | 282000/395000 [00:31<00:11, 9590.31 examples/s]Map:  72%|███████▏  | 283000/395000 [00:31<00:11, 9535.24 examples/s]Map:  72%|███████▏  | 284000/395000 [00:31<00:11, 9632.74 examples/s]Map:  72%|███████▏  | 285000/395000 [00:31<00:15, 7240.44 examples/s]Map:  72%|███████▏  | 286000/395000 [00:32<00:13, 7854.84 examples/s]Map:  73%|███████▎  | 287000/395000 [00:32<00:12, 8391.65 examples/s]Map:  73%|███████▎  | 289000/395000 [00:32<00:11, 8998.84 examples/s]Map:  73%|███████▎  | 290000/395000 [00:32<00:11, 9136.63 examples/s]Map:  74%|███████▎  | 291000/395000 [00:32<00:11, 9274.33 examples/s]Map:  74%|███████▍  | 292000/395000 [00:32<00:11, 9337.85 examples/s]Map:  74%|███████▍  | 293000/395000 [00:32<00:10, 9459.62 examples/s]Map:  74%|███████▍  | 294000/395000 [00:32<00:10, 9552.73 examples/s]Map:  75%|███████▍  | 295000/395000 [00:33<00:10, 9425.87 examples/s]Map:  75%|███████▍  | 296000/395000 [00:33<00:10, 9361.45 examples/s]Map:  75%|███████▌  | 297000/395000 [00:33<00:10, 9357.75 examples/s]Map:  75%|███████▌  | 298000/395000 [00:33<00:10, 9254.72 examples/s]Map:  76%|███████▌  | 299000/395000 [00:33<00:10, 9460.66 examples/s]Map:  76%|███████▌  | 300000/395000 [00:33<00:09, 9519.63 examples/s]Map:  76%|███████▌  | 301000/395000 [00:33<00:09, 9576.77 examples/s]Map:  76%|███████▋  | 302000/395000 [00:33<00:09, 9611.12 examples/s]Map:  77%|███████▋  | 303000/395000 [00:33<00:09, 9656.26 examples/s]Map:  77%|███████▋  | 304000/395000 [00:33<00:09, 9678.09 examples/s]Map:  77%|███████▋  | 305000/395000 [00:34<00:12, 7227.23 examples/s]Map:  77%|███████▋  | 306000/395000 [00:34<00:11, 7846.24 examples/s]Map:  78%|███████▊  | 307000/395000 [00:34<00:10, 8299.54 examples/s]Map:  78%|███████▊  | 308000/395000 [00:34<00:10, 8639.42 examples/s]Map:  78%|███████▊  | 309000/395000 [00:34<00:09, 8918.68 examples/s]Map:  78%|███████▊  | 310000/395000 [00:34<00:09, 9123.07 examples/s]Map:  79%|███████▊  | 311000/395000 [00:34<00:09, 9327.85 examples/s]Map:  79%|███████▉  | 312000/395000 [00:34<00:08, 9391.69 examples/s]Map:  79%|███████▉  | 313000/395000 [00:34<00:08, 9423.74 examples/s]Map:  79%|███████▉  | 314000/395000 [00:35<00:08, 9459.54 examples/s]Map:  80%|███████▉  | 315000/395000 [00:35<00:08, 9538.87 examples/s]Map:  80%|████████  | 316000/395000 [00:35<00:08, 9598.73 examples/s]Map:  80%|████████  | 317000/395000 [00:35<00:08, 9561.58 examples/s]Map:  81%|████████  | 318000/395000 [00:35<00:07, 9645.00 examples/s]Map:  81%|████████  | 319000/395000 [00:35<00:08, 9403.67 examples/s]Map:  81%|████████  | 320000/395000 [00:35<00:07, 9461.47 examples/s]Map:  81%|████████▏ | 321000/395000 [00:35<00:07, 9502.18 examples/s]Map:  82%|████████▏ | 322000/395000 [00:35<00:07, 9634.77 examples/s]Map:  82%|████████▏ | 323000/395000 [00:36<00:07, 9614.68 examples/s]Map:  82%|████████▏ | 324000/395000 [00:36<00:09, 7220.87 examples/s]Map:  82%|████████▏ | 325000/395000 [00:36<00:08, 7794.84 examples/s]Map:  83%|████████▎ | 326000/395000 [00:36<00:08, 8242.81 examples/s]Map:  83%|████████▎ | 327000/395000 [00:36<00:07, 8582.25 examples/s]Map:  83%|████████▎ | 328000/395000 [00:36<00:07, 8840.59 examples/s]Map:  83%|████████▎ | 329000/395000 [00:36<00:07, 9027.81 examples/s]Map:  84%|████████▎ | 330000/395000 [00:36<00:07, 9271.12 examples/s]Map:  84%|████████▍ | 331000/395000 [00:36<00:06, 9295.19 examples/s]Map:  84%|████████▍ | 332000/395000 [00:37<00:06, 9374.94 examples/s]Map:  84%|████████▍ | 333000/395000 [00:37<00:06, 9470.71 examples/s]Map:  85%|████████▍ | 334000/395000 [00:37<00:06, 9522.05 examples/s]Map:  85%|████████▍ | 335000/395000 [00:37<00:06, 9474.24 examples/s]Map:  85%|████████▌ | 336000/395000 [00:37<00:06, 9494.15 examples/s]Map:  85%|████████▌ | 337000/395000 [00:37<00:06, 9537.91 examples/s]Map:  86%|████████▌ | 338000/395000 [00:37<00:05, 9651.95 examples/s]Map:  86%|████████▌ | 339000/395000 [00:37<00:05, 9654.87 examples/s]Map:  86%|████████▌ | 340000/395000 [00:37<00:05, 9529.30 examples/s]Map:  86%|████████▋ | 341000/395000 [00:38<00:05, 9613.99 examples/s]Map:  87%|████████▋ | 342000/395000 [00:38<00:05, 9616.91 examples/s]Map:  87%|████████▋ | 343000/395000 [00:38<00:05, 9604.00 examples/s]Map:  87%|████████▋ | 344000/395000 [00:38<00:07, 6820.23 examples/s]Map:  87%|████████▋ | 345000/395000 [00:38<00:06, 7475.86 examples/s]Map:  88%|████████▊ | 346000/395000 [00:38<00:06, 8014.07 examples/s]Map:  88%|████████▊ | 347000/395000 [00:38<00:05, 8494.28 examples/s]Map:  88%|████████▊ | 348000/395000 [00:38<00:05, 8819.48 examples/s]Map:  88%|████████▊ | 349000/395000 [00:39<00:05, 8973.23 examples/s]Map:  89%|████████▊ | 350000/395000 [00:39<00:04, 9229.53 examples/s]Map:  89%|████████▉ | 351000/395000 [00:39<00:04, 9349.27 examples/s]Map:  89%|████████▉ | 352000/395000 [00:39<00:04, 9455.34 examples/s]Map:  89%|████████▉ | 353000/395000 [00:39<00:04, 9582.53 examples/s]Map:  90%|████████▉ | 354000/395000 [00:39<00:04, 9658.90 examples/s]Map:  90%|████████▉ | 355000/395000 [00:39<00:04, 9656.63 examples/s]Map:  90%|█████████ | 356000/395000 [00:39<00:04, 9626.26 examples/s]Map:  90%|█████████ | 357000/395000 [00:39<00:03, 9654.95 examples/s]Map:  91%|█████████ | 358000/395000 [00:39<00:03, 9604.39 examples/s]Map:  91%|█████████ | 359000/395000 [00:40<00:03, 9640.78 examples/s]Map:  91%|█████████ | 360000/395000 [00:40<00:03, 9677.80 examples/s]Map:  91%|█████████▏| 361000/395000 [00:40<00:03, 9681.17 examples/s]Map:  92%|█████████▏| 362000/395000 [00:40<00:03, 9637.95 examples/s]Map:  92%|█████████▏| 363000/395000 [00:40<00:04, 7243.94 examples/s]Map:  92%|█████████▏| 364000/395000 [00:40<00:03, 7831.70 examples/s]Map:  92%|█████████▏| 365000/395000 [00:40<00:03, 8366.40 examples/s]Map:  93%|█████████▎| 366000/395000 [00:40<00:03, 8786.41 examples/s]Map:  93%|█████████▎| 367000/395000 [00:40<00:03, 8997.03 examples/s]Map:  93%|█████████▎| 368000/395000 [00:41<00:02, 9091.75 examples/s]Map:  93%|█████████▎| 369000/395000 [00:41<00:02, 9262.03 examples/s]Map:  94%|█████████▎| 370000/395000 [00:41<00:02, 9366.34 examples/s]Map:  94%|█████████▍| 371000/395000 [00:41<00:02, 9430.49 examples/s]Map:  94%|█████████▍| 372000/395000 [00:41<00:02, 9493.01 examples/s]Map:  94%|█████████▍| 373000/395000 [00:41<00:02, 9542.82 examples/s]Map:  95%|█████████▍| 374000/395000 [00:41<00:02, 9475.62 examples/s]Map:  95%|█████████▍| 375000/395000 [00:41<00:02, 9558.72 examples/s]Map:  95%|█████████▌| 376000/395000 [00:41<00:01, 9507.42 examples/s]Map:  95%|█████████▌| 377000/395000 [00:42<00:01, 9453.44 examples/s]Map:  96%|█████████▌| 378000/395000 [00:42<00:01, 9545.05 examples/s]Map:  96%|█████████▌| 379000/395000 [00:42<00:01, 9571.49 examples/s]Map:  96%|█████████▌| 380000/395000 [00:42<00:01, 9524.43 examples/s]Map:  96%|█████████▋| 381000/395000 [00:42<00:01, 9523.09 examples/s]Map:  97%|█████████▋| 382000/395000 [00:42<00:01, 9489.53 examples/s]Map:  97%|█████████▋| 383000/395000 [00:42<00:01, 6928.25 examples/s]Map:  97%|█████████▋| 384000/395000 [00:42<00:01, 7588.51 examples/s]Map:  97%|█████████▋| 385000/395000 [00:42<00:01, 8059.84 examples/s]Map:  98%|█████████▊| 386000/395000 [00:43<00:01, 8473.09 examples/s]Map:  98%|█████████▊| 387000/395000 [00:43<00:00, 8785.24 examples/s]Map:  98%|█████████▊| 388000/395000 [00:43<00:00, 8978.19 examples/s]Map:  98%|█████████▊| 389000/395000 [00:43<00:00, 9120.57 examples/s]Map:  99%|█████████▊| 390000/395000 [00:43<00:00, 9289.42 examples/s]Map:  99%|█████████▉| 391000/395000 [00:43<00:00, 9440.61 examples/s]Map:  99%|█████████▉| 392000/395000 [00:43<00:00, 9465.73 examples/s]Map:  99%|█████████▉| 393000/395000 [00:43<00:00, 9559.24 examples/s]Map: 100%|█████████▉| 394000/395000 [00:43<00:00, 9534.74 examples/s]Map: 100%|██████████| 395000/395000 [00:44<00:00, 9612.78 examples/s]Map: 100%|██████████| 395000/395000 [00:44<00:00, 8969.46 examples/s]
11/12/2024 10:04:34 - INFO - __main__ - Dataset({
    features: ['query', 'response', 'type', 'original_question', 'prompt', 'prefix'],
    num_rows: 395000
})
11/12/2024 10:04:34 - INFO - __main__ - ImplicitTransBridge(
  (llm): Qwen2ForCausalLM(
    (model): Qwen2Model(
      (embed_tokens): Embedding(152064, 3584)
      (layers): ModuleList(
        (0-27): 28 x Qwen2DecoderLayer(
          (self_attn): Qwen2SdpaAttention(
            (q_proj): Linear(in_features=3584, out_features=3584, bias=True)
            (k_proj): Linear(in_features=3584, out_features=512, bias=True)
            (v_proj): Linear(in_features=3584, out_features=512, bias=True)
            (o_proj): Linear(in_features=3584, out_features=3584, bias=False)
            (rotary_emb): Qwen2RotaryEmbedding()
          )
          (mlp): Qwen2MLP(
            (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)
            (up_proj): Linear(in_features=3584, out_features=18944, bias=False)
            (down_proj): Linear(in_features=18944, out_features=3584, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
          (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        )
      )
      (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    )
    (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
  )
  (llm_embedding_layer): Embedding(152064, 3584)
  (mt_model): MT5Model(
    (shared): Embedding(250112, 1024)
    (encoder): MT5Stack(
      (embed_tokens): Embedding(250112, 1024)
      (block): ModuleList(
        (0): MT5Block(
          (layer): ModuleList(
            (0): MT5LayerSelfAttention(
              (SelfAttention): MT5Attention(
                (q): Linear(in_features=1024, out_features=1024, bias=False)
                (k): Linear(in_features=1024, out_features=1024, bias=False)
                (v): Linear(in_features=1024, out_features=1024, bias=False)
                (o): Linear(in_features=1024, out_features=1024, bias=False)
                (relative_attention_bias): Embedding(32, 16)
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): MT5LayerFF(
              (DenseReluDense): MT5DenseGatedActDense(
                (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
                (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
                (wo): Linear(in_features=2816, out_features=1024, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (act): NewGELUActivation()
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1-23): 23 x MT5Block(
          (layer): ModuleList(
            (0): MT5LayerSelfAttention(
              (SelfAttention): MT5Attention(
                (q): Linear(in_features=1024, out_features=1024, bias=False)
                (k): Linear(in_features=1024, out_features=1024, bias=False)
                (v): Linear(in_features=1024, out_features=1024, bias=False)
                (o): Linear(in_features=1024, out_features=1024, bias=False)
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): MT5LayerFF(
              (DenseReluDense): MT5DenseGatedActDense(
                (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
                (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
                (wo): Linear(in_features=2816, out_features=1024, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (act): NewGELUActivation()
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (final_layer_norm): MT5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (decoder): MT5Stack(
      (embed_tokens): Embedding(250112, 1024)
      (block): ModuleList(
        (0): MT5Block(
          (layer): ModuleList(
            (0): MT5LayerSelfAttention(
              (SelfAttention): MT5Attention(
                (q): Linear(in_features=1024, out_features=1024, bias=False)
                (k): Linear(in_features=1024, out_features=1024, bias=False)
                (v): Linear(in_features=1024, out_features=1024, bias=False)
                (o): Linear(in_features=1024, out_features=1024, bias=False)
                (relative_attention_bias): Embedding(32, 16)
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): MT5LayerCrossAttention(
              (EncDecAttention): MT5Attention(
                (q): Linear(in_features=1024, out_features=1024, bias=False)
                (k): Linear(in_features=1024, out_features=1024, bias=False)
                (v): Linear(in_features=1024, out_features=1024, bias=False)
                (o): Linear(in_features=1024, out_features=1024, bias=False)
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (2): MT5LayerFF(
              (DenseReluDense): MT5DenseGatedActDense(
                (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
                (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
                (wo): Linear(in_features=2816, out_features=1024, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (act): NewGELUActivation()
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1-23): 23 x MT5Block(
          (layer): ModuleList(
            (0): MT5LayerSelfAttention(
              (SelfAttention): MT5Attention(
                (q): Linear(in_features=1024, out_features=1024, bias=False)
                (k): Linear(in_features=1024, out_features=1024, bias=False)
                (v): Linear(in_features=1024, out_features=1024, bias=False)
                (o): Linear(in_features=1024, out_features=1024, bias=False)
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): MT5LayerCrossAttention(
              (EncDecAttention): MT5Attention(
                (q): Linear(in_features=1024, out_features=1024, bias=False)
                (k): Linear(in_features=1024, out_features=1024, bias=False)
                (v): Linear(in_features=1024, out_features=1024, bias=False)
                (o): Linear(in_features=1024, out_features=1024, bias=False)
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (2): MT5LayerFF(
              (DenseReluDense): MT5DenseGatedActDense(
                (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
                (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
                (wo): Linear(in_features=2816, out_features=1024, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (act): NewGELUActivation()
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (final_layer_norm): MT5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (slm_a): MT5Stack(
    (embed_tokens): Embedding(250112, 1024)
    (block): ModuleList(
      (0): MT5Block(
        (layer): ModuleList(
          (0): MT5LayerSelfAttention(
            (SelfAttention): MT5Attention(
              (q): Linear(in_features=1024, out_features=1024, bias=False)
              (k): Linear(in_features=1024, out_features=1024, bias=False)
              (v): Linear(in_features=1024, out_features=1024, bias=False)
              (o): Linear(in_features=1024, out_features=1024, bias=False)
              (relative_attention_bias): Embedding(32, 16)
            )
            (layer_norm): MT5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): MT5LayerFF(
            (DenseReluDense): MT5DenseGatedActDense(
              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
              (wo): Linear(in_features=2816, out_features=1024, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): NewGELUActivation()
            )
            (layer_norm): MT5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1-23): 23 x MT5Block(
        (layer): ModuleList(
          (0): MT5LayerSelfAttention(
            (SelfAttention): MT5Attention(
              (q): Linear(in_features=1024, out_features=1024, bias=False)
              (k): Linear(in_features=1024, out_features=1024, bias=False)
              (v): Linear(in_features=1024, out_features=1024, bias=False)
              (o): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (layer_norm): MT5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): MT5LayerFF(
            (DenseReluDense): MT5DenseGatedActDense(
              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
              (wo): Linear(in_features=2816, out_features=1024, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): NewGELUActivation()
            )
            (layer_norm): MT5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (final_layer_norm): MT5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (mapping_a): Mapping(
    (mlp): MLP(
      (linear1): Linear(in_features=1024, out_features=2048, bias=True)
      (linear2): Linear(in_features=2048, out_features=3584, bias=True)
      (relu): ReLU()
    )
  )
)
11/12/2024 10:04:34 - INFO - __main__ - trainable params: 9446400 || all params: 8598529536 || trainable%: 0.1099
11/12/2024 10:04:34 - WARNING - accelerate.utils.other - Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:648] 2024-11-12 10:04:34,471 >> Using auto half precision backend
[2024-11-12 10:04:34,891] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.4, git-hash=unknown, git-branch=unknown
[2024-11-12 10:05:12,461] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-12 10:05:12,463] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-12 10:05:12,463] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-12 10:05:12,467] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-11-12 10:05:12,467] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-11-12 10:05:12,467] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-12 10:05:12,467] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 500000000
[2024-11-12 10:05:12,467] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 500000000
[2024-11-12 10:05:12,467] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: False
[2024-11-12 10:05:12,467] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False
[WARNING|logging.py:313] 2024-11-12 10:05:12,643 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-12 10:05:12,643 >> You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-12 10:05:12,655 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-12 10:05:12,655 >> You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-12 10:05:12,662 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-12 10:05:12,662 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-12 10:05:12,662 >> You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-12 10:05:12,663 >> You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-12 10:05:12,669 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-12 10:05:12,670 >> You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-12 10:05:12,671 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-12 10:05:12,671 >> You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-12 10:05:12,675 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-12 10:05:12,676 >> You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[2024-11-12 10:05:12,804] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2024-11-12 10:05:12,804] [INFO] [utils.py:782:see_memory_usage] MA 16.16 GB         Max_MA 16.16 GB         CA 16.29 GB         Max_CA 16 GB 
[2024-11-12 10:05:12,804] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 83.43 GB, percent = 4.1%
[2024-11-12 10:05:12,926] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2024-11-12 10:05:12,926] [INFO] [utils.py:782:see_memory_usage] MA 16.16 GB         Max_MA 16.17 GB         CA 16.29 GB         Max_CA 16 GB 
[2024-11-12 10:05:12,927] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 83.43 GB, percent = 4.1%
[2024-11-12 10:05:12,927] [INFO] [stage_1_and_2.py:543:__init__] optimizer state initialized
[2024-11-12 10:05:13,056] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2024-11-12 10:05:13,056] [INFO] [utils.py:782:see_memory_usage] MA 16.16 GB         Max_MA 16.16 GB         CA 16.29 GB         Max_CA 16 GB 
[2024-11-12 10:05:13,056] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 83.43 GB, percent = 4.1%
[2024-11-12 10:05:13,057] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2024-11-12 10:05:13,057] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-12 10:05:13,057] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-12 10:05:13,057] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0006, 0.0006], mom=[(0.9, 0.999), (0.9, 0.999)]
[2024-11-12 10:05:13,059] [INFO] [config.py:997:print] DeepSpeedEngine configuration:
[2024-11-12 10:05:13,059] [INFO] [config.py:1001:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   amp_enabled .................. False
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   amp_params ................... False
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   bfloat16_enabled ............. True
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   bfloat16_immediate_grad_update  False
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   checkpoint_parallel_write_pipeline  False
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   checkpoint_tag_validation_enabled  True
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   checkpoint_tag_validation_fail  False
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f13df14ca00>
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   communication_data_type ...... None
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   curriculum_enabled_legacy .... False
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   curriculum_params_legacy ..... False
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   data_efficiency_enabled ...... False
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   dataloader_drop_last ......... False
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   disable_allgather ............ False
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   dump_state ................... False
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   dynamic_loss_scale_args ...... None
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   eigenvalue_enabled ........... False
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   eigenvalue_layer_num ......... 0
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   eigenvalue_max_iter .......... 100
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   eigenvalue_stability ......... 1e-06
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   eigenvalue_tol ............... 0.01
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   eigenvalue_verbose ........... False
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   elasticity_enabled ........... False
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   fp16_auto_cast ............... None
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   fp16_enabled ................. False
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   fp16_master_weights_and_gradients  False
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   global_rank .................. 0
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   grad_accum_dtype ............. None
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   gradient_accumulation_steps .. 4
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   gradient_clipping ............ 1.0
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   gradient_predivide_factor .... 1.0
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   graph_harvesting ............. False
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   initial_dynamic_scale ........ 1
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   load_universal_checkpoint .... False
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   loss_scale ................... 1.0
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   memory_breakdown ............. False
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   mics_hierarchial_params_gather  False
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   mics_shard_size .............. -1
[2024-11-12 10:05:13,060] [INFO] [config.py:1001:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-12 10:05:13,061] [INFO] [config.py:1001:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-12 10:05:13,061] [INFO] [config.py:1001:print]   optimizer_legacy_fusion ...... False
[2024-11-12 10:05:13,061] [INFO] [config.py:1001:print]   optimizer_name ............... None
[2024-11-12 10:05:13,061] [INFO] [config.py:1001:print]   optimizer_params ............. None
[2024-11-12 10:05:13,061] [INFO] [config.py:1001:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-12 10:05:13,061] [INFO] [config.py:1001:print]   pld_enabled .................. False
[2024-11-12 10:05:13,061] [INFO] [config.py:1001:print]   pld_params ................... False
[2024-11-12 10:05:13,061] [INFO] [config.py:1001:print]   prescale_gradients ........... False
[2024-11-12 10:05:13,061] [INFO] [config.py:1001:print]   scheduler_name ............... None
[2024-11-12 10:05:13,061] [INFO] [config.py:1001:print]   scheduler_params ............. None
[2024-11-12 10:05:13,061] [INFO] [config.py:1001:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-12 10:05:13,061] [INFO] [config.py:1001:print]   sparse_attention ............. None
[2024-11-12 10:05:13,061] [INFO] [config.py:1001:print]   sparse_gradients_enabled ..... False
[2024-11-12 10:05:13,061] [INFO] [config.py:1001:print]   steps_per_print .............. inf
[2024-11-12 10:05:13,061] [INFO] [config.py:1001:print]   timers_config ................ enabled=True synchronized=True
[2024-11-12 10:05:13,061] [INFO] [config.py:1001:print]   train_batch_size ............. 128
[2024-11-12 10:05:13,061] [INFO] [config.py:1001:print]   train_micro_batch_size_per_gpu  4
[2024-11-12 10:05:13,061] [INFO] [config.py:1001:print]   use_data_before_expert_parallel_  False
[2024-11-12 10:05:13,061] [INFO] [config.py:1001:print]   use_node_local_storage ....... False
[2024-11-12 10:05:13,061] [INFO] [config.py:1001:print]   wall_clock_breakdown ......... False
[2024-11-12 10:05:13,061] [INFO] [config.py:1001:print]   weight_quantization_config ... None
[2024-11-12 10:05:13,061] [INFO] [config.py:1001:print]   world_size ................... 8
[2024-11-12 10:05:13,061] [INFO] [config.py:1001:print]   zero_allow_untested_optimizer  True
[2024-11-12 10:05:13,061] [INFO] [config.py:1001:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-12 10:05:13,061] [INFO] [config.py:1001:print]   zero_enabled ................. True
[2024-11-12 10:05:13,061] [INFO] [config.py:1001:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-12 10:05:13,061] [INFO] [config.py:1001:print]   zero_optimization_stage ...... 2
[2024-11-12 10:05:13,061] [INFO] [config.py:987:print_user_config]   json = {
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 4, 
    "gradient_accumulation_steps": 4, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }
}
[INFO|trainer.py:2134] 2024-11-12 10:05:13,061 >> ***** Running training *****
[INFO|trainer.py:2135] 2024-11-12 10:05:13,061 >>   Num examples = 395,000
[INFO|trainer.py:2136] 2024-11-12 10:05:13,061 >>   Num Epochs = 1
[INFO|trainer.py:2137] 2024-11-12 10:05:13,061 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2140] 2024-11-12 10:05:13,061 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2141] 2024-11-12 10:05:13,061 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2142] 2024-11-12 10:05:13,061 >>   Total optimization steps = 3,086
[INFO|trainer.py:2143] 2024-11-12 10:05:13,064 >>   Number of trainable parameters = 9,446,400
  0%|          | 0/3086 [00:00<?, ?it/s][WARNING|logging.py:313] 2024-11-12 10:05:13,083 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-12 10:05:13,086 >> You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|modeling_utils.py:1264] 2024-11-12 10:05:13,812 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed
[WARNING|modeling_utils.py:1264] 2024-11-12 10:05:13,812 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed
[WARNING|modeling_utils.py:1264] 2024-11-12 10:05:13,812 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed
[WARNING|modeling_utils.py:1264] 2024-11-12 10:05:13,812 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed
[WARNING|modeling_utils.py:1264] 2024-11-12 10:05:13,812 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed
[WARNING|modeling_utils.py:1264] 2024-11-12 10:05:13,812 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed
[WARNING|modeling_utils.py:1264] 2024-11-12 10:05:13,812 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed
[WARNING|modeling_utils.py:1264] 2024-11-12 10:05:13,813 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed
  0%|          | 1/3086 [00:02<2:03:06,  2.39s/it]  0%|          | 2/3086 [00:04<1:47:34,  2.09s/it]  0%|          | 3/3086 [00:06<1:45:06,  2.05s/it]  0%|          | 4/3086 [00:08<1:51:33,  2.17s/it]  0%|          | 5/3086 [00:10<1:52:22,  2.19s/it]  0%|          | 6/3086 [00:12<1:41:21,  1.97s/it]  0%|          | 7/3086 [00:14<1:47:54,  2.10s/it]  0%|          | 8/3086 [00:17<1:50:56,  2.16s/it]  0%|          | 9/3086 [00:18<1:43:12,  2.01s/it]  0%|          | 10/3086 [00:20<1:42:42,  2.00s/it]                                                   {'loss': 1.1966, 'grad_norm': 0.20491313934326172, 'learning_rate': 0.0005980557355800388, 'epoch': 0.0}
  0%|          | 10/3086 [00:20<1:42:42,  2.00s/it]  0%|          | 11/3086 [00:22<1:45:28,  2.06s/it]  0%|          | 12/3086 [00:24<1:44:11,  2.03s/it]  0%|          | 13/3086 [00:27<1:51:44,  2.18s/it]  0%|          | 14/3086 [00:30<2:01:05,  2.36s/it]  0%|          | 15/3086 [00:31<1:50:47,  2.16s/it]  1%|          | 16/3086 [00:33<1:49:28,  2.14s/it]  1%|          | 17/3086 [00:35<1:40:50,  1.97s/it]  1%|          | 18/3086 [00:38<1:48:07,  2.11s/it]  1%|          | 19/3086 [00:39<1:40:52,  1.97s/it]  1%|          | 20/3086 [00:41<1:42:56,  2.01s/it]                                                   {'loss': 1.0647, 'grad_norm': 0.271290123462677, 'learning_rate': 0.0005961114711600777, 'epoch': 0.01}
  1%|          | 20/3086 [00:41<1:42:56,  2.01s/it]  1%|          | 21/3086 [00:43<1:38:47,  1.93s/it]  1%|          | 22/3086 [00:45<1:37:00,  1.90s/it]  1%|          | 23/3086 [00:47<1:38:45,  1.93s/it]  1%|          | 24/3086 [00:48<1:34:19,  1.85s/it]  1%|          | 25/3086 [00:50<1:31:35,  1.80s/it]  1%|          | 26/3086 [00:52<1:25:12,  1.67s/it]  1%|          | 27/3086 [00:53<1:28:01,  1.73s/it]  1%|          | 28/3086 [00:55<1:25:23,  1.68s/it]  1%|          | 29/3086 [00:57<1:29:37,  1.76s/it]  1%|          | 30/3086 [00:59<1:35:51,  1.88s/it]                                                   {'loss': 0.8824, 'grad_norm': 0.12207168340682983, 'learning_rate': 0.0005941672067401166, 'epoch': 0.01}
  1%|          | 30/3086 [00:59<1:35:51,  1.88s/it]  1%|          | 31/3086 [01:01<1:38:20,  1.93s/it]  1%|          | 32/3086 [01:03<1:34:39,  1.86s/it]  1%|          | 33/3086 [01:05<1:32:53,  1.83s/it]  1%|          | 34/3086 [01:06<1:31:42,  1.80s/it]  1%|          | 35/3086 [01:08<1:29:58,  1.77s/it]  1%|          | 36/3086 [01:10<1:39:30,  1.96s/it]  1%|          | 37/3086 [01:12<1:34:32,  1.86s/it]  1%|          | 38/3086 [01:14<1:39:43,  1.96s/it]  1%|▏         | 39/3086 [01:16<1:37:08,  1.91s/it]  1%|▏         | 40/3086 [01:18<1:34:49,  1.87s/it]                                                   {'loss': 0.7945, 'grad_norm': 0.0861520916223526, 'learning_rate': 0.0005922229423201555, 'epoch': 0.01}
  1%|▏         | 40/3086 [01:18<1:34:49,  1.87s/it]  1%|▏         | 41/3086 [01:21<1:48:47,  2.14s/it]  1%|▏         | 42/3086 [01:23<1:45:32,  2.08s/it]  1%|▏         | 43/3086 [01:24<1:43:51,  2.05s/it]  1%|▏         | 44/3086 [01:26<1:39:52,  1.97s/it]  1%|▏         | 45/3086 [01:29<1:51:01,  2.19s/it]  1%|▏         | 46/3086 [01:31<1:44:01,  2.05s/it]  2%|▏         | 47/3086 [01:32<1:37:01,  1.92s/it]  2%|▏         | 48/3086 [01:34<1:33:27,  1.85s/it]  2%|▏         | 49/3086 [01:36<1:32:28,  1.83s/it]  2%|▏         | 50/3086 [01:38<1:37:07,  1.92s/it]                                                   {'loss': 0.7374, 'grad_norm': 0.07308324426412582, 'learning_rate': 0.0005902786779001944, 'epoch': 0.02}
  2%|▏         | 50/3086 [01:38<1:37:07,  1.92s/it]  2%|▏         | 51/3086 [01:40<1:41:21,  2.00s/it]  2%|▏         | 52/3086 [01:42<1:46:34,  2.11s/it]  2%|▏         | 53/3086 [01:45<1:54:19,  2.26s/it]  2%|▏         | 54/3086 [01:47<1:51:57,  2.22s/it]  2%|▏         | 55/3086 [01:49<1:42:21,  2.03s/it]  2%|▏         | 56/3086 [01:52<1:56:59,  2.32s/it]  2%|▏         | 57/3086 [01:54<1:52:51,  2.24s/it]  2%|▏         | 58/3086 [01:56<1:47:36,  2.13s/it]  2%|▏         | 59/3086 [01:58<1:55:25,  2.29s/it]  2%|▏         | 60/3086 [02:01<1:54:50,  2.28s/it]                                                   {'loss': 0.6891, 'grad_norm': 0.06456107646226883, 'learning_rate': 0.0005883344134802333, 'epoch': 0.02}
  2%|▏         | 60/3086 [02:01<1:54:50,  2.28s/it]  2%|▏         | 61/3086 [02:03<1:49:18,  2.17s/it]  2%|▏         | 62/3086 [02:04<1:40:28,  1.99s/it]  2%|▏         | 63/3086 [02:07<1:47:25,  2.13s/it]  2%|▏         | 64/3086 [02:08<1:43:45,  2.06s/it]  2%|▏         | 65/3086 [02:10<1:39:39,  1.98s/it]  2%|▏         | 66/3086 [02:12<1:43:13,  2.05s/it]  2%|▏         | 67/3086 [02:14<1:37:08,  1.93s/it]  2%|▏         | 68/3086 [02:16<1:39:28,  1.98s/it]  2%|▏         | 69/3086 [02:18<1:39:57,  1.99s/it]  2%|▏         | 70/3086 [02:20<1:42:02,  2.03s/it]                                                   {'loss': 0.7061, 'grad_norm': 0.07091397047042847, 'learning_rate': 0.0005863901490602722, 'epoch': 0.02}
  2%|▏         | 70/3086 [02:20<1:42:02,  2.03s/it]  2%|▏         | 71/3086 [02:22<1:39:47,  1.99s/it]  2%|▏         | 72/3086 [02:24<1:37:04,  1.93s/it]  2%|▏         | 73/3086 [02:26<1:33:57,  1.87s/it]  2%|▏         | 74/3086 [02:28<1:38:00,  1.95s/it]  2%|▏         | 75/3086 [02:30<1:43:26,  2.06s/it]  2%|▏         | 76/3086 [02:32<1:39:48,  1.99s/it]  2%|▏         | 77/3086 [02:34<1:37:53,  1.95s/it]  3%|▎         | 78/3086 [02:36<1:34:53,  1.89s/it]  3%|▎         | 79/3086 [02:38<1:42:29,  2.05s/it]  3%|▎         | 80/3086 [02:40<1:43:43,  2.07s/it]                                                   {'loss': 0.6734, 'grad_norm': 0.07912154495716095, 'learning_rate': 0.000584445884640311, 'epoch': 0.03}
  3%|▎         | 80/3086 [02:40<1:43:43,  2.07s/it]  3%|▎         | 81/3086 [02:42<1:45:33,  2.11s/it]  3%|▎         | 82/3086 [02:44<1:39:26,  1.99s/it]  3%|▎         | 83/3086 [02:46<1:44:50,  2.09s/it]  3%|▎         | 84/3086 [02:48<1:40:00,  2.00s/it]  3%|▎         | 85/3086 [02:50<1:42:50,  2.06s/it]  3%|▎         | 86/3086 [02:52<1:36:55,  1.94s/it]  3%|▎         | 87/3086 [02:54<1:34:28,  1.89s/it]  3%|▎         | 88/3086 [02:56<1:32:26,  1.85s/it]  3%|▎         | 89/3086 [02:57<1:29:58,  1.80s/it]  3%|▎         | 90/3086 [03:00<1:37:14,  1.95s/it]                                                   {'loss': 0.6807, 'grad_norm': 0.08409475535154343, 'learning_rate': 0.0005825016202203499, 'epoch': 0.03}
  3%|▎         | 90/3086 [03:00<1:37:14,  1.95s/it]  3%|▎         | 91/3086 [03:01<1:36:49,  1.94s/it]  3%|▎         | 92/3086 [03:03<1:36:19,  1.93s/it]  3%|▎         | 93/3086 [03:06<1:51:48,  2.24s/it]  3%|▎         | 94/3086 [03:08<1:49:22,  2.19s/it]  3%|▎         | 95/3086 [03:11<1:47:19,  2.15s/it]  3%|▎         | 96/3086 [03:12<1:42:27,  2.06s/it]  3%|▎         | 97/3086 [03:14<1:42:42,  2.06s/it]  3%|▎         | 98/3086 [03:17<1:43:06,  2.07s/it]  3%|▎         | 99/3086 [03:19<1:55:35,  2.32s/it]  3%|▎         | 100/3086 [03:21<1:49:58,  2.21s/it]                                                    {'loss': 0.6553, 'grad_norm': 0.07885850220918655, 'learning_rate': 0.0005805573558003888, 'epoch': 0.03}
  3%|▎         | 100/3086 [03:21<1:49:58,  2.21s/it]  3%|▎         | 101/3086 [03:23<1:43:56,  2.09s/it]  3%|▎         | 102/3086 [03:25<1:42:21,  2.06s/it]  3%|▎         | 103/3086 [03:27<1:37:36,  1.96s/it]  3%|▎         | 104/3086 [03:29<1:42:18,  2.06s/it]  3%|▎         | 105/3086 [03:31<1:43:52,  2.09s/it]  3%|▎         | 106/3086 [03:34<1:49:15,  2.20s/it]  3%|▎         | 107/3086 [03:36<1:44:18,  2.10s/it]  3%|▎         | 108/3086 [03:38<1:41:18,  2.04s/it]  4%|▎         | 109/3086 [03:40<1:39:48,  2.01s/it]  4%|▎         | 110/3086 [03:42<1:40:51,  2.03s/it]                                                    {'loss': 0.659, 'grad_norm': 0.0806795284152031, 'learning_rate': 0.0005786130913804277, 'epoch': 0.04}
  4%|▎         | 110/3086 [03:42<1:40:51,  2.03s/it]  4%|▎         | 111/3086 [03:43<1:36:40,  1.95s/it]  4%|▎         | 112/3086 [03:45<1:36:29,  1.95s/it]  4%|▎         | 113/3086 [03:47<1:33:21,  1.88s/it]  4%|▎         | 114/3086 [03:48<1:26:26,  1.75s/it]  4%|▎         | 115/3086 [03:50<1:27:09,  1.76s/it]  4%|▍         | 116/3086 [03:52<1:28:20,  1.78s/it]  4%|▍         | 117/3086 [03:54<1:30:33,  1.83s/it]  4%|▍         | 118/3086 [03:56<1:32:25,  1.87s/it]  4%|▍         | 119/3086 [03:58<1:35:55,  1.94s/it]  4%|▍         | 120/3086 [04:00<1:32:17,  1.87s/it]                                                    {'loss': 0.6562, 'grad_norm': 0.08681143075227737, 'learning_rate': 0.0005766688269604666, 'epoch': 0.04}
  4%|▍         | 120/3086 [04:00<1:32:17,  1.87s/it]  4%|▍         | 121/3086 [04:02<1:35:05,  1.92s/it]  4%|▍         | 122/3086 [04:04<1:40:53,  2.04s/it]  4%|▍         | 123/3086 [04:06<1:43:18,  2.09s/it]  4%|▍         | 124/3086 [04:09<1:49:34,  2.22s/it]  4%|▍         | 125/3086 [04:12<1:55:49,  2.35s/it]  4%|▍         | 126/3086 [04:14<1:56:21,  2.36s/it]  4%|▍         | 127/3086 [04:16<1:52:19,  2.28s/it]  4%|▍         | 128/3086 [04:18<1:48:22,  2.20s/it]  4%|▍         | 129/3086 [04:20<1:49:12,  2.22s/it]  4%|▍         | 130/3086 [04:22<1:44:32,  2.12s/it]                                                    {'loss': 0.6639, 'grad_norm': 0.12925313413143158, 'learning_rate': 0.0005747245625405055, 'epoch': 0.04}
  4%|▍         | 130/3086 [04:22<1:44:32,  2.12s/it]  4%|▍         | 131/3086 [04:24<1:46:13,  2.16s/it]  4%|▍         | 132/3086 [04:26<1:42:00,  2.07s/it]  4%|▍         | 133/3086 [04:28<1:38:32,  2.00s/it]  4%|▍         | 134/3086 [04:30<1:39:18,  2.02s/it]  4%|▍         | 135/3086 [04:32<1:33:28,  1.90s/it]  4%|▍         | 136/3086 [04:33<1:28:30,  1.80s/it]  4%|▍         | 137/3086 [04:35<1:32:52,  1.89s/it]  4%|▍         | 138/3086 [04:37<1:31:26,  1.86s/it]  5%|▍         | 139/3086 [04:39<1:34:07,  1.92s/it]  5%|▍         | 140/3086 [04:41<1:31:09,  1.86s/it]                                                    {'loss': 0.6583, 'grad_norm': 0.12121317535638809, 'learning_rate': 0.0005727802981205444, 'epoch': 0.05}
  5%|▍         | 140/3086 [04:41<1:31:09,  1.86s/it]  5%|▍         | 141/3086 [04:43<1:35:07,  1.94s/it]  5%|▍         | 142/3086 [04:45<1:37:26,  1.99s/it]  5%|▍         | 143/3086 [04:47<1:37:01,  1.98s/it]  5%|▍         | 144/3086 [04:49<1:32:30,  1.89s/it]  5%|▍         | 145/3086 [04:51<1:33:59,  1.92s/it]  5%|▍         | 146/3086 [04:53<1:34:55,  1.94s/it]  5%|▍         | 147/3086 [04:55<1:31:43,  1.87s/it]  5%|▍         | 148/3086 [04:57<1:39:07,  2.02s/it]  5%|▍         | 149/3086 [04:59<1:40:29,  2.05s/it]  5%|▍         | 150/3086 [05:01<1:39:40,  2.04s/it]                                                    {'loss': 0.6469, 'grad_norm': 0.09641176462173462, 'learning_rate': 0.0005708360337005833, 'epoch': 0.05}
  5%|▍         | 150/3086 [05:01<1:39:40,  2.04s/it]  5%|▍         | 151/3086 [05:03<1:35:12,  1.95s/it]  5%|▍         | 152/3086 [05:05<1:38:17,  2.01s/it]  5%|▍         | 153/3086 [05:07<1:37:15,  1.99s/it]  5%|▍         | 154/3086 [05:09<1:35:50,  1.96s/it]  5%|▌         | 155/3086 [05:11<1:33:43,  1.92s/it]  5%|▌         | 156/3086 [05:13<1:37:25,  2.00s/it]  5%|▌         | 157/3086 [05:15<1:36:31,  1.98s/it]  5%|▌         | 158/3086 [05:17<1:38:51,  2.03s/it]  5%|▌         | 159/3086 [05:19<1:36:33,  1.98s/it]  5%|▌         | 160/3086 [05:21<1:41:54,  2.09s/it]                                                    {'loss': 0.632, 'grad_norm': 0.12528757750988007, 'learning_rate': 0.0005688917692806221, 'epoch': 0.05}
  5%|▌         | 160/3086 [05:21<1:41:54,  2.09s/it]  5%|▌         | 161/3086 [05:24<1:54:33,  2.35s/it]  5%|▌         | 162/3086 [05:26<1:48:59,  2.24s/it]  5%|▌         | 163/3086 [05:28<1:44:24,  2.14s/it]  5%|▌         | 164/3086 [05:30<1:44:03,  2.14s/it]  5%|▌         | 165/3086 [05:33<1:49:51,  2.26s/it]  5%|▌         | 166/3086 [05:35<1:53:02,  2.32s/it]  5%|▌         | 167/3086 [05:37<1:48:32,  2.23s/it]  5%|▌         | 168/3086 [05:39<1:44:39,  2.15s/it]  5%|▌         | 169/3086 [05:41<1:37:15,  2.00s/it]  6%|▌         | 170/3086 [05:42<1:33:21,  1.92s/it]                                                    {'loss': 0.6374, 'grad_norm': 0.12486892938613892, 'learning_rate': 0.000566947504860661, 'epoch': 0.06}
  6%|▌         | 170/3086 [05:42<1:33:21,  1.92s/it]  6%|▌         | 171/3086 [05:45<1:44:22,  2.15s/it]  6%|▌         | 172/3086 [05:47<1:36:48,  1.99s/it]  6%|▌         | 173/3086 [05:49<1:34:29,  1.95s/it]  6%|▌         | 174/3086 [05:51<1:41:48,  2.10s/it]  6%|▌         | 175/3086 [05:53<1:42:08,  2.11s/it]  6%|▌         | 176/3086 [05:55<1:33:52,  1.94s/it]  6%|▌         | 177/3086 [05:57<1:37:43,  2.02s/it]  6%|▌         | 178/3086 [05:59<1:38:07,  2.02s/it]  6%|▌         | 179/3086 [06:01<1:32:19,  1.91s/it]  6%|▌         | 180/3086 [06:02<1:26:33,  1.79s/it]                                                    {'loss': 0.6355, 'grad_norm': 0.11496219784021378, 'learning_rate': 0.0005650032404406999, 'epoch': 0.06}
  6%|▌         | 180/3086 [06:02<1:26:33,  1.79s/it]  6%|▌         | 181/3086 [06:04<1:26:13,  1.78s/it]  6%|▌         | 182/3086 [06:06<1:30:52,  1.88s/it]  6%|▌         | 183/3086 [06:08<1:30:29,  1.87s/it]  6%|▌         | 184/3086 [06:10<1:30:13,  1.87s/it]  6%|▌         | 185/3086 [06:11<1:29:06,  1.84s/it]  6%|▌         | 186/3086 [06:13<1:30:05,  1.86s/it]  6%|▌         | 187/3086 [06:15<1:27:43,  1.82s/it]  6%|▌         | 188/3086 [06:17<1:35:43,  1.98s/it]  6%|▌         | 189/3086 [06:20<1:41:51,  2.11s/it]  6%|▌         | 190/3086 [06:21<1:33:11,  1.93s/it]                                                    {'loss': 0.6259, 'grad_norm': 0.12522342801094055, 'learning_rate': 0.0005630589760207388, 'epoch': 0.06}
  6%|▌         | 190/3086 [06:21<1:33:11,  1.93s/it]  6%|▌         | 191/3086 [06:24<1:38:07,  2.03s/it]  6%|▌         | 192/3086 [06:25<1:33:06,  1.93s/it]  6%|▋         | 193/3086 [06:27<1:34:37,  1.96s/it]  6%|▋         | 194/3086 [06:29<1:35:59,  1.99s/it]  6%|▋         | 195/3086 [06:31<1:33:37,  1.94s/it]  6%|▋         | 196/3086 [06:34<1:41:39,  2.11s/it]  6%|▋         | 197/3086 [06:36<1:37:27,  2.02s/it]  6%|▋         | 198/3086 [06:38<1:47:02,  2.22s/it]  6%|▋         | 199/3086 [06:40<1:42:11,  2.12s/it]  6%|▋         | 200/3086 [06:42<1:41:53,  2.12s/it]                                                    {'loss': 0.617, 'grad_norm': 0.09518540650606155, 'learning_rate': 0.0005611147116007777, 'epoch': 0.06}
  6%|▋         | 200/3086 [06:42<1:41:53,  2.12s/it]  7%|▋         | 201/3086 [06:44<1:39:31,  2.07s/it]  7%|▋         | 202/3086 [06:47<1:46:15,  2.21s/it]  7%|▋         | 203/3086 [06:49<1:40:43,  2.10s/it]  7%|▋         | 204/3086 [06:50<1:33:21,  1.94s/it]  7%|▋         | 205/3086 [06:53<1:43:48,  2.16s/it]  7%|▋         | 206/3086 [06:55<1:38:55,  2.06s/it]  7%|▋         | 207/3086 [06:57<1:36:55,  2.02s/it]  7%|▋         | 208/3086 [06:59<1:40:58,  2.11s/it]  7%|▋         | 209/3086 [07:01<1:35:53,  2.00s/it]  7%|▋         | 210/3086 [07:03<1:34:16,  1.97s/it]                                                    {'loss': 0.619, 'grad_norm': 0.10775984078645706, 'learning_rate': 0.0005591704471808166, 'epoch': 0.07}
  7%|▋         | 210/3086 [07:03<1:34:16,  1.97s/it]  7%|▋         | 211/3086 [07:05<1:39:45,  2.08s/it]  7%|▋         | 212/3086 [07:07<1:43:35,  2.16s/it]  7%|▋         | 213/3086 [07:09<1:42:44,  2.15s/it]  7%|▋         | 214/3086 [07:11<1:36:48,  2.02s/it]  7%|▋         | 215/3086 [07:14<1:45:29,  2.20s/it]  7%|▋         | 216/3086 [07:16<1:42:07,  2.14s/it]  7%|▋         | 217/3086 [07:18<1:40:59,  2.11s/it]  7%|▋         | 218/3086 [07:20<1:37:43,  2.04s/it]  7%|▋         | 219/3086 [07:22<1:39:14,  2.08s/it]  7%|▋         | 220/3086 [07:23<1:33:18,  1.95s/it]                                                    {'loss': 0.6191, 'grad_norm': 0.10656294971704483, 'learning_rate': 0.0005572261827608555, 'epoch': 0.07}
  7%|▋         | 220/3086 [07:23<1:33:18,  1.95s/it]  7%|▋         | 221/3086 [07:26<1:41:33,  2.13s/it]  7%|▋         | 222/3086 [07:28<1:36:52,  2.03s/it]  7%|▋         | 223/3086 [07:30<1:39:36,  2.09s/it]  7%|▋         | 224/3086 [07:32<1:38:22,  2.06s/it]  7%|▋         | 225/3086 [07:34<1:44:00,  2.18s/it]  7%|▋         | 226/3086 [07:37<1:47:50,  2.26s/it]  7%|▋         | 227/3086 [07:40<1:56:42,  2.45s/it]  7%|▋         | 228/3086 [07:42<1:57:39,  2.47s/it]  7%|▋         | 229/3086 [07:44<1:49:33,  2.30s/it]  7%|▋         | 230/3086 [07:46<1:37:58,  2.06s/it]                                                    {'loss': 0.6048, 'grad_norm': 0.10866052657365799, 'learning_rate': 0.0005552819183408942, 'epoch': 0.07}
  7%|▋         | 230/3086 [07:46<1:37:58,  2.06s/it]  7%|▋         | 231/3086 [07:48<1:34:24,  1.98s/it]  8%|▊         | 232/3086 [07:50<1:41:24,  2.13s/it]  8%|▊         | 233/3086 [07:52<1:36:15,  2.02s/it]  8%|▊         | 234/3086 [07:54<1:39:35,  2.10s/it]  8%|▊         | 235/3086 [07:56<1:35:13,  2.00s/it]  8%|▊         | 236/3086 [07:58<1:34:34,  1.99s/it]  8%|▊         | 237/3086 [08:00<1:31:35,  1.93s/it]  8%|▊         | 238/3086 [08:02<1:32:17,  1.94s/it]  8%|▊         | 239/3086 [08:03<1:31:24,  1.93s/it]  8%|▊         | 240/3086 [08:05<1:29:45,  1.89s/it]                                                    {'loss': 0.6054, 'grad_norm': 0.07596857100725174, 'learning_rate': 0.0005533376539209332, 'epoch': 0.08}
  8%|▊         | 240/3086 [08:05<1:29:45,  1.89s/it]  8%|▊         | 241/3086 [08:07<1:28:32,  1.87s/it]  8%|▊         | 242/3086 [08:09<1:28:33,  1.87s/it]  8%|▊         | 243/3086 [08:11<1:30:57,  1.92s/it]  8%|▊         | 244/3086 [08:13<1:37:15,  2.05s/it]  8%|▊         | 245/3086 [08:15<1:35:05,  2.01s/it]  8%|▊         | 246/3086 [08:17<1:34:09,  1.99s/it]  8%|▊         | 247/3086 [08:19<1:36:23,  2.04s/it]  8%|▊         | 248/3086 [08:21<1:33:42,  1.98s/it]  8%|▊         | 249/3086 [08:23<1:28:35,  1.87s/it]  8%|▊         | 250/3086 [08:25<1:30:54,  1.92s/it]                                                    {'loss': 0.6123, 'grad_norm': 0.11247911304235458, 'learning_rate': 0.0005513933895009721, 'epoch': 0.08}
  8%|▊         | 250/3086 [08:25<1:30:54,  1.92s/it]  8%|▊         | 251/3086 [08:27<1:28:13,  1.87s/it]  8%|▊         | 252/3086 [08:29<1:29:04,  1.89s/it]  8%|▊         | 253/3086 [08:31<1:34:31,  2.00s/it]  8%|▊         | 254/3086 [08:33<1:36:18,  2.04s/it]  8%|▊         | 255/3086 [08:34<1:28:01,  1.87s/it]  8%|▊         | 256/3086 [08:37<1:43:09,  2.19s/it]  8%|▊         | 257/3086 [08:40<1:44:14,  2.21s/it]  8%|▊         | 258/3086 [08:41<1:37:11,  2.06s/it]  8%|▊         | 259/3086 [08:43<1:36:38,  2.05s/it]  8%|▊         | 260/3086 [08:45<1:29:15,  1.90s/it]                                                    {'loss': 0.612, 'grad_norm': 0.1377895325422287, 'learning_rate': 0.000549449125081011, 'epoch': 0.08}
  8%|▊         | 260/3086 [08:45<1:29:15,  1.90s/it]  8%|▊         | 261/3086 [08:47<1:26:17,  1.83s/it]  8%|▊         | 262/3086 [08:49<1:31:18,  1.94s/it]  9%|▊         | 263/3086 [08:51<1:29:31,  1.90s/it]  9%|▊         | 264/3086 [08:53<1:35:09,  2.02s/it]  9%|▊         | 265/3086 [08:55<1:34:49,  2.02s/it]  9%|▊         | 266/3086 [08:57<1:33:25,  1.99s/it]  9%|▊         | 267/3086 [08:59<1:30:10,  1.92s/it]  9%|▊         | 268/3086 [09:01<1:32:59,  1.98s/it]  9%|▊         | 269/3086 [09:03<1:32:46,  1.98s/it]  9%|▊         | 270/3086 [09:04<1:28:15,  1.88s/it]                                                    {'loss': 0.6109, 'grad_norm': 0.11709560453891754, 'learning_rate': 0.0005475048606610499, 'epoch': 0.09}
  9%|▊         | 270/3086 [09:04<1:28:15,  1.88s/it]  9%|▉         | 271/3086 [09:06<1:27:29,  1.86s/it]  9%|▉         | 272/3086 [09:09<1:45:36,  2.25s/it]  9%|▉         | 273/3086 [09:11<1:41:02,  2.16s/it]  9%|▉         | 274/3086 [09:13<1:39:57,  2.13s/it]  9%|▉         | 275/3086 [09:15<1:39:32,  2.12s/it]  9%|▉         | 276/3086 [09:18<1:42:36,  2.19s/it]  9%|▉         | 277/3086 [09:20<1:43:51,  2.22s/it]  9%|▉         | 278/3086 [09:22<1:42:36,  2.19s/it]  9%|▉         | 279/3086 [09:24<1:35:28,  2.04s/it]  9%|▉         | 280/3086 [09:26<1:39:24,  2.13s/it]                                                    {'loss': 0.611, 'grad_norm': 0.08353966474533081, 'learning_rate': 0.0005455605962410887, 'epoch': 0.09}
  9%|▉         | 280/3086 [09:26<1:39:24,  2.13s/it]  9%|▉         | 281/3086 [09:28<1:31:54,  1.97s/it]  9%|▉         | 282/3086 [09:30<1:35:18,  2.04s/it]  9%|▉         | 283/3086 [09:32<1:32:59,  1.99s/it]  9%|▉         | 284/3086 [09:34<1:30:18,  1.93s/it]  9%|▉         | 285/3086 [09:36<1:29:58,  1.93s/it]  9%|▉         | 286/3086 [09:38<1:39:26,  2.13s/it]  9%|▉         | 287/3086 [09:40<1:33:07,  2.00s/it]  9%|▉         | 288/3086 [09:43<1:42:25,  2.20s/it]  9%|▉         | 289/3086 [09:44<1:37:11,  2.08s/it]  9%|▉         | 290/3086 [09:46<1:35:18,  2.05s/it]                                                    {'loss': 0.5851, 'grad_norm': 0.13352437317371368, 'learning_rate': 0.0005436163318211277, 'epoch': 0.09}
  9%|▉         | 290/3086 [09:46<1:35:18,  2.05s/it]  9%|▉         | 291/3086 [09:48<1:30:12,  1.94s/it]  9%|▉         | 292/3086 [09:50<1:32:26,  1.99s/it]  9%|▉         | 293/3086 [09:52<1:34:59,  2.04s/it] 10%|▉         | 294/3086 [09:54<1:32:21,  1.98s/it] 10%|▉         | 295/3086 [09:56<1:36:52,  2.08s/it] 10%|▉         | 296/3086 [09:58<1:30:20,  1.94s/it] 10%|▉         | 297/3086 [10:00<1:31:36,  1.97s/it] 10%|▉         | 298/3086 [10:02<1:32:30,  1.99s/it] 10%|▉         | 299/3086 [10:04<1:31:54,  1.98s/it] 10%|▉         | 300/3086 [10:06<1:26:32,  1.86s/it]                                                    {'loss': 0.603, 'grad_norm': 0.12868817150592804, 'learning_rate': 0.0005416720674011666, 'epoch': 0.1}
 10%|▉         | 300/3086 [10:06<1:26:32,  1.86s/it] 10%|▉         | 301/3086 [10:08<1:30:14,  1.94s/it] 10%|▉         | 302/3086 [10:09<1:26:36,  1.87s/it] 10%|▉         | 303/3086 [10:12<1:31:24,  1.97s/it] 10%|▉         | 304/3086 [10:14<1:37:22,  2.10s/it] 10%|▉         | 305/3086 [10:16<1:37:54,  2.11s/it] 10%|▉         | 306/3086 [10:18<1:38:05,  2.12s/it] 10%|▉         | 307/3086 [10:21<1:39:16,  2.14s/it] 10%|▉         | 308/3086 [10:22<1:33:45,  2.02s/it] 10%|█         | 309/3086 [10:24<1:27:38,  1.89s/it] 10%|█         | 310/3086 [10:26<1:24:15,  1.82s/it]                                                    {'loss': 0.6074, 'grad_norm': 0.14084206521511078, 'learning_rate': 0.0005397278029812053, 'epoch': 0.1}
 10%|█         | 310/3086 [10:26<1:24:15,  1.82s/it] 10%|█         | 311/3086 [10:27<1:23:56,  1.82s/it] 10%|█         | 312/3086 [10:29<1:25:52,  1.86s/it] 10%|█         | 313/3086 [10:31<1:23:20,  1.80s/it] 10%|█         | 314/3086 [10:33<1:24:02,  1.82s/it] 10%|█         | 315/3086 [10:35<1:29:32,  1.94s/it] 10%|█         | 316/3086 [10:37<1:26:12,  1.87s/it] 10%|█         | 317/3086 [10:39<1:27:07,  1.89s/it] 10%|█         | 318/3086 [10:41<1:28:53,  1.93s/it] 10%|█         | 319/3086 [10:43<1:29:47,  1.95s/it] 10%|█         | 320/3086 [10:45<1:29:54,  1.95s/it]                                                    {'loss': 0.583, 'grad_norm': 0.11240861564874649, 'learning_rate': 0.0005377835385612443, 'epoch': 0.1}
 10%|█         | 320/3086 [10:45<1:29:54,  1.95s/it] 10%|█         | 321/3086 [10:47<1:31:11,  1.98s/it] 10%|█         | 322/3086 [10:48<1:27:37,  1.90s/it] 10%|█         | 323/3086 [10:50<1:25:09,  1.85s/it] 10%|█         | 324/3086 [10:52<1:26:36,  1.88s/it] 11%|█         | 325/3086 [10:54<1:23:43,  1.82s/it] 11%|█         | 326/3086 [10:56<1:30:43,  1.97s/it] 11%|█         | 327/3086 [10:58<1:27:58,  1.91s/it] 11%|█         | 328/3086 [11:00<1:29:12,  1.94s/it] 11%|█         | 329/3086 [11:02<1:33:47,  2.04s/it] 11%|█         | 330/3086 [11:04<1:34:05,  2.05s/it]                                                    {'loss': 0.5983, 'grad_norm': 0.12192139774560928, 'learning_rate': 0.0005358392741412831, 'epoch': 0.11}
 11%|█         | 330/3086 [11:04<1:34:05,  2.05s/it] 11%|█         | 331/3086 [11:06<1:35:58,  2.09s/it] 11%|█         | 332/3086 [11:08<1:32:32,  2.02s/it] 11%|█         | 333/3086 [11:10<1:28:39,  1.93s/it] 11%|█         | 334/3086 [11:12<1:34:49,  2.07s/it] 11%|█         | 335/3086 [11:14<1:33:22,  2.04s/it] 11%|█         | 336/3086 [11:16<1:34:07,  2.05s/it] 11%|█         | 337/3086 [11:18<1:31:28,  2.00s/it] 11%|█         | 338/3086 [11:20<1:27:16,  1.91s/it] 11%|█         | 339/3086 [11:22<1:28:47,  1.94s/it] 11%|█         | 340/3086 [11:24<1:33:03,  2.03s/it]                                                    {'loss': 0.5867, 'grad_norm': 0.11849284172058105, 'learning_rate': 0.000533895009721322, 'epoch': 0.11}
 11%|█         | 340/3086 [11:24<1:33:03,  2.03s/it] 11%|█         | 341/3086 [11:26<1:30:12,  1.97s/it] 11%|█         | 342/3086 [11:28<1:30:01,  1.97s/it] 11%|█         | 343/3086 [11:30<1:23:50,  1.83s/it] 11%|█         | 344/3086 [11:31<1:20:42,  1.77s/it] 11%|█         | 345/3086 [11:34<1:28:44,  1.94s/it] 11%|█         | 346/3086 [11:36<1:34:36,  2.07s/it] 11%|█         | 347/3086 [11:38<1:29:43,  1.97s/it] 11%|█▏        | 348/3086 [11:40<1:30:39,  1.99s/it] 11%|█▏        | 349/3086 [11:42<1:30:45,  1.99s/it] 11%|█▏        | 350/3086 [11:44<1:32:51,  2.04s/it]                                                    {'loss': 0.5979, 'grad_norm': 0.07611962407827377, 'learning_rate': 0.000531950745301361, 'epoch': 0.11}
 11%|█▏        | 350/3086 [11:44<1:32:51,  2.04s/it] 11%|█▏        | 351/3086 [11:46<1:28:31,  1.94s/it] 11%|█▏        | 352/3086 [11:47<1:28:11,  1.94s/it] 11%|█▏        | 353/3086 [11:49<1:28:06,  1.93s/it] 11%|█▏        | 354/3086 [11:52<1:31:05,  2.00s/it] 12%|█▏        | 355/3086 [11:53<1:28:24,  1.94s/it] 12%|█▏        | 356/3086 [11:55<1:28:18,  1.94s/it] 12%|█▏        | 357/3086 [11:57<1:23:06,  1.83s/it] 12%|█▏        | 358/3086 [11:59<1:30:16,  1.99s/it] 12%|█▏        | 359/3086 [12:01<1:33:13,  2.05s/it] 12%|█▏        | 360/3086 [12:03<1:26:33,  1.91s/it]                                                    {'loss': 0.5896, 'grad_norm': 0.09109733998775482, 'learning_rate': 0.0005300064808813998, 'epoch': 0.12}
 12%|█▏        | 360/3086 [12:03<1:26:33,  1.91s/it] 12%|█▏        | 361/3086 [12:05<1:30:23,  1.99s/it] 12%|█▏        | 362/3086 [12:07<1:31:43,  2.02s/it] 12%|█▏        | 363/3086 [12:09<1:28:17,  1.95s/it] 12%|█▏        | 364/3086 [12:11<1:29:12,  1.97s/it] 12%|█▏        | 365/3086 [12:13<1:28:57,  1.96s/it] 12%|█▏        | 366/3086 [12:15<1:28:35,  1.95s/it] 12%|█▏        | 367/3086 [12:17<1:31:41,  2.02s/it] 12%|█▏        | 368/3086 [12:19<1:31:19,  2.02s/it] 12%|█▏        | 369/3086 [12:21<1:28:40,  1.96s/it] 12%|█▏        | 370/3086 [12:23<1:27:33,  1.93s/it]                                                    {'loss': 0.5981, 'grad_norm': 0.11720340698957443, 'learning_rate': 0.0005280622164614388, 'epoch': 0.12}
 12%|█▏        | 370/3086 [12:23<1:27:33,  1.93s/it] 12%|█▏        | 371/3086 [12:25<1:29:12,  1.97s/it] 12%|█▏        | 372/3086 [12:27<1:26:23,  1.91s/it] 12%|█▏        | 373/3086 [12:29<1:27:21,  1.93s/it] 12%|█▏        | 374/3086 [12:30<1:24:18,  1.87s/it] 12%|█▏        | 375/3086 [12:33<1:30:21,  2.00s/it] 12%|█▏        | 376/3086 [12:35<1:30:19,  2.00s/it] 12%|█▏        | 377/3086 [12:37<1:29:36,  1.98s/it] 12%|█▏        | 378/3086 [12:39<1:29:40,  1.99s/it] 12%|█▏        | 379/3086 [12:41<1:39:05,  2.20s/it] 12%|█▏        | 380/3086 [12:43<1:37:46,  2.17s/it]                                                    {'loss': 0.5888, 'grad_norm': 0.1142539456486702, 'learning_rate': 0.0005261179520414775, 'epoch': 0.12}
 12%|█▏        | 380/3086 [12:43<1:37:46,  2.17s/it] 12%|█▏        | 381/3086 [12:45<1:34:29,  2.10s/it] 12%|█▏        | 382/3086 [12:47<1:35:24,  2.12s/it] 12%|█▏        | 383/3086 [12:50<1:36:01,  2.13s/it] 12%|█▏        | 384/3086 [12:52<1:36:54,  2.15s/it] 12%|█▏        | 385/3086 [12:54<1:35:45,  2.13s/it] 13%|█▎        | 386/3086 [12:56<1:34:53,  2.11s/it] 13%|█▎        | 387/3086 [12:58<1:37:01,  2.16s/it] 13%|█▎        | 388/3086 [13:00<1:32:02,  2.05s/it] 13%|█▎        | 389/3086 [13:02<1:33:08,  2.07s/it] 13%|█▎        | 390/3086 [13:04<1:26:56,  1.93s/it]                                                    {'loss': 0.5991, 'grad_norm': 0.09277376532554626, 'learning_rate': 0.0005241736876215164, 'epoch': 0.13}
 13%|█▎        | 390/3086 [13:04<1:26:56,  1.93s/it] 13%|█▎        | 391/3086 [13:07<1:39:45,  2.22s/it] 13%|█▎        | 392/3086 [13:09<1:39:03,  2.21s/it] 13%|█▎        | 393/3086 [13:11<1:33:45,  2.09s/it] 13%|█▎        | 394/3086 [13:13<1:37:05,  2.16s/it] 13%|█▎        | 395/3086 [13:15<1:35:58,  2.14s/it] 13%|█▎        | 396/3086 [13:17<1:33:39,  2.09s/it] 13%|█▎        | 397/3086 [13:19<1:25:30,  1.91s/it] 13%|█▎        | 398/3086 [13:20<1:21:50,  1.83s/it] 13%|█▎        | 399/3086 [13:22<1:25:23,  1.91s/it] 13%|█▎        | 400/3086 [13:25<1:30:35,  2.02s/it]                                                    {'loss': 0.5926, 'grad_norm': 0.08413265645503998, 'learning_rate': 0.0005222294232015554, 'epoch': 0.13}
 13%|█▎        | 400/3086 [13:25<1:30:35,  2.02s/it] 13%|█▎        | 401/3086 [13:27<1:32:45,  2.07s/it] 13%|█▎        | 402/3086 [13:29<1:33:31,  2.09s/it] 13%|█▎        | 403/3086 [13:31<1:28:33,  1.98s/it] 13%|█▎        | 404/3086 [13:33<1:33:55,  2.10s/it] 13%|█▎        | 405/3086 [13:35<1:33:19,  2.09s/it] 13%|█▎        | 406/3086 [13:37<1:35:47,  2.14s/it] 13%|█▎        | 407/3086 [13:40<1:38:20,  2.20s/it] 13%|█▎        | 408/3086 [13:42<1:36:44,  2.17s/it] 13%|█▎        | 409/3086 [13:44<1:33:55,  2.11s/it] 13%|█▎        | 410/3086 [13:45<1:26:27,  1.94s/it]                                                    {'loss': 0.5749, 'grad_norm': 0.10812919586896896, 'learning_rate': 0.0005202851587815942, 'epoch': 0.13}
 13%|█▎        | 410/3086 [13:45<1:26:27,  1.94s/it] 13%|█▎        | 411/3086 [13:48<1:31:16,  2.05s/it] 13%|█▎        | 412/3086 [13:49<1:27:11,  1.96s/it] 13%|█▎        | 413/3086 [13:51<1:24:23,  1.89s/it] 13%|█▎        | 414/3086 [13:53<1:22:44,  1.86s/it] 13%|█▎        | 415/3086 [13:55<1:28:30,  1.99s/it] 13%|█▎        | 416/3086 [13:57<1:33:09,  2.09s/it] 14%|█▎        | 417/3086 [13:59<1:28:26,  1.99s/it] 14%|█▎        | 418/3086 [14:01<1:28:02,  1.98s/it] 14%|█▎        | 419/3086 [14:03<1:29:51,  2.02s/it] 14%|█▎        | 420/3086 [14:06<1:37:55,  2.20s/it]                                                    {'loss': 0.5872, 'grad_norm': 0.11608872562646866, 'learning_rate': 0.0005183408943616331, 'epoch': 0.14}
 14%|█▎        | 420/3086 [14:06<1:37:55,  2.20s/it] 14%|█▎        | 421/3086 [14:08<1:33:11,  2.10s/it] 14%|█▎        | 422/3086 [14:10<1:30:20,  2.03s/it] 14%|█▎        | 423/3086 [14:11<1:24:36,  1.91s/it] 14%|█▎        | 424/3086 [14:14<1:31:32,  2.06s/it] 14%|█▍        | 425/3086 [14:16<1:38:07,  2.21s/it] 14%|█▍        | 426/3086 [14:18<1:32:15,  2.08s/it] 14%|█▍        | 427/3086 [14:20<1:37:14,  2.19s/it] 14%|█▍        | 428/3086 [14:22<1:32:13,  2.08s/it] 14%|█▍        | 429/3086 [14:24<1:32:22,  2.09s/it] 14%|█▍        | 430/3086 [14:27<1:33:46,  2.12s/it]                                                    {'loss': 0.5891, 'grad_norm': 0.10682464390993118, 'learning_rate': 0.0005163966299416721, 'epoch': 0.14}
 14%|█▍        | 430/3086 [14:27<1:33:46,  2.12s/it] 14%|█▍        | 431/3086 [14:28<1:30:13,  2.04s/it] 14%|█▍        | 432/3086 [14:31<1:37:33,  2.21s/it] 14%|█▍        | 433/3086 [14:33<1:33:09,  2.11s/it] 14%|█▍        | 434/3086 [14:35<1:29:40,  2.03s/it] 14%|█▍        | 435/3086 [14:37<1:35:42,  2.17s/it] 14%|█▍        | 436/3086 [14:39<1:31:57,  2.08s/it] 14%|█▍        | 437/3086 [14:42<1:36:46,  2.19s/it] 14%|█▍        | 438/3086 [14:43<1:30:05,  2.04s/it] 14%|█▍        | 439/3086 [14:45<1:29:50,  2.04s/it] 14%|█▍        | 440/3086 [14:47<1:25:26,  1.94s/it]                                                    {'loss': 0.5699, 'grad_norm': 0.10331655293703079, 'learning_rate': 0.0005144523655217109, 'epoch': 0.14}
 14%|█▍        | 440/3086 [14:47<1:25:26,  1.94s/it] 14%|█▍        | 441/3086 [14:49<1:31:53,  2.08s/it] 14%|█▍        | 442/3086 [14:51<1:28:00,  2.00s/it] 14%|█▍        | 443/3086 [14:54<1:32:59,  2.11s/it] 14%|█▍        | 444/3086 [14:56<1:35:36,  2.17s/it] 14%|█▍        | 445/3086 [14:58<1:29:47,  2.04s/it] 14%|█▍        | 446/3086 [15:00<1:27:51,  2.00s/it] 14%|█▍        | 447/3086 [15:01<1:27:08,  1.98s/it] 15%|█▍        | 448/3086 [15:03<1:22:50,  1.88s/it] 15%|█▍        | 449/3086 [15:05<1:24:35,  1.92s/it] 15%|█▍        | 450/3086 [15:07<1:19:45,  1.82s/it]                                                    {'loss': 0.5876, 'grad_norm': 0.097168929874897, 'learning_rate': 0.0005125081011017499, 'epoch': 0.15}
 15%|█▍        | 450/3086 [15:07<1:19:45,  1.82s/it] 15%|█▍        | 451/3086 [15:09<1:23:53,  1.91s/it] 15%|█▍        | 452/3086 [15:10<1:18:02,  1.78s/it] 15%|█▍        | 453/3086 [15:12<1:19:33,  1.81s/it] 15%|█▍        | 454/3086 [15:14<1:23:54,  1.91s/it] 15%|█▍        | 455/3086 [15:17<1:27:34,  2.00s/it] 15%|█▍        | 456/3086 [15:19<1:28:45,  2.02s/it] 15%|█▍        | 457/3086 [15:21<1:33:49,  2.14s/it] 15%|█▍        | 458/3086 [15:23<1:28:14,  2.01s/it] 15%|█▍        | 459/3086 [15:24<1:23:46,  1.91s/it] 15%|█▍        | 460/3086 [15:26<1:21:31,  1.86s/it]                                                    {'loss': 0.5834, 'grad_norm': 0.09814725816249847, 'learning_rate': 0.0005105638366817886, 'epoch': 0.15}
 15%|█▍        | 460/3086 [15:26<1:21:31,  1.86s/it] 15%|█▍        | 461/3086 [15:28<1:23:50,  1.92s/it] 15%|█▍        | 462/3086 [15:30<1:20:24,  1.84s/it] 15%|█▌        | 463/3086 [15:32<1:20:00,  1.83s/it] 15%|█▌        | 464/3086 [15:34<1:27:42,  2.01s/it] 15%|█▌        | 465/3086 [15:36<1:30:32,  2.07s/it] 15%|█▌        | 466/3086 [15:39<1:32:45,  2.12s/it] 15%|█▌        | 467/3086 [15:40<1:27:02,  1.99s/it] 15%|█▌        | 468/3086 [15:43<1:31:57,  2.11s/it] 15%|█▌        | 469/3086 [15:44<1:26:02,  1.97s/it] 15%|█▌        | 470/3086 [15:47<1:29:25,  2.05s/it]                                                    {'loss': 0.5824, 'grad_norm': 0.0939226746559143, 'learning_rate': 0.0005086195722618275, 'epoch': 0.15}
 15%|█▌        | 470/3086 [15:47<1:29:25,  2.05s/it] 15%|█▌        | 471/3086 [15:48<1:23:11,  1.91s/it] 15%|█▌        | 472/3086 [15:50<1:21:31,  1.87s/it] 15%|█▌        | 473/3086 [15:52<1:19:14,  1.82s/it] 15%|█▌        | 474/3086 [15:53<1:20:14,  1.84s/it] 15%|█▌        | 475/3086 [15:56<1:22:52,  1.90s/it] 15%|█▌        | 476/3086 [15:57<1:21:03,  1.86s/it] 15%|█▌        | 477/3086 [15:59<1:24:11,  1.94s/it] 15%|█▌        | 478/3086 [16:02<1:26:47,  2.00s/it] 16%|█▌        | 479/3086 [16:04<1:26:55,  2.00s/it] 16%|█▌        | 480/3086 [16:06<1:28:10,  2.03s/it]                                                    {'loss': 0.5879, 'grad_norm': 0.1125536859035492, 'learning_rate': 0.0005066753078418665, 'epoch': 0.16}
 16%|█▌        | 480/3086 [16:06<1:28:10,  2.03s/it] 16%|█▌        | 481/3086 [16:08<1:25:54,  1.98s/it] 16%|█▌        | 482/3086 [16:09<1:24:31,  1.95s/it] 16%|█▌        | 483/3086 [16:11<1:24:38,  1.95s/it] 16%|█▌        | 484/3086 [16:13<1:25:04,  1.96s/it] 16%|█▌        | 485/3086 [16:15<1:24:59,  1.96s/it] 16%|█▌        | 486/3086 [16:17<1:23:45,  1.93s/it] 16%|█▌        | 487/3086 [16:19<1:20:51,  1.87s/it] 16%|█▌        | 488/3086 [16:21<1:19:47,  1.84s/it] 16%|█▌        | 489/3086 [16:23<1:22:01,  1.90s/it] 16%|█▌        | 490/3086 [16:25<1:25:26,  1.97s/it]                                                    {'loss': 0.5733, 'grad_norm': 0.07950711995363235, 'learning_rate': 0.0005047310434219053, 'epoch': 0.16}
 16%|█▌        | 490/3086 [16:25<1:25:26,  1.97s/it] 16%|█▌        | 491/3086 [16:27<1:24:48,  1.96s/it] 16%|█▌        | 492/3086 [16:29<1:22:51,  1.92s/it] 16%|█▌        | 493/3086 [16:30<1:19:32,  1.84s/it] 16%|█▌        | 494/3086 [16:32<1:22:10,  1.90s/it] 16%|█▌        | 495/3086 [16:34<1:18:01,  1.81s/it] 16%|█▌        | 496/3086 [16:36<1:21:34,  1.89s/it] 16%|█▌        | 497/3086 [16:37<1:16:25,  1.77s/it] 16%|█▌        | 498/3086 [16:39<1:17:39,  1.80s/it] 16%|█▌        | 499/3086 [16:41<1:18:22,  1.82s/it] 16%|█▌        | 500/3086 [16:44<1:32:50,  2.15s/it]                                                    {'loss': 0.5845, 'grad_norm': 0.07951507717370987, 'learning_rate': 0.0005027867790019442, 'epoch': 0.16}
 16%|█▌        | 500/3086 [16:44<1:32:50,  2.15s/it] 16%|█▌        | 501/3086 [16:46<1:29:12,  2.07s/it] 16%|█▋        | 502/3086 [16:48<1:24:10,  1.95s/it] 16%|█▋        | 503/3086 [16:49<1:20:07,  1.86s/it] 16%|█▋        | 504/3086 [16:51<1:18:52,  1.83s/it] 16%|█▋        | 505/3086 [16:53<1:24:22,  1.96s/it] 16%|█▋        | 506/3086 [16:55<1:22:11,  1.91s/it] 16%|█▋        | 507/3086 [16:57<1:24:31,  1.97s/it] 16%|█▋        | 508/3086 [16:59<1:23:42,  1.95s/it] 16%|█▋        | 509/3086 [17:01<1:21:03,  1.89s/it] 17%|█▋        | 510/3086 [17:03<1:19:31,  1.85s/it]                                                    {'loss': 0.5826, 'grad_norm': 0.09335750341415405, 'learning_rate': 0.0005008425145819831, 'epoch': 0.17}
 17%|█▋        | 510/3086 [17:03<1:19:31,  1.85s/it] 17%|█▋        | 511/3086 [17:04<1:17:25,  1.80s/it] 17%|█▋        | 512/3086 [17:06<1:17:36,  1.81s/it] 17%|█▋        | 513/3086 [17:08<1:18:22,  1.83s/it] 17%|█▋        | 514/3086 [17:10<1:22:37,  1.93s/it] 17%|█▋        | 515/3086 [17:12<1:20:29,  1.88s/it] 17%|█▋        | 516/3086 [17:15<1:36:10,  2.25s/it] 17%|█▋        | 517/3086 [17:17<1:27:21,  2.04s/it] 17%|█▋        | 518/3086 [17:19<1:28:42,  2.07s/it] 17%|█▋        | 519/3086 [17:21<1:35:19,  2.23s/it] 17%|█▋        | 520/3086 [17:23<1:31:50,  2.15s/it]                                                    {'loss': 0.5709, 'grad_norm': 0.08313937485218048, 'learning_rate': 0.000498898250162022, 'epoch': 0.17}
 17%|█▋        | 520/3086 [17:23<1:31:50,  2.15s/it] 17%|█▋        | 521/3086 [17:25<1:31:48,  2.15s/it] 17%|█▋        | 522/3086 [17:28<1:35:51,  2.24s/it] 17%|█▋        | 523/3086 [17:30<1:38:09,  2.30s/it] 17%|█▋        | 524/3086 [17:33<1:37:34,  2.29s/it] 17%|█▋        | 525/3086 [17:35<1:33:52,  2.20s/it] 17%|█▋        | 526/3086 [17:37<1:30:36,  2.12s/it] 17%|█▋        | 527/3086 [17:39<1:34:30,  2.22s/it] 17%|█▋        | 528/3086 [17:41<1:35:37,  2.24s/it] 17%|█▋        | 529/3086 [17:43<1:34:32,  2.22s/it] 17%|█▋        | 530/3086 [17:46<1:32:58,  2.18s/it]                                                    {'loss': 0.5733, 'grad_norm': 0.10039504617452621, 'learning_rate': 0.0004969539857420609, 'epoch': 0.17}
 17%|█▋        | 530/3086 [17:46<1:32:58,  2.18s/it] 17%|█▋        | 531/3086 [17:48<1:33:17,  2.19s/it] 17%|█▋        | 532/3086 [17:50<1:32:32,  2.17s/it] 17%|█▋        | 533/3086 [17:52<1:32:13,  2.17s/it] 17%|█▋        | 534/3086 [17:54<1:31:16,  2.15s/it] 17%|█▋        | 535/3086 [17:57<1:34:59,  2.23s/it] 17%|█▋        | 536/3086 [17:58<1:30:04,  2.12s/it] 17%|█▋        | 537/3086 [18:00<1:23:40,  1.97s/it] 17%|█▋        | 538/3086 [18:02<1:23:08,  1.96s/it] 17%|█▋        | 539/3086 [18:04<1:25:48,  2.02s/it] 17%|█▋        | 540/3086 [18:06<1:21:26,  1.92s/it]                                                    {'loss': 0.5865, 'grad_norm': 0.115457683801651, 'learning_rate': 0.0004950097213220997, 'epoch': 0.17}
 17%|█▋        | 540/3086 [18:06<1:21:26,  1.92s/it] 18%|█▊        | 541/3086 [18:08<1:22:56,  1.96s/it] 18%|█▊        | 542/3086 [18:10<1:26:34,  2.04s/it] 18%|█▊        | 543/3086 [18:12<1:25:14,  2.01s/it] 18%|█▊        | 544/3086 [18:14<1:21:15,  1.92s/it] 18%|█▊        | 545/3086 [18:16<1:19:11,  1.87s/it] 18%|█▊        | 546/3086 [18:17<1:16:56,  1.82s/it] 18%|█▊        | 547/3086 [18:19<1:13:21,  1.73s/it] 18%|█▊        | 548/3086 [18:21<1:21:12,  1.92s/it] 18%|█▊        | 549/3086 [18:23<1:17:15,  1.83s/it] 18%|█▊        | 550/3086 [18:25<1:17:16,  1.83s/it]                                                    {'loss': 0.5758, 'grad_norm': 0.09686410427093506, 'learning_rate': 0.0004930654569021386, 'epoch': 0.18}
 18%|█▊        | 550/3086 [18:25<1:17:16,  1.83s/it] 18%|█▊        | 551/3086 [18:27<1:20:16,  1.90s/it] 18%|█▊        | 552/3086 [18:28<1:16:49,  1.82s/it] 18%|█▊        | 553/3086 [18:30<1:16:53,  1.82s/it] 18%|█▊        | 554/3086 [18:32<1:13:47,  1.75s/it] 18%|█▊        | 555/3086 [18:33<1:13:35,  1.74s/it] 18%|█▊        | 556/3086 [18:35<1:11:59,  1.71s/it] 18%|█▊        | 557/3086 [18:37<1:15:26,  1.79s/it] 18%|█▊        | 558/3086 [18:39<1:15:30,  1.79s/it] 18%|█▊        | 559/3086 [18:41<1:17:07,  1.83s/it] 18%|█▊        | 560/3086 [18:43<1:21:58,  1.95s/it]                                                    {'loss': 0.5665, 'grad_norm': 0.08598525077104568, 'learning_rate': 0.0004911211924821775, 'epoch': 0.18}
 18%|█▊        | 560/3086 [18:43<1:21:58,  1.95s/it] 18%|█▊        | 561/3086 [18:45<1:20:47,  1.92s/it] 18%|█▊        | 562/3086 [18:47<1:21:04,  1.93s/it] 18%|█▊        | 563/3086 [18:49<1:20:35,  1.92s/it] 18%|█▊        | 564/3086 [18:50<1:19:52,  1.90s/it] 18%|█▊        | 565/3086 [18:52<1:17:32,  1.85s/it] 18%|█▊        | 566/3086 [18:54<1:17:09,  1.84s/it] 18%|█▊        | 567/3086 [18:56<1:17:09,  1.84s/it] 18%|█▊        | 568/3086 [18:58<1:17:31,  1.85s/it] 18%|█▊        | 569/3086 [19:00<1:18:28,  1.87s/it] 18%|█▊        | 570/3086 [19:01<1:16:33,  1.83s/it]                                                    {'loss': 0.5729, 'grad_norm': 0.08496133983135223, 'learning_rate': 0.0004891769280622164, 'epoch': 0.18}
 18%|█▊        | 570/3086 [19:01<1:16:33,  1.83s/it] 19%|█▊        | 571/3086 [19:04<1:20:41,  1.93s/it] 19%|█▊        | 572/3086 [19:05<1:19:07,  1.89s/it] 19%|█▊        | 573/3086 [19:07<1:18:07,  1.87s/it] 19%|█▊        | 574/3086 [19:10<1:25:15,  2.04s/it] 19%|█▊        | 575/3086 [19:12<1:32:49,  2.22s/it] 19%|█▊        | 576/3086 [19:14<1:24:48,  2.03s/it] 19%|█▊        | 577/3086 [19:16<1:26:52,  2.08s/it] 19%|█▊        | 578/3086 [19:18<1:25:37,  2.05s/it] 19%|█▉        | 579/3086 [19:20<1:27:16,  2.09s/it] 19%|█▉        | 580/3086 [19:22<1:20:55,  1.94s/it]                                                    {'loss': 0.5761, 'grad_norm': 0.0845751091837883, 'learning_rate': 0.0004872326636422553, 'epoch': 0.19}
 19%|█▉        | 580/3086 [19:22<1:20:55,  1.94s/it] 19%|█▉        | 581/3086 [19:24<1:22:53,  1.99s/it] 19%|█▉        | 582/3086 [19:26<1:19:20,  1.90s/it] 19%|█▉        | 583/3086 [19:28<1:22:36,  1.98s/it] 19%|█▉        | 584/3086 [19:30<1:25:47,  2.06s/it] 19%|█▉        | 585/3086 [19:32<1:21:08,  1.95s/it] 19%|█▉        | 586/3086 [19:34<1:25:07,  2.04s/it] 19%|█▉        | 587/3086 [19:36<1:23:30,  2.01s/it] 19%|█▉        | 588/3086 [19:38<1:23:18,  2.00s/it] 19%|█▉        | 589/3086 [19:40<1:23:28,  2.01s/it] 19%|█▉        | 590/3086 [19:42<1:24:54,  2.04s/it]                                                    {'loss': 0.5642, 'grad_norm': 0.0957777127623558, 'learning_rate': 0.0004852883992222942, 'epoch': 0.19}
 19%|█▉        | 590/3086 [19:42<1:24:54,  2.04s/it] 19%|█▉        | 591/3086 [19:44<1:19:49,  1.92s/it] 19%|█▉        | 592/3086 [19:45<1:18:20,  1.88s/it] 19%|█▉        | 593/3086 [19:48<1:22:13,  1.98s/it] 19%|█▉        | 594/3086 [19:49<1:21:00,  1.95s/it] 19%|█▉        | 595/3086 [19:52<1:22:30,  1.99s/it] 19%|█▉        | 596/3086 [19:53<1:20:00,  1.93s/it] 19%|█▉        | 597/3086 [19:56<1:22:43,  1.99s/it] 19%|█▉        | 598/3086 [19:58<1:22:47,  2.00s/it] 19%|█▉        | 599/3086 [20:00<1:31:41,  2.21s/it] 19%|█▉        | 600/3086 [20:03<1:33:46,  2.26s/it]                                                    {'loss': 0.5687, 'grad_norm': 0.0961141511797905, 'learning_rate': 0.00048334413480233306, 'epoch': 0.19}
 19%|█▉        | 600/3086 [20:03<1:33:46,  2.26s/it] 19%|█▉        | 601/3086 [20:05<1:32:48,  2.24s/it] 20%|█▉        | 602/3086 [20:07<1:27:39,  2.12s/it] 20%|█▉        | 603/3086 [20:09<1:31:05,  2.20s/it] 20%|█▉        | 604/3086 [20:12<1:34:48,  2.29s/it] 20%|█▉        | 605/3086 [20:13<1:30:09,  2.18s/it] 20%|█▉        | 606/3086 [20:15<1:26:25,  2.09s/it] 20%|█▉        | 607/3086 [20:18<1:30:01,  2.18s/it] 20%|█▉        | 608/3086 [20:21<1:39:51,  2.42s/it] 20%|█▉        | 609/3086 [20:22<1:31:02,  2.21s/it] 20%|█▉        | 610/3086 [20:24<1:29:38,  2.17s/it]                                                    {'loss': 0.5642, 'grad_norm': 0.12835882604122162, 'learning_rate': 0.00048139987038237195, 'epoch': 0.2}
 20%|█▉        | 610/3086 [20:24<1:29:38,  2.17s/it] 20%|█▉        | 611/3086 [20:27<1:31:19,  2.21s/it] 20%|█▉        | 612/3086 [20:29<1:31:21,  2.22s/it] 20%|█▉        | 613/3086 [20:31<1:27:15,  2.12s/it] 20%|█▉        | 614/3086 [20:34<1:36:11,  2.33s/it] 20%|█▉        | 615/3086 [20:36<1:36:00,  2.33s/it] 20%|█▉        | 616/3086 [20:38<1:29:42,  2.18s/it] 20%|█▉        | 617/3086 [20:40<1:22:50,  2.01s/it] 20%|██        | 618/3086 [20:42<1:25:45,  2.08s/it] 20%|██        | 619/3086 [20:44<1:22:38,  2.01s/it] 20%|██        | 620/3086 [20:45<1:19:13,  1.93s/it]                                                    {'loss': 0.5796, 'grad_norm': 0.0901157483458519, 'learning_rate': 0.00047945560596241083, 'epoch': 0.2}
 20%|██        | 620/3086 [20:45<1:19:13,  1.93s/it] 20%|██        | 621/3086 [20:47<1:17:50,  1.89s/it] 20%|██        | 622/3086 [20:50<1:24:53,  2.07s/it] 20%|██        | 623/3086 [20:52<1:24:16,  2.05s/it] 20%|██        | 624/3086 [20:53<1:20:57,  1.97s/it] 20%|██        | 625/3086 [20:55<1:20:38,  1.97s/it] 20%|██        | 626/3086 [20:57<1:15:55,  1.85s/it] 20%|██        | 627/3086 [20:59<1:20:33,  1.97s/it] 20%|██        | 628/3086 [21:01<1:20:07,  1.96s/it] 20%|██        | 629/3086 [21:03<1:23:30,  2.04s/it] 20%|██        | 630/3086 [21:06<1:24:37,  2.07s/it]                                                    {'loss': 0.5688, 'grad_norm': 0.09812746196985245, 'learning_rate': 0.0004775113415424497, 'epoch': 0.2}
 20%|██        | 630/3086 [21:06<1:24:37,  2.07s/it] 20%|██        | 631/3086 [21:07<1:22:33,  2.02s/it] 20%|██        | 632/3086 [21:10<1:25:23,  2.09s/it] 21%|██        | 633/3086 [21:12<1:22:26,  2.02s/it] 21%|██        | 634/3086 [21:14<1:29:53,  2.20s/it] 21%|██        | 635/3086 [21:16<1:27:59,  2.15s/it] 21%|██        | 636/3086 [21:18<1:24:53,  2.08s/it] 21%|██        | 637/3086 [21:20<1:28:33,  2.17s/it] 21%|██        | 638/3086 [21:23<1:33:45,  2.30s/it] 21%|██        | 639/3086 [21:25<1:34:26,  2.32s/it] 21%|██        | 640/3086 [21:27<1:26:17,  2.12s/it]                                                    {'loss': 0.5667, 'grad_norm': 0.08343622833490372, 'learning_rate': 0.00047556707712248866, 'epoch': 0.21}
 21%|██        | 640/3086 [21:27<1:26:17,  2.12s/it] 21%|██        | 641/3086 [21:29<1:22:13,  2.02s/it] 21%|██        | 642/3086 [21:31<1:23:01,  2.04s/it] 21%|██        | 643/3086 [21:33<1:27:31,  2.15s/it] 21%|██        | 644/3086 [21:35<1:26:57,  2.14s/it] 21%|██        | 645/3086 [21:38<1:27:44,  2.16s/it] 21%|██        | 646/3086 [21:40<1:29:18,  2.20s/it] 21%|██        | 647/3086 [21:42<1:32:30,  2.28s/it] 21%|██        | 648/3086 [21:44<1:28:55,  2.19s/it] 21%|██        | 649/3086 [21:46<1:22:34,  2.03s/it] 21%|██        | 650/3086 [21:48<1:20:46,  1.99s/it]                                                    {'loss': 0.5747, 'grad_norm': 0.0857514888048172, 'learning_rate': 0.0004736228127025275, 'epoch': 0.21}
 21%|██        | 650/3086 [21:48<1:20:46,  1.99s/it] 21%|██        | 651/3086 [21:50<1:15:55,  1.87s/it] 21%|██        | 652/3086 [21:51<1:13:51,  1.82s/it] 21%|██        | 653/3086 [21:53<1:14:58,  1.85s/it] 21%|██        | 654/3086 [21:55<1:15:05,  1.85s/it] 21%|██        | 655/3086 [21:57<1:20:13,  1.98s/it] 21%|██▏       | 656/3086 [21:59<1:18:49,  1.95s/it] 21%|██▏       | 657/3086 [22:01<1:20:51,  2.00s/it] 21%|██▏       | 658/3086 [22:04<1:23:48,  2.07s/it] 21%|██▏       | 659/3086 [22:07<1:36:19,  2.38s/it] 21%|██▏       | 660/3086 [22:09<1:30:02,  2.23s/it]                                                    {'loss': 0.5605, 'grad_norm': 0.08363264054059982, 'learning_rate': 0.0004716785482825664, 'epoch': 0.21}
 21%|██▏       | 660/3086 [22:09<1:30:02,  2.23s/it] 21%|██▏       | 661/3086 [22:10<1:25:25,  2.11s/it] 21%|██▏       | 662/3086 [22:12<1:20:48,  2.00s/it] 21%|██▏       | 663/3086 [22:14<1:17:08,  1.91s/it] 22%|██▏       | 664/3086 [22:16<1:19:08,  1.96s/it] 22%|██▏       | 665/3086 [22:18<1:20:21,  1.99s/it] 22%|██▏       | 666/3086 [22:20<1:18:10,  1.94s/it] 22%|██▏       | 667/3086 [22:22<1:25:23,  2.12s/it] 22%|██▏       | 668/3086 [22:25<1:33:06,  2.31s/it] 22%|██▏       | 669/3086 [22:27<1:32:37,  2.30s/it] 22%|██▏       | 670/3086 [22:29<1:28:05,  2.19s/it]                                                    {'loss': 0.5631, 'grad_norm': 0.12030868977308273, 'learning_rate': 0.0004697342838626053, 'epoch': 0.22}
 22%|██▏       | 670/3086 [22:29<1:28:05,  2.19s/it] 22%|██▏       | 671/3086 [22:31<1:26:09,  2.14s/it] 22%|██▏       | 672/3086 [22:33<1:22:54,  2.06s/it] 22%|██▏       | 673/3086 [22:35<1:23:12,  2.07s/it] 22%|██▏       | 674/3086 [22:37<1:20:11,  1.99s/it] 22%|██▏       | 675/3086 [22:39<1:22:44,  2.06s/it] 22%|██▏       | 676/3086 [22:41<1:19:32,  1.98s/it] 22%|██▏       | 677/3086 [22:43<1:15:35,  1.88s/it] 22%|██▏       | 678/3086 [22:45<1:15:08,  1.87s/it] 22%|██▏       | 679/3086 [22:47<1:15:58,  1.89s/it] 22%|██▏       | 680/3086 [22:48<1:14:55,  1.87s/it]                                                    {'loss': 0.5629, 'grad_norm': 0.09285441040992737, 'learning_rate': 0.00046779001944264416, 'epoch': 0.22}
 22%|██▏       | 680/3086 [22:48<1:14:55,  1.87s/it] 22%|██▏       | 681/3086 [22:50<1:13:15,  1.83s/it] 22%|██▏       | 682/3086 [22:52<1:11:38,  1.79s/it] 22%|██▏       | 683/3086 [22:54<1:16:54,  1.92s/it] 22%|██▏       | 684/3086 [22:56<1:13:17,  1.83s/it] 22%|██▏       | 685/3086 [22:57<1:11:04,  1.78s/it] 22%|██▏       | 686/3086 [22:59<1:13:03,  1.83s/it] 22%|██▏       | 687/3086 [23:01<1:13:06,  1.83s/it] 22%|██▏       | 688/3086 [23:03<1:15:32,  1.89s/it] 22%|██▏       | 689/3086 [23:05<1:11:34,  1.79s/it] 22%|██▏       | 690/3086 [23:07<1:19:58,  2.00s/it]                                                    {'loss': 0.5694, 'grad_norm': 0.08140670508146286, 'learning_rate': 0.00046584575502268304, 'epoch': 0.22}
 22%|██▏       | 690/3086 [23:07<1:19:58,  2.00s/it] 22%|██▏       | 691/3086 [23:09<1:18:55,  1.98s/it] 22%|██▏       | 692/3086 [23:11<1:22:22,  2.06s/it] 22%|██▏       | 693/3086 [23:13<1:17:50,  1.95s/it] 22%|██▏       | 694/3086 [23:15<1:19:37,  2.00s/it] 23%|██▎       | 695/3086 [23:17<1:18:34,  1.97s/it] 23%|██▎       | 696/3086 [23:19<1:17:14,  1.94s/it] 23%|██▎       | 697/3086 [23:21<1:15:52,  1.91s/it] 23%|██▎       | 698/3086 [23:23<1:17:21,  1.94s/it] 23%|██▎       | 699/3086 [23:25<1:17:19,  1.94s/it] 23%|██▎       | 700/3086 [23:27<1:18:20,  1.97s/it]                                                    {'loss': 0.5678, 'grad_norm': 0.08466312289237976, 'learning_rate': 0.00046390149060272193, 'epoch': 0.23}
 23%|██▎       | 700/3086 [23:27<1:18:20,  1.97s/it] 23%|██▎       | 701/3086 [23:29<1:18:07,  1.97s/it] 23%|██▎       | 702/3086 [23:31<1:18:36,  1.98s/it] 23%|██▎       | 703/3086 [23:33<1:18:00,  1.96s/it] 23%|██▎       | 704/3086 [23:35<1:17:13,  1.95s/it] 23%|██▎       | 705/3086 [23:36<1:16:13,  1.92s/it] 23%|██▎       | 706/3086 [23:39<1:20:07,  2.02s/it] 23%|██▎       | 707/3086 [23:41<1:21:49,  2.06s/it] 23%|██▎       | 708/3086 [23:43<1:18:47,  1.99s/it] 23%|██▎       | 709/3086 [23:45<1:19:24,  2.00s/it] 23%|██▎       | 710/3086 [23:47<1:18:55,  1.99s/it]                                                    {'loss': 0.5635, 'grad_norm': 0.08522479236125946, 'learning_rate': 0.0004619572261827608, 'epoch': 0.23}
 23%|██▎       | 710/3086 [23:47<1:18:55,  1.99s/it] 23%|██▎       | 711/3086 [23:49<1:19:32,  2.01s/it] 23%|██▎       | 712/3086 [23:51<1:18:32,  1.99s/it] 23%|██▎       | 713/3086 [23:53<1:21:43,  2.07s/it] 23%|██▎       | 714/3086 [23:55<1:18:14,  1.98s/it] 23%|██▎       | 715/3086 [23:56<1:15:42,  1.92s/it] 23%|██▎       | 716/3086 [23:58<1:12:09,  1.83s/it] 23%|██▎       | 717/3086 [24:00<1:11:48,  1.82s/it] 23%|██▎       | 718/3086 [24:02<1:12:56,  1.85s/it] 23%|██▎       | 719/3086 [24:04<1:12:35,  1.84s/it] 23%|██▎       | 720/3086 [24:05<1:12:31,  1.84s/it]                                                    {'loss': 0.5632, 'grad_norm': 0.08751020580530167, 'learning_rate': 0.00046001296176279965, 'epoch': 0.23}
 23%|██▎       | 720/3086 [24:05<1:12:31,  1.84s/it] 23%|██▎       | 721/3086 [24:07<1:14:04,  1.88s/it] 23%|██▎       | 722/3086 [24:10<1:21:22,  2.07s/it] 23%|██▎       | 723/3086 [24:11<1:16:28,  1.94s/it] 23%|██▎       | 724/3086 [24:14<1:19:23,  2.02s/it] 23%|██▎       | 725/3086 [24:15<1:15:36,  1.92s/it] 24%|██▎       | 726/3086 [24:17<1:13:05,  1.86s/it] 24%|██▎       | 727/3086 [24:19<1:15:18,  1.92s/it] 24%|██▎       | 728/3086 [24:21<1:13:38,  1.87s/it] 24%|██▎       | 729/3086 [24:23<1:15:39,  1.93s/it] 24%|██▎       | 730/3086 [24:25<1:18:14,  1.99s/it]                                                    {'loss': 0.5661, 'grad_norm': 0.07935359328985214, 'learning_rate': 0.0004580686973428386, 'epoch': 0.24}
 24%|██▎       | 730/3086 [24:25<1:18:14,  1.99s/it] 24%|██▎       | 731/3086 [24:27<1:20:21,  2.05s/it] 24%|██▎       | 732/3086 [24:29<1:19:15,  2.02s/it] 24%|██▍       | 733/3086 [24:32<1:23:39,  2.13s/it] 24%|██▍       | 734/3086 [24:34<1:25:04,  2.17s/it] 24%|██▍       | 735/3086 [24:36<1:27:08,  2.22s/it] 24%|██▍       | 736/3086 [24:38<1:21:39,  2.08s/it] 24%|██▍       | 737/3086 [24:40<1:19:14,  2.02s/it] 24%|██▍       | 738/3086 [24:42<1:21:15,  2.08s/it] 24%|██▍       | 739/3086 [24:44<1:19:41,  2.04s/it] 24%|██▍       | 740/3086 [24:46<1:16:25,  1.95s/it]                                                    {'loss': 0.5535, 'grad_norm': 0.08375709503889084, 'learning_rate': 0.0004561244329228775, 'epoch': 0.24}
 24%|██▍       | 740/3086 [24:46<1:16:25,  1.95s/it] 24%|██▍       | 741/3086 [24:48<1:15:17,  1.93s/it] 24%|██▍       | 742/3086 [24:50<1:16:17,  1.95s/it] 24%|██▍       | 743/3086 [24:51<1:14:16,  1.90s/it] 24%|██▍       | 744/3086 [24:53<1:14:25,  1.91s/it] 24%|██▍       | 745/3086 [24:55<1:14:54,  1.92s/it] 24%|██▍       | 746/3086 [24:57<1:15:15,  1.93s/it] 24%|██▍       | 747/3086 [24:59<1:16:27,  1.96s/it] 24%|██▍       | 748/3086 [25:02<1:21:36,  2.09s/it] 24%|██▍       | 749/3086 [25:03<1:16:06,  1.95s/it] 24%|██▍       | 750/3086 [25:05<1:17:03,  1.98s/it]                                                    {'loss': 0.5445, 'grad_norm': 0.09454942494630814, 'learning_rate': 0.00045418016850291636, 'epoch': 0.24}
 24%|██▍       | 750/3086 [25:05<1:17:03,  1.98s/it] 24%|██▍       | 751/3086 [25:07<1:15:22,  1.94s/it] 24%|██▍       | 752/3086 [25:10<1:20:17,  2.06s/it] 24%|██▍       | 753/3086 [25:11<1:16:57,  1.98s/it] 24%|██▍       | 754/3086 [25:14<1:21:01,  2.08s/it] 24%|██▍       | 755/3086 [25:16<1:18:28,  2.02s/it] 24%|██▍       | 756/3086 [25:17<1:15:47,  1.95s/it] 25%|██▍       | 757/3086 [25:19<1:14:20,  1.92s/it] 25%|██▍       | 758/3086 [25:21<1:09:52,  1.80s/it] 25%|██▍       | 759/3086 [25:23<1:17:10,  1.99s/it] 25%|██▍       | 760/3086 [25:25<1:20:42,  2.08s/it]                                                    {'loss': 0.5653, 'grad_norm': 0.09469813108444214, 'learning_rate': 0.00045223590408295525, 'epoch': 0.25}
 25%|██▍       | 760/3086 [25:25<1:20:42,  2.08s/it] 25%|██▍       | 761/3086 [25:28<1:22:47,  2.14s/it] 25%|██▍       | 762/3086 [25:30<1:26:55,  2.24s/it] 25%|██▍       | 763/3086 [25:32<1:23:11,  2.15s/it] 25%|██▍       | 764/3086 [25:34<1:18:17,  2.02s/it] 25%|██▍       | 765/3086 [25:36<1:13:53,  1.91s/it] 25%|██▍       | 766/3086 [25:38<1:15:43,  1.96s/it] 25%|██▍       | 767/3086 [25:39<1:13:52,  1.91s/it] 25%|██▍       | 768/3086 [25:41<1:15:22,  1.95s/it] 25%|██▍       | 769/3086 [25:44<1:18:32,  2.03s/it] 25%|██▍       | 770/3086 [25:45<1:14:58,  1.94s/it]                                                    {'loss': 0.5618, 'grad_norm': 0.10256720334291458, 'learning_rate': 0.0004502916396629941, 'epoch': 0.25}
 25%|██▍       | 770/3086 [25:45<1:14:58,  1.94s/it] 25%|██▍       | 771/3086 [25:47<1:14:19,  1.93s/it] 25%|██▌       | 772/3086 [25:50<1:25:32,  2.22s/it] 25%|██▌       | 773/3086 [25:52<1:26:03,  2.23s/it] 25%|██▌       | 774/3086 [25:54<1:15:47,  1.97s/it] 25%|██▌       | 775/3086 [25:56<1:18:28,  2.04s/it] 25%|██▌       | 776/3086 [25:58<1:19:13,  2.06s/it] 25%|██▌       | 777/3086 [26:00<1:14:28,  1.94s/it] 25%|██▌       | 778/3086 [26:02<1:12:34,  1.89s/it] 25%|██▌       | 779/3086 [26:03<1:12:35,  1.89s/it] 25%|██▌       | 780/3086 [26:05<1:10:19,  1.83s/it]                                                    {'loss': 0.5495, 'grad_norm': 0.08558674901723862, 'learning_rate': 0.000448347375243033, 'epoch': 0.25}
 25%|██▌       | 780/3086 [26:05<1:10:19,  1.83s/it] 25%|██▌       | 781/3086 [26:07<1:08:19,  1.78s/it] 25%|██▌       | 782/3086 [26:09<1:10:28,  1.84s/it] 25%|██▌       | 783/3086 [26:10<1:08:45,  1.79s/it] 25%|██▌       | 784/3086 [26:13<1:12:08,  1.88s/it] 25%|██▌       | 785/3086 [26:14<1:11:33,  1.87s/it] 25%|██▌       | 786/3086 [26:16<1:07:17,  1.76s/it] 26%|██▌       | 787/3086 [26:18<1:12:05,  1.88s/it] 26%|██▌       | 788/3086 [26:20<1:10:39,  1.84s/it] 26%|██▌       | 789/3086 [26:21<1:08:40,  1.79s/it] 26%|██▌       | 790/3086 [26:23<1:09:03,  1.80s/it]                                                    {'loss': 0.55, 'grad_norm': 0.08269112557172775, 'learning_rate': 0.0004464031108230719, 'epoch': 0.26}
 26%|██▌       | 790/3086 [26:23<1:09:03,  1.80s/it] 26%|██▌       | 791/3086 [26:25<1:08:16,  1.78s/it] 26%|██▌       | 792/3086 [26:27<1:07:49,  1.77s/it] 26%|██▌       | 793/3086 [26:29<1:13:12,  1.92s/it] 26%|██▌       | 794/3086 [26:31<1:14:00,  1.94s/it] 26%|██▌       | 795/3086 [26:33<1:17:06,  2.02s/it] 26%|██▌       | 796/3086 [26:36<1:22:14,  2.15s/it] 26%|██▌       | 797/3086 [26:37<1:17:17,  2.03s/it] 26%|██▌       | 798/3086 [26:40<1:22:22,  2.16s/it] 26%|██▌       | 799/3086 [26:42<1:19:18,  2.08s/it] 26%|██▌       | 800/3086 [26:44<1:22:46,  2.17s/it]                                                    {'loss': 0.5428, 'grad_norm': 0.09810185432434082, 'learning_rate': 0.00044445884640311075, 'epoch': 0.26}
 26%|██▌       | 800/3086 [26:44<1:22:46,  2.17s/it] 26%|██▌       | 801/3086 [26:46<1:19:53,  2.10s/it] 26%|██▌       | 802/3086 [26:48<1:15:23,  1.98s/it] 26%|██▌       | 803/3086 [26:50<1:16:45,  2.02s/it] 26%|██▌       | 804/3086 [26:51<1:11:21,  1.88s/it] 26%|██▌       | 805/3086 [26:54<1:14:02,  1.95s/it] 26%|██▌       | 806/3086 [26:56<1:20:26,  2.12s/it] 26%|██▌       | 807/3086 [26:58<1:14:52,  1.97s/it] 26%|██▌       | 808/3086 [26:59<1:12:39,  1.91s/it] 26%|██▌       | 809/3086 [27:01<1:10:24,  1.86s/it] 26%|██▌       | 810/3086 [27:03<1:11:24,  1.88s/it]                                                    {'loss': 0.5464, 'grad_norm': 0.09924809634685516, 'learning_rate': 0.0004425145819831497, 'epoch': 0.26}
 26%|██▌       | 810/3086 [27:03<1:11:24,  1.88s/it] 26%|██▋       | 811/3086 [27:06<1:19:49,  2.11s/it] 26%|██▋       | 812/3086 [27:08<1:17:30,  2.05s/it] 26%|██▋       | 813/3086 [27:10<1:17:51,  2.06s/it] 26%|██▋       | 814/3086 [27:12<1:18:13,  2.07s/it] 26%|██▋       | 815/3086 [27:14<1:16:55,  2.03s/it] 26%|██▋       | 816/3086 [27:16<1:17:44,  2.05s/it] 26%|██▋       | 817/3086 [27:18<1:12:52,  1.93s/it] 27%|██▋       | 818/3086 [27:19<1:12:42,  1.92s/it] 27%|██▋       | 819/3086 [27:21<1:13:04,  1.93s/it] 27%|██▋       | 820/3086 [27:24<1:16:02,  2.01s/it]                                                    {'loss': 0.543, 'grad_norm': 0.07412116974592209, 'learning_rate': 0.0004405703175631885, 'epoch': 0.27}
 27%|██▋       | 820/3086 [27:24<1:16:02,  2.01s/it] 27%|██▋       | 821/3086 [27:26<1:17:01,  2.04s/it] 27%|██▋       | 822/3086 [27:27<1:12:20,  1.92s/it] 27%|██▋       | 823/3086 [27:30<1:16:31,  2.03s/it] 27%|██▋       | 824/3086 [27:32<1:14:46,  1.98s/it] 27%|██▋       | 825/3086 [27:34<1:18:23,  2.08s/it] 27%|██▋       | 826/3086 [27:36<1:14:42,  1.98s/it] 27%|██▋       | 827/3086 [27:38<1:20:21,  2.13s/it] 27%|██▋       | 828/3086 [27:41<1:24:26,  2.24s/it] 27%|██▋       | 829/3086 [27:42<1:20:34,  2.14s/it] 27%|██▋       | 830/3086 [27:45<1:23:23,  2.22s/it]                                                    {'loss': 0.5354, 'grad_norm': 0.07222673296928406, 'learning_rate': 0.0004386260531432274, 'epoch': 0.27}
 27%|██▋       | 830/3086 [27:45<1:23:23,  2.22s/it] 27%|██▋       | 831/3086 [27:47<1:21:50,  2.18s/it] 27%|██▋       | 832/3086 [27:50<1:33:30,  2.49s/it] 27%|██▋       | 833/3086 [27:52<1:24:10,  2.24s/it] 27%|██▋       | 834/3086 [27:54<1:21:44,  2.18s/it] 27%|██▋       | 835/3086 [27:56<1:19:23,  2.12s/it] 27%|██▋       | 836/3086 [27:58<1:17:50,  2.08s/it] 27%|██▋       | 837/3086 [28:00<1:14:35,  1.99s/it] 27%|██▋       | 838/3086 [28:02<1:16:53,  2.05s/it] 27%|██▋       | 839/3086 [28:04<1:21:50,  2.19s/it] 27%|██▋       | 840/3086 [28:06<1:18:34,  2.10s/it]                                                    {'loss': 0.5462, 'grad_norm': 0.11192875355482101, 'learning_rate': 0.00043668178872326635, 'epoch': 0.27}
 27%|██▋       | 840/3086 [28:06<1:18:34,  2.10s/it] 27%|██▋       | 841/3086 [28:08<1:19:43,  2.13s/it] 27%|██▋       | 842/3086 [28:10<1:15:47,  2.03s/it] 27%|██▋       | 843/3086 [28:13<1:19:44,  2.13s/it] 27%|██▋       | 844/3086 [28:15<1:17:40,  2.08s/it] 27%|██▋       | 845/3086 [28:17<1:23:07,  2.23s/it] 27%|██▋       | 846/3086 [28:20<1:25:58,  2.30s/it] 27%|██▋       | 847/3086 [28:22<1:22:19,  2.21s/it] 27%|██▋       | 848/3086 [28:23<1:17:16,  2.07s/it] 28%|██▊       | 849/3086 [28:25<1:14:20,  1.99s/it] 28%|██▊       | 850/3086 [28:27<1:09:47,  1.87s/it]                                                    {'loss': 0.5354, 'grad_norm': 0.08670108765363693, 'learning_rate': 0.0004347375243033052, 'epoch': 0.28}
 28%|██▊       | 850/3086 [28:27<1:09:47,  1.87s/it] 28%|██▊       | 851/3086 [28:29<1:09:04,  1.85s/it] 28%|██▊       | 852/3086 [28:30<1:09:08,  1.86s/it] 28%|██▊       | 853/3086 [28:32<1:11:12,  1.91s/it] 28%|██▊       | 854/3086 [28:34<1:11:36,  1.92s/it] 28%|██▊       | 855/3086 [28:37<1:20:06,  2.15s/it] 28%|██▊       | 856/3086 [28:39<1:23:15,  2.24s/it] 28%|██▊       | 857/3086 [28:42<1:20:43,  2.17s/it] 28%|██▊       | 858/3086 [28:44<1:24:38,  2.28s/it] 28%|██▊       | 859/3086 [28:46<1:19:53,  2.15s/it] 28%|██▊       | 860/3086 [28:48<1:15:44,  2.04s/it]                                                    {'loss': 0.5423, 'grad_norm': 0.08087020367383957, 'learning_rate': 0.0004327932598833441, 'epoch': 0.28}
 28%|██▊       | 860/3086 [28:48<1:15:44,  2.04s/it] 28%|██▊       | 861/3086 [28:49<1:10:45,  1.91s/it] 28%|██▊       | 862/3086 [28:51<1:12:24,  1.95s/it] 28%|██▊       | 863/3086 [28:53<1:13:37,  1.99s/it] 28%|██▊       | 864/3086 [28:55<1:11:00,  1.92s/it] 28%|██▊       | 865/3086 [28:57<1:09:38,  1.88s/it] 28%|██▊       | 866/3086 [28:59<1:13:58,  2.00s/it] 28%|██▊       | 867/3086 [29:01<1:11:09,  1.92s/it] 28%|██▊       | 868/3086 [29:03<1:11:04,  1.92s/it] 28%|██▊       | 869/3086 [29:05<1:11:38,  1.94s/it] 28%|██▊       | 870/3086 [29:07<1:11:43,  1.94s/it]                                                    {'loss': 0.5506, 'grad_norm': 0.07411140203475952, 'learning_rate': 0.000430848995463383, 'epoch': 0.28}
 28%|██▊       | 870/3086 [29:07<1:11:43,  1.94s/it] 28%|██▊       | 871/3086 [29:09<1:09:59,  1.90s/it] 28%|██▊       | 872/3086 [29:11<1:10:51,  1.92s/it] 28%|██▊       | 873/3086 [29:13<1:11:01,  1.93s/it] 28%|██▊       | 874/3086 [29:15<1:14:04,  2.01s/it] 28%|██▊       | 875/3086 [29:17<1:13:25,  1.99s/it] 28%|██▊       | 876/3086 [29:19<1:17:41,  2.11s/it] 28%|██▊       | 877/3086 [29:21<1:18:14,  2.13s/it] 28%|██▊       | 878/3086 [29:23<1:19:06,  2.15s/it] 28%|██▊       | 879/3086 [29:25<1:13:50,  2.01s/it] 29%|██▊       | 880/3086 [29:27<1:13:41,  2.00s/it]                                                    {'loss': 0.5425, 'grad_norm': 0.09457969665527344, 'learning_rate': 0.00042890473104342184, 'epoch': 0.29}
 29%|██▊       | 880/3086 [29:27<1:13:41,  2.00s/it] 29%|██▊       | 881/3086 [29:29<1:12:25,  1.97s/it] 29%|██▊       | 882/3086 [29:31<1:08:02,  1.85s/it] 29%|██▊       | 883/3086 [29:33<1:10:47,  1.93s/it] 29%|██▊       | 884/3086 [29:35<1:12:14,  1.97s/it] 29%|██▊       | 885/3086 [29:37<1:19:38,  2.17s/it] 29%|██▊       | 886/3086 [29:39<1:12:14,  1.97s/it] 29%|██▊       | 887/3086 [29:42<1:20:23,  2.19s/it] 29%|██▉       | 888/3086 [29:44<1:25:03,  2.32s/it] 29%|██▉       | 889/3086 [29:46<1:18:32,  2.15s/it] 29%|██▉       | 890/3086 [29:48<1:22:10,  2.25s/it]                                                    {'loss': 0.5354, 'grad_norm': 0.07091308385133743, 'learning_rate': 0.0004269604666234608, 'epoch': 0.29}
 29%|██▉       | 890/3086 [29:48<1:22:10,  2.25s/it] 29%|██▉       | 891/3086 [29:50<1:18:42,  2.15s/it] 29%|██▉       | 892/3086 [29:52<1:16:43,  2.10s/it] 29%|██▉       | 893/3086 [29:54<1:14:17,  2.03s/it] 29%|██▉       | 894/3086 [29:56<1:12:09,  1.97s/it] 29%|██▉       | 895/3086 [29:58<1:09:36,  1.91s/it] 29%|██▉       | 896/3086 [30:00<1:14:58,  2.05s/it] 29%|██▉       | 897/3086 [30:03<1:19:27,  2.18s/it] 29%|██▉       | 898/3086 [30:05<1:17:59,  2.14s/it] 29%|██▉       | 899/3086 [30:07<1:17:30,  2.13s/it] 29%|██▉       | 900/3086 [30:09<1:18:04,  2.14s/it]                                                    {'loss': 0.5341, 'grad_norm': 0.07512741535902023, 'learning_rate': 0.0004250162022034996, 'epoch': 0.29}
 29%|██▉       | 900/3086 [30:09<1:18:04,  2.14s/it] 29%|██▉       | 901/3086 [30:11<1:13:26,  2.02s/it] 29%|██▉       | 902/3086 [30:12<1:10:31,  1.94s/it] 29%|██▉       | 903/3086 [30:14<1:10:22,  1.93s/it] 29%|██▉       | 904/3086 [30:16<1:07:30,  1.86s/it] 29%|██▉       | 905/3086 [30:18<1:09:22,  1.91s/it] 29%|██▉       | 906/3086 [30:20<1:07:05,  1.85s/it] 29%|██▉       | 907/3086 [30:22<1:12:55,  2.01s/it] 29%|██▉       | 908/3086 [30:25<1:20:36,  2.22s/it] 29%|██▉       | 909/3086 [30:27<1:15:06,  2.07s/it] 29%|██▉       | 910/3086 [30:29<1:15:09,  2.07s/it]                                                    {'loss': 0.5482, 'grad_norm': 0.08281993865966797, 'learning_rate': 0.0004230719377835385, 'epoch': 0.29}
 29%|██▉       | 910/3086 [30:29<1:15:09,  2.07s/it] 30%|██▉       | 911/3086 [30:31<1:16:04,  2.10s/it] 30%|██▉       | 912/3086 [30:33<1:12:35,  2.00s/it] 30%|██▉       | 913/3086 [30:35<1:12:00,  1.99s/it] 30%|██▉       | 914/3086 [30:37<1:11:33,  1.98s/it] 30%|██▉       | 915/3086 [30:38<1:08:24,  1.89s/it] 30%|██▉       | 916/3086 [30:41<1:13:19,  2.03s/it] 30%|██▉       | 917/3086 [30:42<1:09:39,  1.93s/it] 30%|██▉       | 918/3086 [30:44<1:08:08,  1.89s/it] 30%|██▉       | 919/3086 [30:46<1:08:35,  1.90s/it] 30%|██▉       | 920/3086 [30:48<1:05:38,  1.82s/it]                                                    {'loss': 0.5354, 'grad_norm': 0.08426320552825928, 'learning_rate': 0.00042112767336357744, 'epoch': 0.3}
 30%|██▉       | 920/3086 [30:48<1:05:38,  1.82s/it] 30%|██▉       | 921/3086 [30:49<1:04:24,  1.79s/it] 30%|██▉       | 922/3086 [30:51<1:04:43,  1.79s/it] 30%|██▉       | 923/3086 [30:53<1:07:49,  1.88s/it] 30%|██▉       | 924/3086 [30:55<1:11:34,  1.99s/it] 30%|██▉       | 925/3086 [30:57<1:11:06,  1.97s/it] 30%|███       | 926/3086 [30:59<1:10:59,  1.97s/it] 30%|███       | 927/3086 [31:02<1:12:42,  2.02s/it] 30%|███       | 928/3086 [31:04<1:14:02,  2.06s/it] 30%|███       | 929/3086 [31:06<1:13:17,  2.04s/it] 30%|███       | 930/3086 [31:08<1:13:10,  2.04s/it]                                                    {'loss': 0.5275, 'grad_norm': 0.09146714955568314, 'learning_rate': 0.0004191834089436163, 'epoch': 0.3}
 30%|███       | 930/3086 [31:08<1:13:10,  2.04s/it] 30%|███       | 931/3086 [31:09<1:08:55,  1.92s/it] 30%|███       | 932/3086 [31:11<1:08:03,  1.90s/it] 30%|███       | 933/3086 [31:13<1:10:54,  1.98s/it] 30%|███       | 934/3086 [31:16<1:17:51,  2.17s/it] 30%|███       | 935/3086 [31:18<1:17:00,  2.15s/it] 30%|███       | 936/3086 [31:20<1:16:02,  2.12s/it] 30%|███       | 937/3086 [31:22<1:10:36,  1.97s/it] 30%|███       | 938/3086 [31:23<1:08:00,  1.90s/it] 30%|███       | 939/3086 [31:26<1:09:44,  1.95s/it] 30%|███       | 940/3086 [31:28<1:12:05,  2.02s/it]                                                    {'loss': 0.5364, 'grad_norm': 0.07761497050523758, 'learning_rate': 0.0004172391445236552, 'epoch': 0.3}
 30%|███       | 940/3086 [31:28<1:12:05,  2.02s/it] 30%|███       | 941/3086 [31:30<1:16:06,  2.13s/it] 31%|███       | 942/3086 [31:33<1:21:31,  2.28s/it] 31%|███       | 943/3086 [31:35<1:19:08,  2.22s/it] 31%|███       | 944/3086 [31:37<1:18:51,  2.21s/it] 31%|███       | 945/3086 [31:39<1:14:19,  2.08s/it] 31%|███       | 946/3086 [31:41<1:15:55,  2.13s/it] 31%|███       | 947/3086 [31:43<1:12:39,  2.04s/it] 31%|███       | 948/3086 [31:45<1:12:54,  2.05s/it] 31%|███       | 949/3086 [31:47<1:10:17,  1.97s/it] 31%|███       | 950/3086 [31:49<1:13:45,  2.07s/it]                                                    {'loss': 0.5264, 'grad_norm': 0.07504983246326447, 'learning_rate': 0.00041529488010369405, 'epoch': 0.31}
 31%|███       | 950/3086 [31:49<1:13:45,  2.07s/it] 31%|███       | 951/3086 [31:51<1:14:21,  2.09s/it] 31%|███       | 952/3086 [31:54<1:19:21,  2.23s/it] 31%|███       | 953/3086 [31:56<1:15:08,  2.11s/it] 31%|███       | 954/3086 [31:58<1:13:40,  2.07s/it] 31%|███       | 955/3086 [31:59<1:11:50,  2.02s/it] 31%|███       | 956/3086 [32:02<1:15:09,  2.12s/it] 31%|███       | 957/3086 [32:04<1:15:42,  2.13s/it] 31%|███       | 958/3086 [32:06<1:16:09,  2.15s/it] 31%|███       | 959/3086 [32:08<1:14:54,  2.11s/it] 31%|███       | 960/3086 [32:10<1:15:41,  2.14s/it]                                                    {'loss': 0.5218, 'grad_norm': 0.07420700043439865, 'learning_rate': 0.00041335061568373294, 'epoch': 0.31}
 31%|███       | 960/3086 [32:10<1:15:41,  2.14s/it] 31%|███       | 961/3086 [32:13<1:17:24,  2.19s/it] 31%|███       | 962/3086 [32:14<1:13:35,  2.08s/it] 31%|███       | 963/3086 [32:17<1:13:55,  2.09s/it] 31%|███       | 964/3086 [32:18<1:09:36,  1.97s/it] 31%|███▏      | 965/3086 [32:21<1:13:02,  2.07s/it] 31%|███▏      | 966/3086 [32:23<1:15:59,  2.15s/it] 31%|███▏      | 967/3086 [32:25<1:17:20,  2.19s/it] 31%|███▏      | 968/3086 [32:27<1:12:13,  2.05s/it] 31%|███▏      | 969/3086 [32:29<1:07:28,  1.91s/it] 31%|███▏      | 970/3086 [32:31<1:09:20,  1.97s/it]                                                    {'loss': 0.5281, 'grad_norm': 0.07766681909561157, 'learning_rate': 0.0004114063512637719, 'epoch': 0.31}
 31%|███▏      | 970/3086 [32:31<1:09:20,  1.97s/it] 31%|███▏      | 971/3086 [32:32<1:06:50,  1.90s/it] 31%|███▏      | 972/3086 [32:34<1:05:38,  1.86s/it] 32%|███▏      | 973/3086 [32:36<1:03:52,  1.81s/it] 32%|███▏      | 974/3086 [32:38<1:08:12,  1.94s/it] 32%|███▏      | 975/3086 [32:40<1:03:41,  1.81s/it] 32%|███▏      | 976/3086 [32:42<1:05:53,  1.87s/it] 32%|███▏      | 977/3086 [32:44<1:07:13,  1.91s/it] 32%|███▏      | 978/3086 [32:46<1:08:25,  1.95s/it] 32%|███▏      | 979/3086 [32:47<1:07:22,  1.92s/it] 32%|███▏      | 980/3086 [32:49<1:05:18,  1.86s/it]                                                    {'loss': 0.5277, 'grad_norm': 0.08206283301115036, 'learning_rate': 0.0004094620868438107, 'epoch': 0.32}
 32%|███▏      | 980/3086 [32:49<1:05:18,  1.86s/it] 32%|███▏      | 981/3086 [32:51<1:08:16,  1.95s/it] 32%|███▏      | 982/3086 [32:54<1:10:44,  2.02s/it] 32%|███▏      | 983/3086 [32:56<1:11:34,  2.04s/it] 32%|███▏      | 984/3086 [32:57<1:08:13,  1.95s/it] 32%|███▏      | 985/3086 [32:59<1:08:47,  1.96s/it] 32%|███▏      | 986/3086 [33:01<1:08:52,  1.97s/it] 32%|███▏      | 987/3086 [33:03<1:10:00,  2.00s/it] 32%|███▏      | 988/3086 [33:05<1:06:21,  1.90s/it] 32%|███▏      | 989/3086 [33:07<1:06:38,  1.91s/it] 32%|███▏      | 990/3086 [33:09<1:02:41,  1.79s/it]                                                    {'loss': 0.5363, 'grad_norm': 0.06618605554103851, 'learning_rate': 0.0004075178224238496, 'epoch': 0.32}
 32%|███▏      | 990/3086 [33:09<1:02:41,  1.79s/it] 32%|███▏      | 991/3086 [33:11<1:06:04,  1.89s/it] 32%|███▏      | 992/3086 [33:12<1:04:32,  1.85s/it] 32%|███▏      | 993/3086 [33:14<1:06:47,  1.91s/it] 32%|███▏      | 994/3086 [33:16<1:07:14,  1.93s/it] 32%|███▏      | 995/3086 [33:18<1:04:19,  1.85s/it] 32%|███▏      | 996/3086 [33:20<1:06:33,  1.91s/it] 32%|███▏      | 997/3086 [33:22<1:06:03,  1.90s/it] 32%|███▏      | 998/3086 [33:24<1:10:33,  2.03s/it] 32%|███▏      | 999/3086 [33:26<1:07:15,  1.93s/it] 32%|███▏      | 1000/3086 [33:28<1:03:37,  1.83s/it]                                                     {'loss': 0.5266, 'grad_norm': 0.07046221196651459, 'learning_rate': 0.0004055735580038885, 'epoch': 0.32}
 32%|███▏      | 1000/3086 [33:28<1:03:37,  1.83s/it] 32%|███▏      | 1001/3086 [33:30<1:08:12,  1.96s/it] 32%|███▏      | 1002/3086 [33:32<1:12:38,  2.09s/it] 33%|███▎      | 1003/3086 [33:34<1:07:33,  1.95s/it] 33%|███▎      | 1004/3086 [33:35<1:03:46,  1.84s/it] 33%|███▎      | 1005/3086 [33:37<1:05:29,  1.89s/it] 33%|███▎      | 1006/3086 [33:40<1:08:23,  1.97s/it] 33%|███▎      | 1007/3086 [33:42<1:13:18,  2.12s/it] 33%|███▎      | 1008/3086 [33:44<1:12:56,  2.11s/it] 33%|███▎      | 1009/3086 [33:46<1:12:48,  2.10s/it] 33%|███▎      | 1010/3086 [33:48<1:11:11,  2.06s/it]                                                     {'loss': 0.533, 'grad_norm': 0.0645737573504448, 'learning_rate': 0.00040362929358392737, 'epoch': 0.33}
 33%|███▎      | 1010/3086 [33:48<1:11:11,  2.06s/it] 33%|███▎      | 1011/3086 [33:50<1:06:54,  1.93s/it] 33%|███▎      | 1012/3086 [33:52<1:11:14,  2.06s/it] 33%|███▎      | 1013/3086 [33:54<1:07:16,  1.95s/it] 33%|███▎      | 1014/3086 [33:56<1:03:53,  1.85s/it] 33%|███▎      | 1015/3086 [33:58<1:11:02,  2.06s/it] 33%|███▎      | 1016/3086 [34:00<1:06:21,  1.92s/it] 33%|███▎      | 1017/3086 [34:02<1:08:30,  1.99s/it] 33%|███▎      | 1018/3086 [34:04<1:06:13,  1.92s/it] 33%|███▎      | 1019/3086 [34:06<1:06:58,  1.94s/it] 33%|███▎      | 1020/3086 [34:08<1:08:07,  1.98s/it]                                                     {'loss': 0.5287, 'grad_norm': 0.07927601039409637, 'learning_rate': 0.0004016850291639662, 'epoch': 0.33}
 33%|███▎      | 1020/3086 [34:08<1:08:07,  1.98s/it] 33%|███▎      | 1021/3086 [34:09<1:04:42,  1.88s/it] 33%|███▎      | 1022/3086 [34:12<1:08:13,  1.98s/it] 33%|███▎      | 1023/3086 [34:13<1:05:33,  1.91s/it] 33%|███▎      | 1024/3086 [34:15<1:08:32,  1.99s/it] 33%|███▎      | 1025/3086 [34:17<1:05:54,  1.92s/it] 33%|███▎      | 1026/3086 [34:19<1:03:32,  1.85s/it] 33%|███▎      | 1027/3086 [34:21<1:01:40,  1.80s/it] 33%|███▎      | 1028/3086 [34:23<1:05:27,  1.91s/it] 33%|███▎      | 1029/3086 [34:25<1:09:50,  2.04s/it] 33%|███▎      | 1030/3086 [34:27<1:05:36,  1.91s/it]                                                     {'loss': 0.5286, 'grad_norm': 0.07744311541318893, 'learning_rate': 0.00039974076474400515, 'epoch': 0.33}
 33%|███▎      | 1030/3086 [34:27<1:05:36,  1.91s/it] 33%|███▎      | 1031/3086 [34:29<1:07:31,  1.97s/it] 33%|███▎      | 1032/3086 [34:31<1:06:30,  1.94s/it] 33%|███▎      | 1033/3086 [34:32<1:03:49,  1.87s/it] 34%|███▎      | 1034/3086 [34:35<1:09:44,  2.04s/it] 34%|███▎      | 1035/3086 [34:37<1:10:49,  2.07s/it] 34%|███▎      | 1036/3086 [34:39<1:08:43,  2.01s/it] 34%|███▎      | 1037/3086 [34:41<1:10:17,  2.06s/it] 34%|███▎      | 1038/3086 [34:43<1:05:20,  1.91s/it] 34%|███▎      | 1039/3086 [34:44<1:01:56,  1.82s/it] 34%|███▎      | 1040/3086 [34:46<1:06:14,  1.94s/it]                                                     {'loss': 0.5257, 'grad_norm': 0.0981491208076477, 'learning_rate': 0.00039779650032404403, 'epoch': 0.34}
 34%|███▎      | 1040/3086 [34:46<1:06:14,  1.94s/it] 34%|███▎      | 1041/3086 [34:48<1:05:00,  1.91s/it] 34%|███▍      | 1042/3086 [34:51<1:09:21,  2.04s/it] 34%|███▍      | 1043/3086 [34:52<1:06:18,  1.95s/it] 34%|███▍      | 1044/3086 [34:54<1:06:12,  1.95s/it] 34%|███▍      | 1045/3086 [34:56<1:07:41,  1.99s/it] 34%|███▍      | 1046/3086 [34:58<1:08:44,  2.02s/it] 34%|███▍      | 1047/3086 [35:00<1:07:35,  1.99s/it] 34%|███▍      | 1048/3086 [35:02<1:08:23,  2.01s/it] 34%|███▍      | 1049/3086 [35:04<1:06:24,  1.96s/it] 34%|███▍      | 1050/3086 [35:06<1:08:23,  2.02s/it]                                                     {'loss': 0.5219, 'grad_norm': 0.07214857637882233, 'learning_rate': 0.000395852235904083, 'epoch': 0.34}
 34%|███▍      | 1050/3086 [35:06<1:08:23,  2.02s/it] 34%|███▍      | 1051/3086 [35:08<1:04:13,  1.89s/it] 34%|███▍      | 1052/3086 [35:10<1:02:42,  1.85s/it] 34%|███▍      | 1053/3086 [35:12<1:02:30,  1.84s/it] 34%|███▍      | 1054/3086 [35:13<59:12,  1.75s/it]   34%|███▍      | 1055/3086 [35:16<1:08:45,  2.03s/it] 34%|███▍      | 1056/3086 [35:17<1:03:35,  1.88s/it] 34%|███▍      | 1057/3086 [35:20<1:07:08,  1.99s/it] 34%|███▍      | 1058/3086 [35:22<1:06:58,  1.98s/it] 34%|███▍      | 1059/3086 [35:24<1:12:56,  2.16s/it] 34%|███▍      | 1060/3086 [35:26<1:11:05,  2.11s/it]                                                     {'loss': 0.5296, 'grad_norm': 0.0792478621006012, 'learning_rate': 0.0003939079714841218, 'epoch': 0.34}
 34%|███▍      | 1060/3086 [35:26<1:11:05,  2.11s/it] 34%|███▍      | 1061/3086 [35:28<1:06:49,  1.98s/it] 34%|███▍      | 1062/3086 [35:30<1:07:35,  2.00s/it] 34%|███▍      | 1063/3086 [35:32<1:04:12,  1.90s/it] 34%|███▍      | 1064/3086 [35:34<1:11:56,  2.13s/it] 35%|███▍      | 1065/3086 [35:36<1:09:05,  2.05s/it] 35%|███▍      | 1066/3086 [35:38<1:05:03,  1.93s/it] 35%|███▍      | 1067/3086 [35:40<1:05:34,  1.95s/it] 35%|███▍      | 1068/3086 [35:42<1:10:42,  2.10s/it] 35%|███▍      | 1069/3086 [35:44<1:08:10,  2.03s/it] 35%|███▍      | 1070/3086 [35:46<1:08:10,  2.03s/it]                                                     {'loss': 0.5275, 'grad_norm': 0.09423761069774628, 'learning_rate': 0.0003919637070641607, 'epoch': 0.35}
 35%|███▍      | 1070/3086 [35:46<1:08:10,  2.03s/it] 35%|███▍      | 1071/3086 [35:48<1:09:34,  2.07s/it] 35%|███▍      | 1072/3086 [35:50<1:05:28,  1.95s/it] 35%|███▍      | 1073/3086 [35:52<1:07:13,  2.00s/it] 35%|███▍      | 1074/3086 [35:54<1:08:49,  2.05s/it] 35%|███▍      | 1075/3086 [35:56<1:03:33,  1.90s/it] 35%|███▍      | 1076/3086 [35:58<1:02:53,  1.88s/it] 35%|███▍      | 1077/3086 [36:00<1:05:02,  1.94s/it] 35%|███▍      | 1078/3086 [36:01<1:04:24,  1.92s/it] 35%|███▍      | 1079/3086 [36:03<1:02:19,  1.86s/it] 35%|███▍      | 1080/3086 [36:05<1:03:47,  1.91s/it]                                                     {'loss': 0.5217, 'grad_norm': 0.0761357769370079, 'learning_rate': 0.0003900194426441996, 'epoch': 0.35}
 35%|███▍      | 1080/3086 [36:05<1:03:47,  1.91s/it] 35%|███▌      | 1081/3086 [36:07<1:06:23,  1.99s/it] 35%|███▌      | 1082/3086 [36:10<1:09:40,  2.09s/it] 35%|███▌      | 1083/3086 [36:12<1:11:11,  2.13s/it] 35%|███▌      | 1084/3086 [36:14<1:07:32,  2.02s/it] 35%|███▌      | 1085/3086 [36:16<1:13:41,  2.21s/it] 35%|███▌      | 1086/3086 [36:18<1:10:20,  2.11s/it] 35%|███▌      | 1087/3086 [36:20<1:11:04,  2.13s/it] 35%|███▌      | 1088/3086 [36:23<1:11:30,  2.15s/it] 35%|███▌      | 1089/3086 [36:25<1:10:26,  2.12s/it] 35%|███▌      | 1090/3086 [36:27<1:08:35,  2.06s/it]                                                     {'loss': 0.5162, 'grad_norm': 0.08825627714395523, 'learning_rate': 0.00038807517822423847, 'epoch': 0.35}
 35%|███▌      | 1090/3086 [36:27<1:08:35,  2.06s/it] 35%|███▌      | 1091/3086 [36:28<1:06:24,  2.00s/it] 35%|███▌      | 1092/3086 [36:31<1:08:40,  2.07s/it] 35%|███▌      | 1093/3086 [36:34<1:16:17,  2.30s/it] 35%|███▌      | 1094/3086 [36:36<1:13:48,  2.22s/it] 35%|███▌      | 1095/3086 [36:37<1:09:57,  2.11s/it] 36%|███▌      | 1096/3086 [36:40<1:11:17,  2.15s/it] 36%|███▌      | 1097/3086 [36:42<1:09:34,  2.10s/it] 36%|███▌      | 1098/3086 [36:44<1:09:22,  2.09s/it] 36%|███▌      | 1099/3086 [36:46<1:14:08,  2.24s/it] 36%|███▌      | 1100/3086 [36:48<1:08:00,  2.05s/it]                                                     {'loss': 0.5135, 'grad_norm': 0.08601747453212738, 'learning_rate': 0.0003861309138042773, 'epoch': 0.36}
 36%|███▌      | 1100/3086 [36:48<1:08:00,  2.05s/it] 36%|███▌      | 1101/3086 [36:50<1:05:24,  1.98s/it] 36%|███▌      | 1102/3086 [36:52<1:07:11,  2.03s/it] 36%|███▌      | 1103/3086 [36:54<1:05:57,  2.00s/it] 36%|███▌      | 1104/3086 [36:56<1:08:07,  2.06s/it] 36%|███▌      | 1105/3086 [36:58<1:08:01,  2.06s/it] 36%|███▌      | 1106/3086 [37:00<1:08:21,  2.07s/it] 36%|███▌      | 1107/3086 [37:02<1:08:03,  2.06s/it] 36%|███▌      | 1108/3086 [37:04<1:04:11,  1.95s/it] 36%|███▌      | 1109/3086 [37:06<1:05:22,  1.98s/it] 36%|███▌      | 1110/3086 [37:08<1:05:33,  1.99s/it]                                                     {'loss': 0.5136, 'grad_norm': 0.08781760185956955, 'learning_rate': 0.00038418664938431624, 'epoch': 0.36}
 36%|███▌      | 1110/3086 [37:08<1:05:33,  1.99s/it] 36%|███▌      | 1111/3086 [37:10<1:04:44,  1.97s/it] 36%|███▌      | 1112/3086 [37:12<1:05:15,  1.98s/it] 36%|███▌      | 1113/3086 [37:14<1:03:12,  1.92s/it] 36%|███▌      | 1114/3086 [37:16<1:09:32,  2.12s/it] 36%|███▌      | 1115/3086 [37:19<1:12:15,  2.20s/it] 36%|███▌      | 1116/3086 [37:20<1:06:51,  2.04s/it] 36%|███▌      | 1117/3086 [37:23<1:14:19,  2.26s/it] 36%|███▌      | 1118/3086 [37:25<1:10:14,  2.14s/it] 36%|███▋      | 1119/3086 [37:27<1:07:30,  2.06s/it] 36%|███▋      | 1120/3086 [37:29<1:07:09,  2.05s/it]                                                     {'loss': 0.511, 'grad_norm': 0.08386016637086868, 'learning_rate': 0.00038224238496435513, 'epoch': 0.36}
 36%|███▋      | 1120/3086 [37:29<1:07:09,  2.05s/it] 36%|███▋      | 1121/3086 [37:31<1:06:39,  2.04s/it] 36%|███▋      | 1122/3086 [37:33<1:03:40,  1.95s/it] 36%|███▋      | 1123/3086 [37:35<1:03:56,  1.95s/it] 36%|███▋      | 1124/3086 [37:37<1:08:01,  2.08s/it] 36%|███▋      | 1125/3086 [37:39<1:05:43,  2.01s/it] 36%|███▋      | 1126/3086 [37:41<1:04:48,  1.98s/it] 37%|███▋      | 1127/3086 [37:43<1:03:32,  1.95s/it] 37%|███▋      | 1128/3086 [37:45<1:04:35,  1.98s/it] 37%|███▋      | 1129/3086 [37:46<1:01:55,  1.90s/it] 37%|███▋      | 1130/3086 [37:48<1:01:40,  1.89s/it]                                                     {'loss': 0.5132, 'grad_norm': 0.07101655006408691, 'learning_rate': 0.000380298120544394, 'epoch': 0.37}
 37%|███▋      | 1130/3086 [37:48<1:01:40,  1.89s/it] 37%|███▋      | 1131/3086 [37:50<1:05:07,  2.00s/it] 37%|███▋      | 1132/3086 [37:52<1:01:48,  1.90s/it] 37%|███▋      | 1133/3086 [37:54<1:00:18,  1.85s/it] 37%|███▋      | 1134/3086 [37:56<1:00:23,  1.86s/it] 37%|███▋      | 1135/3086 [37:58<1:04:55,  2.00s/it] 37%|███▋      | 1136/3086 [38:00<1:01:12,  1.88s/it] 37%|███▋      | 1137/3086 [38:02<1:01:34,  1.90s/it] 37%|███▋      | 1138/3086 [38:04<1:02:42,  1.93s/it] 37%|███▋      | 1139/3086 [38:06<1:05:15,  2.01s/it] 37%|███▋      | 1140/3086 [38:08<1:08:21,  2.11s/it]                                                     {'loss': 0.5052, 'grad_norm': 0.10488469153642654, 'learning_rate': 0.0003783538561244329, 'epoch': 0.37}
 37%|███▋      | 1140/3086 [38:08<1:08:21,  2.11s/it] 37%|███▋      | 1141/3086 [38:10<1:09:37,  2.15s/it] 37%|███▋      | 1142/3086 [38:12<1:07:41,  2.09s/it] 37%|███▋      | 1143/3086 [38:14<1:04:50,  2.00s/it] 37%|███▋      | 1144/3086 [38:16<1:05:56,  2.04s/it] 37%|███▋      | 1145/3086 [38:18<1:05:03,  2.01s/it] 37%|███▋      | 1146/3086 [38:20<1:03:03,  1.95s/it] 37%|███▋      | 1147/3086 [38:22<1:03:18,  1.96s/it] 37%|███▋      | 1148/3086 [38:24<1:01:16,  1.90s/it] 37%|███▋      | 1149/3086 [38:25<57:56,  1.79s/it]   37%|███▋      | 1150/3086 [38:28<1:04:34,  2.00s/it]                                                     {'loss': 0.5137, 'grad_norm': 0.07633867859840393, 'learning_rate': 0.00037640959170447174, 'epoch': 0.37}
 37%|███▋      | 1150/3086 [38:28<1:04:34,  2.00s/it] 37%|███▋      | 1151/3086 [38:30<1:03:52,  1.98s/it] 37%|███▋      | 1152/3086 [38:32<1:10:44,  2.19s/it] 37%|███▋      | 1153/3086 [38:34<1:06:01,  2.05s/it] 37%|███▋      | 1154/3086 [38:36<1:05:13,  2.03s/it] 37%|███▋      | 1155/3086 [38:39<1:09:17,  2.15s/it] 37%|███▋      | 1156/3086 [38:40<1:07:24,  2.10s/it] 37%|███▋      | 1157/3086 [38:42<1:06:00,  2.05s/it] 38%|███▊      | 1158/3086 [38:44<1:05:27,  2.04s/it] 38%|███▊      | 1159/3086 [38:47<1:07:36,  2.11s/it] 38%|███▊      | 1160/3086 [38:49<1:11:17,  2.22s/it]                                                     {'loss': 0.4937, 'grad_norm': 0.0785813108086586, 'learning_rate': 0.0003744653272845107, 'epoch': 0.38}
 38%|███▊      | 1160/3086 [38:49<1:11:17,  2.22s/it] 38%|███▊      | 1161/3086 [38:51<1:04:27,  2.01s/it] 38%|███▊      | 1162/3086 [38:53<1:06:39,  2.08s/it] 38%|███▊      | 1163/3086 [38:55<1:02:25,  1.95s/it] 38%|███▊      | 1164/3086 [38:57<1:02:27,  1.95s/it] 38%|███▊      | 1165/3086 [38:59<1:03:32,  1.98s/it] 38%|███▊      | 1166/3086 [39:01<1:10:40,  2.21s/it] 38%|███▊      | 1167/3086 [39:04<1:11:17,  2.23s/it] 38%|███▊      | 1168/3086 [39:05<1:06:49,  2.09s/it] 38%|███▊      | 1169/3086 [39:08<1:13:25,  2.30s/it] 38%|███▊      | 1170/3086 [39:10<1:06:46,  2.09s/it]                                                     {'loss': 0.5129, 'grad_norm': 0.08181478083133698, 'learning_rate': 0.00037252106286454956, 'epoch': 0.38}
 38%|███▊      | 1170/3086 [39:10<1:06:46,  2.09s/it] 38%|███▊      | 1171/3086 [39:12<1:04:45,  2.03s/it] 38%|███▊      | 1172/3086 [39:15<1:13:20,  2.30s/it] 38%|███▊      | 1173/3086 [39:16<1:07:27,  2.12s/it] 38%|███▊      | 1174/3086 [39:18<1:04:36,  2.03s/it] 38%|███▊      | 1175/3086 [39:20<1:06:43,  2.10s/it] 38%|███▊      | 1176/3086 [39:22<1:05:17,  2.05s/it] 38%|███▊      | 1177/3086 [39:24<1:02:24,  1.96s/it] 38%|███▊      | 1178/3086 [39:26<1:03:45,  2.00s/it] 38%|███▊      | 1179/3086 [39:28<1:00:57,  1.92s/it] 38%|███▊      | 1180/3086 [39:29<57:55,  1.82s/it]                                                     {'loss': 0.4893, 'grad_norm': 0.08355478197336197, 'learning_rate': 0.0003705767984445884, 'epoch': 0.38}
 38%|███▊      | 1180/3086 [39:29<57:55,  1.82s/it] 38%|███▊      | 1181/3086 [39:31<56:46,  1.79s/it] 38%|███▊      | 1182/3086 [39:33<55:43,  1.76s/it] 38%|███▊      | 1183/3086 [39:35<55:50,  1.76s/it] 38%|███▊      | 1184/3086 [39:36<56:39,  1.79s/it] 38%|███▊      | 1185/3086 [39:38<56:09,  1.77s/it] 38%|███▊      | 1186/3086 [39:41<1:01:21,  1.94s/it] 38%|███▊      | 1187/3086 [39:42<59:58,  1.89s/it]   38%|███▊      | 1188/3086 [39:44<1:00:54,  1.93s/it] 39%|███▊      | 1189/3086 [39:46<59:49,  1.89s/it]   39%|███▊      | 1190/3086 [39:49<1:06:14,  2.10s/it]                                                     {'loss': 0.4969, 'grad_norm': 0.08452553302049637, 'learning_rate': 0.00036863253402462734, 'epoch': 0.39}
 39%|███▊      | 1190/3086 [39:49<1:06:14,  2.10s/it] 39%|███▊      | 1191/3086 [39:51<1:06:33,  2.11s/it] 39%|███▊      | 1192/3086 [39:53<1:07:37,  2.14s/it] 39%|███▊      | 1193/3086 [39:55<1:06:41,  2.11s/it] 39%|███▊      | 1194/3086 [39:57<1:03:01,  2.00s/it] 39%|███▊      | 1195/3086 [39:59<1:04:04,  2.03s/it] 39%|███▉      | 1196/3086 [40:01<1:02:18,  1.98s/it] 39%|███▉      | 1197/3086 [40:03<1:04:18,  2.04s/it] 39%|███▉      | 1198/3086 [40:05<1:01:50,  1.97s/it] 39%|███▉      | 1199/3086 [40:07<1:01:09,  1.94s/it] 39%|███▉      | 1200/3086 [40:09<1:01:17,  1.95s/it]                                                     {'loss': 0.4948, 'grad_norm': 0.07303037494421005, 'learning_rate': 0.00036668826960466617, 'epoch': 0.39}
 39%|███▉      | 1200/3086 [40:09<1:01:17,  1.95s/it] 39%|███▉      | 1201/3086 [40:11<1:05:18,  2.08s/it] 39%|███▉      | 1202/3086 [40:14<1:11:44,  2.28s/it] 39%|███▉      | 1203/3086 [40:16<1:10:58,  2.26s/it] 39%|███▉      | 1204/3086 [40:18<1:09:54,  2.23s/it] 39%|███▉      | 1205/3086 [40:20<1:06:37,  2.13s/it] 39%|███▉      | 1206/3086 [40:22<1:03:25,  2.02s/it] 39%|███▉      | 1207/3086 [40:24<1:03:10,  2.02s/it] 39%|███▉      | 1208/3086 [40:26<1:00:30,  1.93s/it] 39%|███▉      | 1209/3086 [40:27<1:00:30,  1.93s/it] 39%|███▉      | 1210/3086 [40:30<1:01:54,  1.98s/it]                                                     {'loss': 0.501, 'grad_norm': 0.08916746079921722, 'learning_rate': 0.00036474400518470506, 'epoch': 0.39}
 39%|███▉      | 1210/3086 [40:30<1:01:54,  1.98s/it] 39%|███▉      | 1211/3086 [40:32<1:02:45,  2.01s/it] 39%|███▉      | 1212/3086 [40:33<59:49,  1.92s/it]   39%|███▉      | 1213/3086 [40:35<1:00:12,  1.93s/it] 39%|███▉      | 1214/3086 [40:37<59:29,  1.91s/it]   39%|███▉      | 1215/3086 [40:39<1:02:34,  2.01s/it] 39%|███▉      | 1216/3086 [40:41<1:00:56,  1.96s/it] 39%|███▉      | 1217/3086 [40:43<1:01:34,  1.98s/it] 39%|███▉      | 1218/3086 [40:46<1:04:56,  2.09s/it] 40%|███▉      | 1219/3086 [40:48<1:05:44,  2.11s/it] 40%|███▉      | 1220/3086 [40:50<1:11:14,  2.29s/it]                                                     {'loss': 0.4855, 'grad_norm': 0.09084562212228775, 'learning_rate': 0.000362799740764744, 'epoch': 0.4}
 40%|███▉      | 1220/3086 [40:50<1:11:14,  2.29s/it] 40%|███▉      | 1221/3086 [40:52<1:08:22,  2.20s/it] 40%|███▉      | 1222/3086 [40:55<1:08:53,  2.22s/it] 40%|███▉      | 1223/3086 [40:56<1:04:21,  2.07s/it] 40%|███▉      | 1224/3086 [40:59<1:07:03,  2.16s/it] 40%|███▉      | 1225/3086 [41:01<1:04:40,  2.09s/it] 40%|███▉      | 1226/3086 [41:03<1:06:04,  2.13s/it] 40%|███▉      | 1227/3086 [41:05<1:07:08,  2.17s/it] 40%|███▉      | 1228/3086 [41:07<1:06:56,  2.16s/it] 40%|███▉      | 1229/3086 [41:09<1:02:18,  2.01s/it] 40%|███▉      | 1230/3086 [41:11<1:04:55,  2.10s/it]                                                     {'loss': 0.4867, 'grad_norm': 0.08763699978590012, 'learning_rate': 0.00036085547634478283, 'epoch': 0.4}
 40%|███▉      | 1230/3086 [41:11<1:04:55,  2.10s/it] 40%|███▉      | 1231/3086 [41:13<1:04:59,  2.10s/it] 40%|███▉      | 1232/3086 [41:17<1:13:36,  2.38s/it] 40%|███▉      | 1233/3086 [41:18<1:09:28,  2.25s/it] 40%|███▉      | 1234/3086 [41:20<1:07:04,  2.17s/it] 40%|████      | 1235/3086 [41:22<1:02:02,  2.01s/it] 40%|████      | 1236/3086 [41:24<1:03:39,  2.06s/it] 40%|████      | 1237/3086 [41:26<1:03:07,  2.05s/it] 40%|████      | 1238/3086 [41:28<1:00:45,  1.97s/it] 40%|████      | 1239/3086 [41:30<1:01:27,  2.00s/it] 40%|████      | 1240/3086 [41:33<1:06:07,  2.15s/it]                                                     {'loss': 0.4952, 'grad_norm': 0.08355551213026047, 'learning_rate': 0.0003589112119248218, 'epoch': 0.4}
 40%|████      | 1240/3086 [41:33<1:06:07,  2.15s/it] 40%|████      | 1241/3086 [41:35<1:06:12,  2.15s/it] 40%|████      | 1242/3086 [41:36<1:01:46,  2.01s/it] 40%|████      | 1243/3086 [41:39<1:03:59,  2.08s/it] 40%|████      | 1244/3086 [41:41<1:04:36,  2.10s/it] 40%|████      | 1245/3086 [41:43<1:01:34,  2.01s/it] 40%|████      | 1246/3086 [41:45<1:01:02,  1.99s/it] 40%|████      | 1247/3086 [41:46<57:13,  1.87s/it]   40%|████      | 1248/3086 [41:48<55:22,  1.81s/it] 40%|████      | 1249/3086 [41:50<57:13,  1.87s/it] 41%|████      | 1250/3086 [41:52<59:17,  1.94s/it]                                                   {'loss': 0.4821, 'grad_norm': 0.0788012370467186, 'learning_rate': 0.00035696694750486066, 'epoch': 0.41}
 41%|████      | 1250/3086 [41:52<59:17,  1.94s/it] 41%|████      | 1251/3086 [41:54<58:22,  1.91s/it] 41%|████      | 1252/3086 [41:56<57:25,  1.88s/it] 41%|████      | 1253/3086 [41:57<55:51,  1.83s/it] 41%|████      | 1254/3086 [42:00<1:01:51,  2.03s/it] 41%|████      | 1255/3086 [42:02<1:02:06,  2.04s/it] 41%|████      | 1256/3086 [42:05<1:09:16,  2.27s/it] 41%|████      | 1257/3086 [42:07<1:09:34,  2.28s/it] 41%|████      | 1258/3086 [42:09<1:06:56,  2.20s/it] 41%|████      | 1259/3086 [42:12<1:10:01,  2.30s/it] 41%|████      | 1260/3086 [42:14<1:07:01,  2.20s/it]                                                     {'loss': 0.4847, 'grad_norm': 0.0930870994925499, 'learning_rate': 0.0003550226830848995, 'epoch': 0.41}
 41%|████      | 1260/3086 [42:14<1:07:01,  2.20s/it] 41%|████      | 1261/3086 [42:15<1:03:03,  2.07s/it] 41%|████      | 1262/3086 [42:17<1:00:44,  2.00s/it] 41%|████      | 1263/3086 [42:19<58:49,  1.94s/it]   41%|████      | 1264/3086 [42:21<1:00:45,  2.00s/it] 41%|████      | 1265/3086 [42:23<1:03:01,  2.08s/it] 41%|████      | 1266/3086 [42:25<59:20,  1.96s/it]   41%|████      | 1267/3086 [42:27<56:49,  1.87s/it] 41%|████      | 1268/3086 [42:29<57:11,  1.89s/it] 41%|████      | 1269/3086 [42:31<1:00:50,  2.01s/it] 41%|████      | 1270/3086 [42:33<59:21,  1.96s/it]                                                     {'loss': 0.4842, 'grad_norm': 0.09963542222976685, 'learning_rate': 0.00035307841866493843, 'epoch': 0.41}
 41%|████      | 1270/3086 [42:33<59:21,  1.96s/it] 41%|████      | 1271/3086 [42:35<58:14,  1.93s/it] 41%|████      | 1272/3086 [42:37<59:32,  1.97s/it] 41%|████▏     | 1273/3086 [42:39<59:19,  1.96s/it] 41%|████▏     | 1274/3086 [42:41<1:01:19,  2.03s/it] 41%|████▏     | 1275/3086 [42:43<1:05:30,  2.17s/it] 41%|████▏     | 1276/3086 [42:45<1:01:36,  2.04s/it] 41%|████▏     | 1277/3086 [42:47<1:00:49,  2.02s/it] 41%|████▏     | 1278/3086 [42:49<1:02:13,  2.07s/it] 41%|████▏     | 1279/3086 [42:52<1:08:25,  2.27s/it] 41%|████▏     | 1280/3086 [42:54<1:03:44,  2.12s/it]                                                     {'loss': 0.4718, 'grad_norm': 0.09731431305408478, 'learning_rate': 0.00035113415424497727, 'epoch': 0.41}
 41%|████▏     | 1280/3086 [42:54<1:03:44,  2.12s/it] 42%|████▏     | 1281/3086 [42:56<1:04:47,  2.15s/it] 42%|████▏     | 1282/3086 [42:58<1:08:14,  2.27s/it] 42%|████▏     | 1283/3086 [43:01<1:08:30,  2.28s/it] 42%|████▏     | 1284/3086 [43:03<1:10:40,  2.35s/it] 42%|████▏     | 1285/3086 [43:06<1:09:38,  2.32s/it] 42%|████▏     | 1286/3086 [43:07<1:05:27,  2.18s/it] 42%|████▏     | 1287/3086 [43:10<1:09:21,  2.31s/it] 42%|████▏     | 1288/3086 [43:12<1:03:46,  2.13s/it] 42%|████▏     | 1289/3086 [43:13<59:37,  1.99s/it]   42%|████▏     | 1290/3086 [43:15<57:30,  1.92s/it]                                                   {'loss': 0.4653, 'grad_norm': 0.08532173186540604, 'learning_rate': 0.00034918988982501615, 'epoch': 0.42}
 42%|████▏     | 1290/3086 [43:15<57:30,  1.92s/it] 42%|████▏     | 1291/3086 [43:17<55:39,  1.86s/it] 42%|████▏     | 1292/3086 [43:19<55:13,  1.85s/it] 42%|████▏     | 1293/3086 [43:20<54:15,  1.82s/it] 42%|████▏     | 1294/3086 [43:22<51:34,  1.73s/it] 42%|████▏     | 1295/3086 [43:24<50:58,  1.71s/it] 42%|████▏     | 1296/3086 [43:25<48:45,  1.63s/it] 42%|████▏     | 1297/3086 [43:28<57:22,  1.92s/it] 42%|████▏     | 1298/3086 [43:29<56:21,  1.89s/it] 42%|████▏     | 1299/3086 [43:32<57:57,  1.95s/it] 42%|████▏     | 1300/3086 [43:34<1:05:57,  2.22s/it]                                                     {'loss': 0.4659, 'grad_norm': 0.10221510380506516, 'learning_rate': 0.0003472456254050551, 'epoch': 0.42}
 42%|████▏     | 1300/3086 [43:34<1:05:57,  2.22s/it] 42%|████▏     | 1301/3086 [43:36<1:02:11,  2.09s/it] 42%|████▏     | 1302/3086 [43:39<1:04:32,  2.17s/it] 42%|████▏     | 1303/3086 [43:40<59:54,  2.02s/it]   42%|████▏     | 1304/3086 [43:42<56:45,  1.91s/it] 42%|████▏     | 1305/3086 [43:44<59:55,  2.02s/it] 42%|████▏     | 1306/3086 [43:46<1:02:03,  2.09s/it] 42%|████▏     | 1307/3086 [43:49<1:02:31,  2.11s/it] 42%|████▏     | 1308/3086 [43:50<1:01:16,  2.07s/it] 42%|████▏     | 1309/3086 [43:53<1:03:56,  2.16s/it] 42%|████▏     | 1310/3086 [43:55<1:00:30,  2.04s/it]                                                     {'loss': 0.4471, 'grad_norm': 0.09108416736125946, 'learning_rate': 0.00034530136098509393, 'epoch': 0.42}
 42%|████▏     | 1310/3086 [43:55<1:00:30,  2.04s/it] 42%|████▏     | 1311/3086 [43:56<57:10,  1.93s/it]   43%|████▎     | 1312/3086 [43:58<58:05,  1.96s/it] 43%|████▎     | 1313/3086 [44:01<1:00:28,  2.05s/it] 43%|████▎     | 1314/3086 [44:02<59:06,  2.00s/it]   43%|████▎     | 1315/3086 [44:04<56:23,  1.91s/it] 43%|████▎     | 1316/3086 [44:06<59:48,  2.03s/it] 43%|████▎     | 1317/3086 [44:08<58:03,  1.97s/it] 43%|████▎     | 1318/3086 [44:10<55:15,  1.88s/it] 43%|████▎     | 1319/3086 [44:12<56:11,  1.91s/it] 43%|████▎     | 1320/3086 [44:14<59:15,  2.01s/it]                                                   {'loss': 0.449, 'grad_norm': 0.11008844524621964, 'learning_rate': 0.00034335709656513287, 'epoch': 0.43}
 43%|████▎     | 1320/3086 [44:14<59:15,  2.01s/it] 43%|████▎     | 1321/3086 [44:16<57:51,  1.97s/it] 43%|████▎     | 1322/3086 [44:18<53:33,  1.82s/it] 43%|████▎     | 1323/3086 [44:20<57:46,  1.97s/it] 43%|████▎     | 1324/3086 [44:23<1:05:06,  2.22s/it] 43%|████▎     | 1325/3086 [44:24<59:39,  2.03s/it]   43%|████▎     | 1326/3086 [44:26<56:20,  1.92s/it] 43%|████▎     | 1327/3086 [44:28<58:54,  2.01s/it] 43%|████▎     | 1328/3086 [44:30<57:58,  1.98s/it] 43%|████▎     | 1329/3086 [44:32<56:54,  1.94s/it] 43%|████▎     | 1330/3086 [44:34<57:49,  1.98s/it]                                                   {'loss': 0.4456, 'grad_norm': 0.11564933508634567, 'learning_rate': 0.0003414128321451717, 'epoch': 0.43}
 43%|████▎     | 1330/3086 [44:34<57:49,  1.98s/it] 43%|████▎     | 1331/3086 [44:36<57:04,  1.95s/it] 43%|████▎     | 1332/3086 [44:39<1:04:42,  2.21s/it] 43%|████▎     | 1333/3086 [44:41<1:01:37,  2.11s/it] 43%|████▎     | 1334/3086 [44:43<1:02:46,  2.15s/it] 43%|████▎     | 1335/3086 [44:45<1:01:48,  2.12s/it] 43%|████▎     | 1336/3086 [44:47<1:01:47,  2.12s/it] 43%|████▎     | 1337/3086 [44:49<1:00:07,  2.06s/it] 43%|████▎     | 1338/3086 [44:51<58:30,  2.01s/it]   43%|████▎     | 1339/3086 [44:53<1:03:04,  2.17s/it] 43%|████▎     | 1340/3086 [44:56<1:07:40,  2.33s/it]                                                     {'loss': 0.4339, 'grad_norm': 0.10285110026597977, 'learning_rate': 0.0003394685677252106, 'epoch': 0.43}
 43%|████▎     | 1340/3086 [44:56<1:07:40,  2.33s/it] 43%|████▎     | 1341/3086 [44:59<1:11:22,  2.45s/it] 43%|████▎     | 1342/3086 [45:01<1:05:26,  2.25s/it] 44%|████▎     | 1343/3086 [45:03<1:06:24,  2.29s/it] 44%|████▎     | 1344/3086 [45:05<1:05:24,  2.25s/it] 44%|████▎     | 1345/3086 [45:07<1:05:35,  2.26s/it] 44%|████▎     | 1346/3086 [45:10<1:08:17,  2.35s/it] 44%|████▎     | 1347/3086 [45:12<1:02:49,  2.17s/it] 44%|████▎     | 1348/3086 [45:14<1:01:00,  2.11s/it] 44%|████▎     | 1349/3086 [45:15<57:33,  1.99s/it]   44%|████▎     | 1350/3086 [45:17<57:19,  1.98s/it]                                                   {'loss': 0.4143, 'grad_norm': 0.09415064007043839, 'learning_rate': 0.00033752430330524953, 'epoch': 0.44}
 44%|████▎     | 1350/3086 [45:17<57:19,  1.98s/it] 44%|████▍     | 1351/3086 [45:20<1:03:17,  2.19s/it] 44%|████▍     | 1352/3086 [45:22<1:03:57,  2.21s/it] 44%|████▍     | 1353/3086 [45:24<1:01:06,  2.12s/it] 44%|████▍     | 1354/3086 [45:26<57:58,  2.01s/it]   44%|████▍     | 1355/3086 [45:28<56:57,  1.97s/it] 44%|████▍     | 1356/3086 [45:30<1:01:30,  2.13s/it] 44%|████▍     | 1357/3086 [45:33<1:02:51,  2.18s/it] 44%|████▍     | 1358/3086 [45:34<1:00:17,  2.09s/it] 44%|████▍     | 1359/3086 [45:36<58:07,  2.02s/it]   44%|████▍     | 1360/3086 [45:39<1:00:40,  2.11s/it]                                                     {'loss': 0.4055, 'grad_norm': 0.0997551679611206, 'learning_rate': 0.00033558003888528836, 'epoch': 0.44}
 44%|████▍     | 1360/3086 [45:39<1:00:40,  2.11s/it] 44%|████▍     | 1361/3086 [45:41<59:40,  2.08s/it]   44%|████▍     | 1362/3086 [45:43<1:00:29,  2.11s/it] 44%|████▍     | 1363/3086 [45:45<57:21,  2.00s/it]   44%|████▍     | 1364/3086 [45:46<56:04,  1.95s/it] 44%|████▍     | 1365/3086 [45:48<56:36,  1.97s/it] 44%|████▍     | 1366/3086 [45:50<54:38,  1.91s/it] 44%|████▍     | 1367/3086 [45:52<52:11,  1.82s/it] 44%|████▍     | 1368/3086 [45:54<52:03,  1.82s/it] 44%|████▍     | 1369/3086 [45:56<53:39,  1.88s/it] 44%|████▍     | 1370/3086 [45:58<56:32,  1.98s/it]                                                   {'loss': 0.4106, 'grad_norm': 0.09548933804035187, 'learning_rate': 0.00033363577446532725, 'epoch': 0.44}
 44%|████▍     | 1370/3086 [45:58<56:32,  1.98s/it] 44%|████▍     | 1371/3086 [46:00<55:19,  1.94s/it] 44%|████▍     | 1372/3086 [46:02<1:00:41,  2.12s/it] 44%|████▍     | 1373/3086 [46:05<1:07:17,  2.36s/it] 45%|████▍     | 1374/3086 [46:07<1:03:45,  2.23s/it] 45%|████▍     | 1375/3086 [46:10<1:07:12,  2.36s/it] 45%|████▍     | 1376/3086 [46:12<1:02:53,  2.21s/it] 45%|████▍     | 1377/3086 [46:14<1:04:25,  2.26s/it] 45%|████▍     | 1378/3086 [46:16<59:42,  2.10s/it]   45%|████▍     | 1379/3086 [46:17<57:11,  2.01s/it] 45%|████▍     | 1380/3086 [46:19<56:44,  2.00s/it]                                                   {'loss': 0.3932, 'grad_norm': 0.10213855654001236, 'learning_rate': 0.00033169151004536614, 'epoch': 0.45}
 45%|████▍     | 1380/3086 [46:19<56:44,  2.00s/it] 45%|████▍     | 1381/3086 [46:21<54:11,  1.91s/it] 45%|████▍     | 1382/3086 [46:23<55:28,  1.95s/it] 45%|████▍     | 1383/3086 [46:25<55:15,  1.95s/it] 45%|████▍     | 1384/3086 [46:27<53:38,  1.89s/it] 45%|████▍     | 1385/3086 [46:29<51:06,  1.80s/it] 45%|████▍     | 1386/3086 [46:30<52:16,  1.84s/it] 45%|████▍     | 1387/3086 [46:33<1:00:46,  2.15s/it] 45%|████▍     | 1388/3086 [46:36<1:03:06,  2.23s/it] 45%|████▌     | 1389/3086 [46:37<58:08,  2.06s/it]   45%|████▌     | 1390/3086 [46:39<56:42,  2.01s/it]                                                   {'loss': 0.3876, 'grad_norm': 0.1134793609380722, 'learning_rate': 0.000329747245625405, 'epoch': 0.45}
 45%|████▌     | 1390/3086 [46:39<56:42,  2.01s/it] 45%|████▌     | 1391/3086 [46:41<58:11,  2.06s/it] 45%|████▌     | 1392/3086 [46:43<57:18,  2.03s/it] 45%|████▌     | 1393/3086 [46:45<55:47,  1.98s/it] 45%|████▌     | 1394/3086 [46:47<54:24,  1.93s/it] 45%|████▌     | 1395/3086 [46:49<54:32,  1.94s/it] 45%|████▌     | 1396/3086 [46:52<1:00:09,  2.14s/it] 45%|████▌     | 1397/3086 [46:54<1:04:30,  2.29s/it] 45%|████▌     | 1398/3086 [46:56<1:01:32,  2.19s/it] 45%|████▌     | 1399/3086 [46:58<1:00:50,  2.16s/it] 45%|████▌     | 1400/3086 [47:01<1:03:39,  2.27s/it]                                                     {'loss': 0.3765, 'grad_norm': 0.0962204709649086, 'learning_rate': 0.00032780298120544386, 'epoch': 0.45}
 45%|████▌     | 1400/3086 [47:01<1:03:39,  2.27s/it] 45%|████▌     | 1401/3086 [47:03<1:05:18,  2.33s/it] 45%|████▌     | 1402/3086 [47:05<1:01:35,  2.19s/it] 45%|████▌     | 1403/3086 [47:07<58:04,  2.07s/it]   45%|████▌     | 1404/3086 [47:09<56:31,  2.02s/it] 46%|████▌     | 1405/3086 [47:11<55:50,  1.99s/it] 46%|████▌     | 1406/3086 [47:13<56:15,  2.01s/it] 46%|████▌     | 1407/3086 [47:15<56:30,  2.02s/it] 46%|████▌     | 1408/3086 [47:17<58:24,  2.09s/it] 46%|████▌     | 1409/3086 [47:19<53:22,  1.91s/it] 46%|████▌     | 1410/3086 [47:21<56:03,  2.01s/it]                                                   {'loss': 0.3732, 'grad_norm': 0.10497494041919708, 'learning_rate': 0.0003258587167854828, 'epoch': 0.46}
 46%|████▌     | 1410/3086 [47:21<56:03,  2.01s/it] 46%|████▌     | 1411/3086 [47:23<53:09,  1.90s/it] 46%|████▌     | 1412/3086 [47:24<52:35,  1.89s/it] 46%|████▌     | 1413/3086 [47:27<54:54,  1.97s/it] 46%|████▌     | 1414/3086 [47:29<56:26,  2.03s/it] 46%|████▌     | 1415/3086 [47:31<56:29,  2.03s/it] 46%|████▌     | 1416/3086 [47:33<54:23,  1.95s/it] 46%|████▌     | 1417/3086 [47:35<57:52,  2.08s/it] 46%|████▌     | 1418/3086 [47:37<54:54,  1.98s/it] 46%|████▌     | 1419/3086 [47:39<54:49,  1.97s/it] 46%|████▌     | 1420/3086 [47:40<53:28,  1.93s/it]                                                   {'loss': 0.3659, 'grad_norm': 0.10134405642747879, 'learning_rate': 0.0003239144523655217, 'epoch': 0.46}
 46%|████▌     | 1420/3086 [47:40<53:28,  1.93s/it] 46%|████▌     | 1421/3086 [47:43<57:02,  2.06s/it] 46%|████▌     | 1422/3086 [47:45<54:33,  1.97s/it] 46%|████▌     | 1423/3086 [47:47<54:38,  1.97s/it] 46%|████▌     | 1424/3086 [47:49<56:02,  2.02s/it] 46%|████▌     | 1425/3086 [47:51<54:42,  1.98s/it] 46%|████▌     | 1426/3086 [47:52<52:12,  1.89s/it] 46%|████▌     | 1427/3086 [47:54<52:59,  1.92s/it] 46%|████▋     | 1428/3086 [47:56<54:57,  1.99s/it] 46%|████▋     | 1429/3086 [47:58<54:30,  1.97s/it] 46%|████▋     | 1430/3086 [48:00<52:58,  1.92s/it]                                                   {'loss': 0.359, 'grad_norm': 0.08720172941684723, 'learning_rate': 0.00032197018794556057, 'epoch': 0.46}
 46%|████▋     | 1430/3086 [48:00<52:58,  1.92s/it] 46%|████▋     | 1431/3086 [48:02<57:06,  2.07s/it] 46%|████▋     | 1432/3086 [48:05<56:41,  2.06s/it] 46%|████▋     | 1433/3086 [48:06<53:58,  1.96s/it] 46%|████▋     | 1434/3086 [48:08<54:00,  1.96s/it] 47%|████▋     | 1435/3086 [48:10<54:14,  1.97s/it] 47%|████▋     | 1436/3086 [48:13<57:36,  2.10s/it] 47%|████▋     | 1437/3086 [48:14<54:32,  1.98s/it] 47%|████▋     | 1438/3086 [48:17<56:42,  2.06s/it] 47%|████▋     | 1439/3086 [48:19<58:11,  2.12s/it] 47%|████▋     | 1440/3086 [48:21<1:00:50,  2.22s/it]                                                     {'loss': 0.3614, 'grad_norm': 0.09966882318258286, 'learning_rate': 0.00032002592352559946, 'epoch': 0.47}
 47%|████▋     | 1440/3086 [48:21<1:00:50,  2.22s/it] 47%|████▋     | 1441/3086 [48:23<58:58,  2.15s/it]   47%|████▋     | 1442/3086 [48:25<57:18,  2.09s/it] 47%|████▋     | 1443/3086 [48:27<57:12,  2.09s/it] 47%|████▋     | 1444/3086 [48:29<57:36,  2.11s/it] 47%|████▋     | 1445/3086 [48:32<57:39,  2.11s/it] 47%|████▋     | 1446/3086 [48:33<55:29,  2.03s/it] 47%|████▋     | 1447/3086 [48:36<56:40,  2.07s/it] 47%|████▋     | 1448/3086 [48:38<56:39,  2.08s/it] 47%|████▋     | 1449/3086 [48:40<58:15,  2.14s/it] 47%|████▋     | 1450/3086 [48:42<1:01:13,  2.25s/it]                                                     {'loss': 0.3497, 'grad_norm': 0.08914859592914581, 'learning_rate': 0.00031808165910563835, 'epoch': 0.47}
 47%|████▋     | 1450/3086 [48:42<1:01:13,  2.25s/it] 47%|████▋     | 1451/3086 [48:44<58:24,  2.14s/it]   47%|████▋     | 1452/3086 [48:46<55:38,  2.04s/it] 47%|████▋     | 1453/3086 [48:48<55:55,  2.05s/it] 47%|████▋     | 1454/3086 [48:50<56:09,  2.06s/it] 47%|████▋     | 1455/3086 [48:52<54:35,  2.01s/it] 47%|████▋     | 1456/3086 [48:54<53:52,  1.98s/it] 47%|████▋     | 1457/3086 [48:56<49:23,  1.82s/it] 47%|████▋     | 1458/3086 [48:57<50:03,  1.85s/it] 47%|████▋     | 1459/3086 [48:59<49:51,  1.84s/it] 47%|████▋     | 1460/3086 [49:01<52:32,  1.94s/it]                                                   {'loss': 0.3397, 'grad_norm': 0.09144099056720734, 'learning_rate': 0.00031613739468567723, 'epoch': 0.47}
 47%|████▋     | 1460/3086 [49:01<52:32,  1.94s/it] 47%|████▋     | 1461/3086 [49:04<57:02,  2.11s/it] 47%|████▋     | 1462/3086 [49:06<56:57,  2.10s/it] 47%|████▋     | 1463/3086 [49:08<54:06,  2.00s/it] 47%|████▋     | 1464/3086 [49:10<54:12,  2.01s/it] 47%|████▋     | 1465/3086 [49:12<57:19,  2.12s/it] 48%|████▊     | 1466/3086 [49:14<53:53,  2.00s/it] 48%|████▊     | 1467/3086 [49:16<53:44,  1.99s/it] 48%|████▊     | 1468/3086 [49:18<53:28,  1.98s/it] 48%|████▊     | 1469/3086 [49:20<52:21,  1.94s/it] 48%|████▊     | 1470/3086 [49:22<52:39,  1.96s/it]                                                   {'loss': 0.3418, 'grad_norm': 0.09200838208198547, 'learning_rate': 0.0003141931302657161, 'epoch': 0.48}
 48%|████▊     | 1470/3086 [49:22<52:39,  1.96s/it] 48%|████▊     | 1471/3086 [49:23<50:59,  1.89s/it] 48%|████▊     | 1472/3086 [49:26<52:47,  1.96s/it] 48%|████▊     | 1473/3086 [49:28<53:53,  2.00s/it] 48%|████▊     | 1474/3086 [49:30<53:16,  1.98s/it] 48%|████▊     | 1475/3086 [49:31<50:59,  1.90s/it] 48%|████▊     | 1476/3086 [49:34<53:41,  2.00s/it] 48%|████▊     | 1477/3086 [49:35<50:59,  1.90s/it] 48%|████▊     | 1478/3086 [49:38<55:34,  2.07s/it] 48%|████▊     | 1479/3086 [49:40<56:16,  2.10s/it] 48%|████▊     | 1480/3086 [49:42<57:36,  2.15s/it]                                                   {'loss': 0.3338, 'grad_norm': 0.10594216734170914, 'learning_rate': 0.00031224886584575495, 'epoch': 0.48}
 48%|████▊     | 1480/3086 [49:42<57:36,  2.15s/it] 48%|████▊     | 1481/3086 [49:44<54:54,  2.05s/it] 48%|████▊     | 1482/3086 [49:46<56:27,  2.11s/it] 48%|████▊     | 1483/3086 [49:48<53:12,  1.99s/it] 48%|████▊     | 1484/3086 [49:50<50:12,  1.88s/it] 48%|████▊     | 1485/3086 [49:51<48:59,  1.84s/it] 48%|████▊     | 1486/3086 [49:53<48:14,  1.81s/it] 48%|████▊     | 1487/3086 [49:55<49:26,  1.86s/it] 48%|████▊     | 1488/3086 [49:57<49:29,  1.86s/it] 48%|████▊     | 1489/3086 [49:59<48:46,  1.83s/it] 48%|████▊     | 1490/3086 [50:01<51:29,  1.94s/it]                                                   {'loss': 0.3337, 'grad_norm': 0.08404004573822021, 'learning_rate': 0.0003103046014257939, 'epoch': 0.48}
 48%|████▊     | 1490/3086 [50:01<51:29,  1.94s/it] 48%|████▊     | 1491/3086 [50:03<53:16,  2.00s/it] 48%|████▊     | 1492/3086 [50:05<54:14,  2.04s/it] 48%|████▊     | 1493/3086 [50:07<56:35,  2.13s/it] 48%|████▊     | 1494/3086 [50:09<55:13,  2.08s/it] 48%|████▊     | 1495/3086 [50:11<51:31,  1.94s/it] 48%|████▊     | 1496/3086 [50:13<53:24,  2.02s/it] 49%|████▊     | 1497/3086 [50:15<52:14,  1.97s/it] 49%|████▊     | 1498/3086 [50:17<53:04,  2.01s/it] 49%|████▊     | 1499/3086 [50:20<57:09,  2.16s/it] 49%|████▊     | 1500/3086 [50:21<53:20,  2.02s/it]                                                   {'loss': 0.3254, 'grad_norm': 0.09234552085399628, 'learning_rate': 0.0003083603370058328, 'epoch': 0.49}
 49%|████▊     | 1500/3086 [50:21<53:20,  2.02s/it] 49%|████▊     | 1501/3086 [50:24<54:17,  2.05s/it] 49%|████▊     | 1502/3086 [50:25<52:37,  1.99s/it] 49%|████▊     | 1503/3086 [50:27<52:22,  1.98s/it] 49%|████▊     | 1504/3086 [50:29<52:49,  2.00s/it] 49%|████▉     | 1505/3086 [50:32<56:47,  2.16s/it] 49%|████▉     | 1506/3086 [50:34<56:13,  2.13s/it] 49%|████▉     | 1507/3086 [50:36<53:29,  2.03s/it] 49%|████▉     | 1508/3086 [50:38<51:09,  1.95s/it] 49%|████▉     | 1509/3086 [50:39<51:03,  1.94s/it] 49%|████▉     | 1510/3086 [50:41<50:30,  1.92s/it]                                                   {'loss': 0.318, 'grad_norm': 0.08756928890943527, 'learning_rate': 0.0003064160725858716, 'epoch': 0.49}
 49%|████▉     | 1510/3086 [50:41<50:30,  1.92s/it] 49%|████▉     | 1511/3086 [50:44<55:11,  2.10s/it] 49%|████▉     | 1512/3086 [50:46<55:32,  2.12s/it] 49%|████▉     | 1513/3086 [50:48<52:18,  2.00s/it] 49%|████▉     | 1514/3086 [50:49<49:29,  1.89s/it] 49%|████▉     | 1515/3086 [50:51<48:47,  1.86s/it] 49%|████▉     | 1516/3086 [50:53<51:19,  1.96s/it] 49%|████▉     | 1517/3086 [50:55<51:55,  1.99s/it] 49%|████▉     | 1518/3086 [50:57<52:20,  2.00s/it] 49%|████▉     | 1519/3086 [51:00<54:19,  2.08s/it] 49%|████▉     | 1520/3086 [51:02<52:44,  2.02s/it]                                                   {'loss': 0.3187, 'grad_norm': 0.09104904532432556, 'learning_rate': 0.00030447180816591056, 'epoch': 0.49}
 49%|████▉     | 1520/3086 [51:02<52:44,  2.02s/it] 49%|████▉     | 1521/3086 [51:03<51:14,  1.96s/it] 49%|████▉     | 1522/3086 [51:05<50:16,  1.93s/it] 49%|████▉     | 1523/3086 [51:07<50:05,  1.92s/it] 49%|████▉     | 1524/3086 [51:09<49:54,  1.92s/it] 49%|████▉     | 1525/3086 [51:11<51:40,  1.99s/it] 49%|████▉     | 1526/3086 [51:14<54:54,  2.11s/it] 49%|████▉     | 1527/3086 [51:15<52:50,  2.03s/it] 50%|████▉     | 1528/3086 [51:17<49:31,  1.91s/it] 50%|████▉     | 1529/3086 [51:19<50:24,  1.94s/it] 50%|████▉     | 1530/3086 [51:22<54:46,  2.11s/it]                                                   {'loss': 0.33, 'grad_norm': 0.08582145720720291, 'learning_rate': 0.0003025275437459494, 'epoch': 0.5}
 50%|████▉     | 1530/3086 [51:22<54:46,  2.11s/it] 50%|████▉     | 1531/3086 [51:24<54:31,  2.10s/it] 50%|████▉     | 1532/3086 [51:26<55:59,  2.16s/it] 50%|████▉     | 1533/3086 [51:28<54:23,  2.10s/it] 50%|████▉     | 1534/3086 [51:30<52:20,  2.02s/it] 50%|████▉     | 1535/3086 [51:32<53:40,  2.08s/it] 50%|████▉     | 1536/3086 [51:34<53:32,  2.07s/it] 50%|████▉     | 1537/3086 [51:36<52:49,  2.05s/it] 50%|████▉     | 1538/3086 [51:38<52:21,  2.03s/it] 50%|████▉     | 1539/3086 [51:40<53:25,  2.07s/it] 50%|████▉     | 1540/3086 [51:42<53:37,  2.08s/it]                                                   {'loss': 0.308, 'grad_norm': 0.09527905285358429, 'learning_rate': 0.00030058327932598833, 'epoch': 0.5}
 50%|████▉     | 1540/3086 [51:42<53:37,  2.08s/it] 50%|████▉     | 1541/3086 [51:44<52:33,  2.04s/it] 50%|████▉     | 1542/3086 [51:46<53:44,  2.09s/it] 50%|█████     | 1543/3086 [51:48<53:07,  2.07s/it] 50%|█████     | 1544/3086 [51:50<50:35,  1.97s/it] 50%|█████     | 1545/3086 [51:52<50:22,  1.96s/it] 50%|█████     | 1546/3086 [51:54<49:22,  1.92s/it] 50%|█████     | 1547/3086 [51:56<51:53,  2.02s/it] 50%|█████     | 1548/3086 [51:58<51:58,  2.03s/it] 50%|█████     | 1549/3086 [52:00<50:45,  1.98s/it] 50%|█████     | 1550/3086 [52:02<49:35,  1.94s/it]                                                   {'loss': 0.3064, 'grad_norm': 0.08415897190570831, 'learning_rate': 0.0002986390149060272, 'epoch': 0.5}
 50%|█████     | 1550/3086 [52:02<49:35,  1.94s/it] 50%|█████     | 1551/3086 [52:04<51:21,  2.01s/it] 50%|█████     | 1552/3086 [52:06<50:28,  1.97s/it] 50%|█████     | 1553/3086 [52:08<52:40,  2.06s/it] 50%|█████     | 1554/3086 [52:10<52:40,  2.06s/it] 50%|█████     | 1555/3086 [52:12<49:54,  1.96s/it] 50%|█████     | 1556/3086 [52:14<49:23,  1.94s/it] 50%|█████     | 1557/3086 [52:16<48:00,  1.88s/it] 50%|█████     | 1558/3086 [52:18<53:43,  2.11s/it] 51%|█████     | 1559/3086 [52:21<53:49,  2.12s/it] 51%|█████     | 1560/3086 [52:22<51:08,  2.01s/it]                                                   {'loss': 0.3043, 'grad_norm': 0.09189354628324509, 'learning_rate': 0.0002966947504860661, 'epoch': 0.51}
 51%|█████     | 1560/3086 [52:22<51:08,  2.01s/it] 51%|█████     | 1561/3086 [52:24<48:29,  1.91s/it] 51%|█████     | 1562/3086 [52:26<47:29,  1.87s/it] 51%|█████     | 1563/3086 [52:28<49:35,  1.95s/it] 51%|█████     | 1564/3086 [52:30<47:34,  1.88s/it] 51%|█████     | 1565/3086 [52:31<46:37,  1.84s/it] 51%|█████     | 1566/3086 [52:33<47:01,  1.86s/it] 51%|█████     | 1567/3086 [52:35<49:12,  1.94s/it] 51%|█████     | 1568/3086 [52:37<49:49,  1.97s/it] 51%|█████     | 1569/3086 [52:39<48:29,  1.92s/it] 51%|█████     | 1570/3086 [52:41<50:48,  2.01s/it]                                                   {'loss': 0.3117, 'grad_norm': 0.09052983671426773, 'learning_rate': 0.00029475048606610494, 'epoch': 0.51}
 51%|█████     | 1570/3086 [52:41<50:48,  2.01s/it] 51%|█████     | 1571/3086 [52:44<58:20,  2.31s/it] 51%|█████     | 1572/3086 [52:46<53:56,  2.14s/it] 51%|█████     | 1573/3086 [52:49<57:20,  2.27s/it] 51%|█████     | 1574/3086 [52:51<55:03,  2.19s/it] 51%|█████     | 1575/3086 [52:52<50:56,  2.02s/it] 51%|█████     | 1576/3086 [52:55<54:32,  2.17s/it] 51%|█████     | 1577/3086 [52:57<51:10,  2.03s/it] 51%|█████     | 1578/3086 [52:58<48:16,  1.92s/it] 51%|█████     | 1579/3086 [53:00<50:22,  2.01s/it] 51%|█████     | 1580/3086 [53:02<49:38,  1.98s/it]                                                   {'loss': 0.3041, 'grad_norm': 0.08684022724628448, 'learning_rate': 0.0002928062216461438, 'epoch': 0.51}
 51%|█████     | 1580/3086 [53:02<49:38,  1.98s/it] 51%|█████     | 1581/3086 [53:04<47:49,  1.91s/it] 51%|█████▏    | 1582/3086 [53:06<47:23,  1.89s/it] 51%|█████▏    | 1583/3086 [53:08<47:39,  1.90s/it] 51%|█████▏    | 1584/3086 [53:10<45:54,  1.83s/it] 51%|█████▏    | 1585/3086 [53:11<44:43,  1.79s/it] 51%|█████▏    | 1586/3086 [53:13<45:35,  1.82s/it] 51%|█████▏    | 1587/3086 [53:15<48:18,  1.93s/it] 51%|█████▏    | 1588/3086 [53:17<48:00,  1.92s/it] 51%|█████▏    | 1589/3086 [53:20<50:27,  2.02s/it] 52%|█████▏    | 1590/3086 [53:21<49:23,  1.98s/it]                                                   {'loss': 0.2975, 'grad_norm': 0.09260820597410202, 'learning_rate': 0.00029086195722618276, 'epoch': 0.52}
 52%|█████▏    | 1590/3086 [53:21<49:23,  1.98s/it] 52%|█████▏    | 1591/3086 [53:24<53:43,  2.16s/it] 52%|█████▏    | 1592/3086 [53:26<51:42,  2.08s/it] 52%|█████▏    | 1593/3086 [53:28<51:42,  2.08s/it] 52%|█████▏    | 1594/3086 [53:30<51:58,  2.09s/it] 52%|█████▏    | 1595/3086 [53:32<50:22,  2.03s/it] 52%|█████▏    | 1596/3086 [53:34<48:01,  1.93s/it] 52%|█████▏    | 1597/3086 [53:35<45:07,  1.82s/it] 52%|█████▏    | 1598/3086 [53:37<47:18,  1.91s/it] 52%|█████▏    | 1599/3086 [53:39<48:48,  1.97s/it] 52%|█████▏    | 1600/3086 [53:41<48:49,  1.97s/it]                                                   {'loss': 0.3007, 'grad_norm': 0.08708906173706055, 'learning_rate': 0.00028891769280622165, 'epoch': 0.52}
 52%|█████▏    | 1600/3086 [53:41<48:49,  1.97s/it] 52%|█████▏    | 1601/3086 [53:44<54:23,  2.20s/it] 52%|█████▏    | 1602/3086 [53:46<52:01,  2.10s/it] 52%|█████▏    | 1603/3086 [53:48<50:55,  2.06s/it] 52%|█████▏    | 1604/3086 [53:50<49:53,  2.02s/it] 52%|█████▏    | 1605/3086 [53:52<47:15,  1.91s/it] 52%|█████▏    | 1606/3086 [53:54<48:08,  1.95s/it] 52%|█████▏    | 1607/3086 [53:56<48:59,  1.99s/it] 52%|█████▏    | 1608/3086 [53:58<52:33,  2.13s/it] 52%|█████▏    | 1609/3086 [54:00<51:25,  2.09s/it] 52%|█████▏    | 1610/3086 [54:03<55:02,  2.24s/it]                                                   {'loss': 0.305, 'grad_norm': 0.08330544084310532, 'learning_rate': 0.0002869734283862605, 'epoch': 0.52}
 52%|█████▏    | 1610/3086 [54:03<55:02,  2.24s/it] 52%|█████▏    | 1611/3086 [54:05<55:50,  2.27s/it] 52%|█████▏    | 1612/3086 [54:07<50:42,  2.06s/it] 52%|█████▏    | 1613/3086 [54:09<53:15,  2.17s/it] 52%|█████▏    | 1614/3086 [54:11<50:31,  2.06s/it] 52%|█████▏    | 1615/3086 [54:13<52:42,  2.15s/it] 52%|█████▏    | 1616/3086 [54:15<50:11,  2.05s/it] 52%|█████▏    | 1617/3086 [54:17<50:57,  2.08s/it] 52%|█████▏    | 1618/3086 [54:19<49:17,  2.01s/it] 52%|█████▏    | 1619/3086 [54:21<48:21,  1.98s/it] 52%|█████▏    | 1620/3086 [54:23<49:53,  2.04s/it]                                                   {'loss': 0.2947, 'grad_norm': 0.08161729574203491, 'learning_rate': 0.00028502916396629937, 'epoch': 0.52}
 52%|█████▏    | 1620/3086 [54:23<49:53,  2.04s/it] 53%|█████▎    | 1621/3086 [54:25<48:33,  1.99s/it] 53%|█████▎    | 1622/3086 [54:27<46:51,  1.92s/it] 53%|█████▎    | 1623/3086 [54:29<49:53,  2.05s/it] 53%|█████▎    | 1624/3086 [54:31<45:58,  1.89s/it] 53%|█████▎    | 1625/3086 [54:33<47:18,  1.94s/it] 53%|█████▎    | 1626/3086 [54:35<47:56,  1.97s/it] 53%|█████▎    | 1627/3086 [54:37<48:26,  1.99s/it] 53%|█████▎    | 1628/3086 [54:39<47:02,  1.94s/it] 53%|█████▎    | 1629/3086 [54:41<52:23,  2.16s/it] 53%|█████▎    | 1630/3086 [54:44<54:26,  2.24s/it]                                                   {'loss': 0.3005, 'grad_norm': 0.08176001906394958, 'learning_rate': 0.00028308489954633826, 'epoch': 0.53}
 53%|█████▎    | 1630/3086 [54:44<54:26,  2.24s/it] 53%|█████▎    | 1631/3086 [54:46<51:57,  2.14s/it] 53%|█████▎    | 1632/3086 [54:47<49:13,  2.03s/it] 53%|█████▎    | 1633/3086 [54:49<46:30,  1.92s/it] 53%|█████▎    | 1634/3086 [54:51<46:45,  1.93s/it] 53%|█████▎    | 1635/3086 [54:53<45:07,  1.87s/it] 53%|█████▎    | 1636/3086 [54:55<48:29,  2.01s/it] 53%|█████▎    | 1637/3086 [54:58<52:26,  2.17s/it] 53%|█████▎    | 1638/3086 [55:00<51:53,  2.15s/it] 53%|█████▎    | 1639/3086 [55:02<49:49,  2.07s/it] 53%|█████▎    | 1640/3086 [55:04<51:05,  2.12s/it]                                                   {'loss': 0.2946, 'grad_norm': 0.09407392144203186, 'learning_rate': 0.0002811406351263772, 'epoch': 0.53}
 53%|█████▎    | 1640/3086 [55:04<51:05,  2.12s/it] 53%|█████▎    | 1641/3086 [55:06<49:57,  2.07s/it] 53%|█████▎    | 1642/3086 [55:08<48:47,  2.03s/it] 53%|█████▎    | 1643/3086 [55:10<47:57,  1.99s/it] 53%|█████▎    | 1644/3086 [55:12<48:15,  2.01s/it] 53%|█████▎    | 1645/3086 [55:14<50:12,  2.09s/it] 53%|█████▎    | 1646/3086 [55:16<48:34,  2.02s/it] 53%|█████▎    | 1647/3086 [55:18<48:50,  2.04s/it] 53%|█████▎    | 1648/3086 [55:20<48:17,  2.01s/it] 53%|█████▎    | 1649/3086 [55:22<48:29,  2.02s/it] 53%|█████▎    | 1650/3086 [55:24<47:10,  1.97s/it]                                                   {'loss': 0.2981, 'grad_norm': 0.07954119145870209, 'learning_rate': 0.00027919637070641603, 'epoch': 0.53}
 53%|█████▎    | 1650/3086 [55:24<47:10,  1.97s/it] 53%|█████▎    | 1651/3086 [55:25<44:49,  1.87s/it] 54%|█████▎    | 1652/3086 [55:27<43:50,  1.83s/it] 54%|█████▎    | 1653/3086 [55:29<43:48,  1.83s/it] 54%|█████▎    | 1654/3086 [55:31<43:26,  1.82s/it] 54%|█████▎    | 1655/3086 [55:33<45:35,  1.91s/it] 54%|█████▎    | 1656/3086 [55:35<46:09,  1.94s/it] 54%|█████▎    | 1657/3086 [55:36<42:44,  1.79s/it] 54%|█████▎    | 1658/3086 [55:39<46:08,  1.94s/it] 54%|█████▍    | 1659/3086 [55:41<46:14,  1.94s/it] 54%|█████▍    | 1660/3086 [55:43<46:43,  1.97s/it]                                                   {'loss': 0.2961, 'grad_norm': 0.08659027516841888, 'learning_rate': 0.0002772521062864549, 'epoch': 0.54}
 54%|█████▍    | 1660/3086 [55:43<46:43,  1.97s/it] 54%|█████▍    | 1661/3086 [55:44<45:34,  1.92s/it] 54%|█████▍    | 1662/3086 [55:46<46:49,  1.97s/it] 54%|█████▍    | 1663/3086 [55:48<45:19,  1.91s/it] 54%|█████▍    | 1664/3086 [55:50<44:04,  1.86s/it] 54%|█████▍    | 1665/3086 [55:52<46:06,  1.95s/it] 54%|█████▍    | 1666/3086 [55:54<47:09,  1.99s/it] 54%|█████▍    | 1667/3086 [55:57<50:33,  2.14s/it] 54%|█████▍    | 1668/3086 [55:59<48:20,  2.05s/it] 54%|█████▍    | 1669/3086 [56:00<46:35,  1.97s/it] 54%|█████▍    | 1670/3086 [56:02<45:15,  1.92s/it]                                                   {'loss': 0.2901, 'grad_norm': 0.08410580456256866, 'learning_rate': 0.0002753078418664938, 'epoch': 0.54}
 54%|█████▍    | 1670/3086 [56:02<45:15,  1.92s/it] 54%|█████▍    | 1671/3086 [56:04<47:27,  2.01s/it] 54%|█████▍    | 1672/3086 [56:06<45:44,  1.94s/it] 54%|█████▍    | 1673/3086 [56:08<45:41,  1.94s/it] 54%|█████▍    | 1674/3086 [56:10<44:09,  1.88s/it] 54%|█████▍    | 1675/3086 [56:12<43:04,  1.83s/it] 54%|█████▍    | 1676/3086 [56:13<43:54,  1.87s/it] 54%|█████▍    | 1677/3086 [56:16<45:30,  1.94s/it] 54%|█████▍    | 1678/3086 [56:18<48:22,  2.06s/it] 54%|█████▍    | 1679/3086 [56:20<46:43,  1.99s/it] 54%|█████▍    | 1680/3086 [56:22<47:32,  2.03s/it]                                                   {'loss': 0.2937, 'grad_norm': 0.08198819309473038, 'learning_rate': 0.0002733635774465327, 'epoch': 0.54}
 54%|█████▍    | 1680/3086 [56:22<47:32,  2.03s/it] 54%|█████▍    | 1681/3086 [56:24<44:59,  1.92s/it] 55%|█████▍    | 1682/3086 [56:25<43:18,  1.85s/it] 55%|█████▍    | 1683/3086 [56:27<42:22,  1.81s/it] 55%|█████▍    | 1684/3086 [56:29<42:10,  1.81s/it] 55%|█████▍    | 1685/3086 [56:31<42:46,  1.83s/it] 55%|█████▍    | 1686/3086 [56:33<46:01,  1.97s/it] 55%|█████▍    | 1687/3086 [56:35<46:18,  1.99s/it] 55%|█████▍    | 1688/3086 [56:37<48:35,  2.09s/it] 55%|█████▍    | 1689/3086 [56:39<47:55,  2.06s/it] 55%|█████▍    | 1690/3086 [56:41<45:45,  1.97s/it]                                                   {'loss': 0.2894, 'grad_norm': 0.08464262634515762, 'learning_rate': 0.0002714193130265716, 'epoch': 0.55}
 55%|█████▍    | 1690/3086 [56:41<45:45,  1.97s/it] 55%|█████▍    | 1691/3086 [56:43<45:31,  1.96s/it] 55%|█████▍    | 1692/3086 [56:45<44:30,  1.92s/it] 55%|█████▍    | 1693/3086 [56:47<46:32,  2.00s/it] 55%|█████▍    | 1694/3086 [56:49<47:14,  2.04s/it] 55%|█████▍    | 1695/3086 [56:51<47:40,  2.06s/it] 55%|█████▍    | 1696/3086 [56:53<47:08,  2.03s/it] 55%|█████▍    | 1697/3086 [56:56<53:24,  2.31s/it] 55%|█████▌    | 1698/3086 [56:58<48:36,  2.10s/it] 55%|█████▌    | 1699/3086 [57:00<48:08,  2.08s/it] 55%|█████▌    | 1700/3086 [57:02<47:59,  2.08s/it]                                                   {'loss': 0.2898, 'grad_norm': 0.08309422433376312, 'learning_rate': 0.00026947504860661047, 'epoch': 0.55}
 55%|█████▌    | 1700/3086 [57:02<47:59,  2.08s/it] 55%|█████▌    | 1701/3086 [57:04<50:00,  2.17s/it] 55%|█████▌    | 1702/3086 [57:06<46:33,  2.02s/it] 55%|█████▌    | 1703/3086 [57:08<46:39,  2.02s/it] 55%|█████▌    | 1704/3086 [57:10<44:07,  1.92s/it] 55%|█████▌    | 1705/3086 [57:12<47:06,  2.05s/it] 55%|█████▌    | 1706/3086 [57:14<48:53,  2.13s/it] 55%|█████▌    | 1707/3086 [57:16<47:47,  2.08s/it] 55%|█████▌    | 1708/3086 [57:18<45:41,  1.99s/it] 55%|█████▌    | 1709/3086 [57:20<44:57,  1.96s/it] 55%|█████▌    | 1710/3086 [57:22<45:39,  1.99s/it]                                                   {'loss': 0.2825, 'grad_norm': 0.08127040416002274, 'learning_rate': 0.00026753078418664935, 'epoch': 0.55}
 55%|█████▌    | 1710/3086 [57:22<45:39,  1.99s/it] 55%|█████▌    | 1711/3086 [57:25<51:03,  2.23s/it] 55%|█████▌    | 1712/3086 [57:27<53:09,  2.32s/it] 56%|█████▌    | 1713/3086 [57:29<52:06,  2.28s/it] 56%|█████▌    | 1714/3086 [57:31<47:44,  2.09s/it] 56%|█████▌    | 1715/3086 [57:33<49:39,  2.17s/it] 56%|█████▌    | 1716/3086 [57:35<46:52,  2.05s/it] 56%|█████▌    | 1717/3086 [57:37<42:36,  1.87s/it] 56%|█████▌    | 1718/3086 [57:39<47:18,  2.07s/it] 56%|█████▌    | 1719/3086 [57:41<44:55,  1.97s/it] 56%|█████▌    | 1720/3086 [57:43<42:34,  1.87s/it]                                                   {'loss': 0.2793, 'grad_norm': 0.08274237811565399, 'learning_rate': 0.00026558651976668824, 'epoch': 0.56}
 56%|█████▌    | 1720/3086 [57:43<42:34,  1.87s/it] 56%|█████▌    | 1721/3086 [57:44<41:48,  1.84s/it] 56%|█████▌    | 1722/3086 [57:46<43:03,  1.89s/it] 56%|█████▌    | 1723/3086 [57:49<47:23,  2.09s/it] 56%|█████▌    | 1724/3086 [57:51<45:57,  2.02s/it] 56%|█████▌    | 1725/3086 [57:53<47:24,  2.09s/it] 56%|█████▌    | 1726/3086 [57:55<45:06,  1.99s/it] 56%|█████▌    | 1727/3086 [57:57<43:32,  1.92s/it] 56%|█████▌    | 1728/3086 [57:58<41:42,  1.84s/it] 56%|█████▌    | 1729/3086 [58:00<42:02,  1.86s/it] 56%|█████▌    | 1730/3086 [58:02<41:48,  1.85s/it]                                                   {'loss': 0.2848, 'grad_norm': 0.07641083747148514, 'learning_rate': 0.00026364225534672713, 'epoch': 0.56}
 56%|█████▌    | 1730/3086 [58:02<41:48,  1.85s/it] 56%|█████▌    | 1731/3086 [58:04<44:25,  1.97s/it] 56%|█████▌    | 1732/3086 [58:07<46:40,  2.07s/it] 56%|█████▌    | 1733/3086 [58:08<45:51,  2.03s/it] 56%|█████▌    | 1734/3086 [58:10<45:34,  2.02s/it] 56%|█████▌    | 1735/3086 [58:12<44:54,  1.99s/it] 56%|█████▋    | 1736/3086 [58:14<45:33,  2.02s/it] 56%|█████▋    | 1737/3086 [58:17<45:33,  2.03s/it] 56%|█████▋    | 1738/3086 [58:18<43:09,  1.92s/it] 56%|█████▋    | 1739/3086 [58:20<45:19,  2.02s/it] 56%|█████▋    | 1740/3086 [58:23<50:21,  2.25s/it]                                                   {'loss': 0.2746, 'grad_norm': 0.07446587830781937, 'learning_rate': 0.000261697990926766, 'epoch': 0.56}
 56%|█████▋    | 1740/3086 [58:23<50:21,  2.25s/it] 56%|█████▋    | 1741/3086 [58:25<44:17,  1.98s/it] 56%|█████▋    | 1742/3086 [58:27<45:03,  2.01s/it] 56%|█████▋    | 1743/3086 [58:29<46:16,  2.07s/it] 57%|█████▋    | 1744/3086 [58:31<45:07,  2.02s/it] 57%|█████▋    | 1745/3086 [58:33<46:30,  2.08s/it] 57%|█████▋    | 1746/3086 [58:35<44:18,  1.98s/it] 57%|█████▋    | 1747/3086 [58:36<41:15,  1.85s/it] 57%|█████▋    | 1748/3086 [58:39<44:59,  2.02s/it] 57%|█████▋    | 1749/3086 [58:41<45:50,  2.06s/it] 57%|█████▋    | 1750/3086 [58:43<43:31,  1.95s/it]                                                   {'loss': 0.2853, 'grad_norm': 0.08265648782253265, 'learning_rate': 0.0002597537265068049, 'epoch': 0.57}
 57%|█████▋    | 1750/3086 [58:43<43:31,  1.95s/it] 57%|█████▋    | 1751/3086 [58:44<41:16,  1.86s/it] 57%|█████▋    | 1752/3086 [58:46<41:35,  1.87s/it] 57%|█████▋    | 1753/3086 [58:48<40:50,  1.84s/it] 57%|█████▋    | 1754/3086 [58:49<39:25,  1.78s/it] 57%|█████▋    | 1755/3086 [58:51<39:37,  1.79s/it] 57%|█████▋    | 1756/3086 [58:54<43:25,  1.96s/it] 57%|█████▋    | 1757/3086 [58:55<42:18,  1.91s/it] 57%|█████▋    | 1758/3086 [58:58<43:28,  1.96s/it] 57%|█████▋    | 1759/3086 [59:00<44:16,  2.00s/it] 57%|█████▋    | 1760/3086 [59:02<44:01,  1.99s/it]                                                   {'loss': 0.2893, 'grad_norm': 0.0988781750202179, 'learning_rate': 0.0002578094620868438, 'epoch': 0.57}
 57%|█████▋    | 1760/3086 [59:02<44:01,  1.99s/it] 57%|█████▋    | 1761/3086 [59:04<44:31,  2.02s/it] 57%|█████▋    | 1762/3086 [59:06<47:07,  2.14s/it] 57%|█████▋    | 1763/3086 [59:08<47:46,  2.17s/it] 57%|█████▋    | 1764/3086 [59:10<47:15,  2.15s/it] 57%|█████▋    | 1765/3086 [59:12<44:51,  2.04s/it] 57%|█████▋    | 1766/3086 [59:14<45:50,  2.08s/it] 57%|█████▋    | 1767/3086 [59:16<44:51,  2.04s/it] 57%|█████▋    | 1768/3086 [59:18<42:05,  1.92s/it] 57%|█████▋    | 1769/3086 [59:20<43:07,  1.96s/it] 57%|█████▋    | 1770/3086 [59:22<43:26,  1.98s/it]                                                   {'loss': 0.2861, 'grad_norm': 0.08110172301530838, 'learning_rate': 0.0002558651976668827, 'epoch': 0.57}
 57%|█████▋    | 1770/3086 [59:22<43:26,  1.98s/it] 57%|█████▋    | 1771/3086 [59:24<42:26,  1.94s/it] 57%|█████▋    | 1772/3086 [59:26<44:23,  2.03s/it] 57%|█████▋    | 1773/3086 [59:28<42:55,  1.96s/it] 57%|█████▋    | 1774/3086 [59:30<44:55,  2.05s/it] 58%|█████▊    | 1775/3086 [59:32<44:17,  2.03s/it] 58%|█████▊    | 1776/3086 [59:35<46:51,  2.15s/it] 58%|█████▊    | 1777/3086 [59:37<45:20,  2.08s/it] 58%|█████▊    | 1778/3086 [59:39<48:34,  2.23s/it] 58%|█████▊    | 1779/3086 [59:41<48:15,  2.22s/it] 58%|█████▊    | 1780/3086 [59:43<45:04,  2.07s/it]                                                   {'loss': 0.2721, 'grad_norm': 0.07886381447315216, 'learning_rate': 0.00025392093324692156, 'epoch': 0.58}
 58%|█████▊    | 1780/3086 [59:43<45:04,  2.07s/it] 58%|█████▊    | 1781/3086 [59:45<45:20,  2.08s/it] 58%|█████▊    | 1782/3086 [59:47<44:16,  2.04s/it] 58%|█████▊    | 1783/3086 [59:49<43:28,  2.00s/it] 58%|█████▊    | 1784/3086 [59:51<42:49,  1.97s/it] 58%|█████▊    | 1785/3086 [59:53<43:22,  2.00s/it] 58%|█████▊    | 1786/3086 [59:55<45:11,  2.09s/it] 58%|█████▊    | 1787/3086 [59:57<44:51,  2.07s/it] 58%|█████▊    | 1788/3086 [59:59<43:54,  2.03s/it] 58%|█████▊    | 1789/3086 [1:00:01<41:51,  1.94s/it] 58%|█████▊    | 1790/3086 [1:00:03<40:10,  1.86s/it]                                                     {'loss': 0.2748, 'grad_norm': 0.08376607298851013, 'learning_rate': 0.00025197666882696045, 'epoch': 0.58}
 58%|█████▊    | 1790/3086 [1:00:03<40:10,  1.86s/it] 58%|█████▊    | 1791/3086 [1:00:04<39:59,  1.85s/it] 58%|█████▊    | 1792/3086 [1:00:07<42:29,  1.97s/it] 58%|█████▊    | 1793/3086 [1:00:09<43:57,  2.04s/it] 58%|█████▊    | 1794/3086 [1:00:11<42:26,  1.97s/it] 58%|█████▊    | 1795/3086 [1:00:13<44:13,  2.06s/it] 58%|█████▊    | 1796/3086 [1:00:15<41:42,  1.94s/it] 58%|█████▊    | 1797/3086 [1:00:16<40:29,  1.88s/it] 58%|█████▊    | 1798/3086 [1:00:18<41:21,  1.93s/it] 58%|█████▊    | 1799/3086 [1:00:21<46:08,  2.15s/it] 58%|█████▊    | 1800/3086 [1:00:23<44:30,  2.08s/it]                                                     {'loss': 0.2764, 'grad_norm': 0.08100343495607376, 'learning_rate': 0.00025003240440699934, 'epoch': 0.58}
 58%|█████▊    | 1800/3086 [1:00:23<44:30,  2.08s/it] 58%|█████▊    | 1801/3086 [1:00:25<43:13,  2.02s/it] 58%|█████▊    | 1802/3086 [1:00:26<40:23,  1.89s/it] 58%|█████▊    | 1803/3086 [1:00:29<44:38,  2.09s/it] 58%|█████▊    | 1804/3086 [1:00:32<47:17,  2.21s/it] 58%|█████▊    | 1805/3086 [1:00:33<44:52,  2.10s/it] 59%|█████▊    | 1806/3086 [1:00:35<45:05,  2.11s/it] 59%|█████▊    | 1807/3086 [1:00:37<43:05,  2.02s/it] 59%|█████▊    | 1808/3086 [1:00:39<42:15,  1.98s/it] 59%|█████▊    | 1809/3086 [1:00:42<44:24,  2.09s/it] 59%|█████▊    | 1810/3086 [1:00:44<45:31,  2.14s/it]                                                     {'loss': 0.268, 'grad_norm': 0.07362009584903717, 'learning_rate': 0.0002480881399870382, 'epoch': 0.59}
 59%|█████▊    | 1810/3086 [1:00:44<45:31,  2.14s/it] 59%|█████▊    | 1811/3086 [1:00:46<46:22,  2.18s/it] 59%|█████▊    | 1812/3086 [1:00:48<43:46,  2.06s/it] 59%|█████▊    | 1813/3086 [1:00:50<42:56,  2.02s/it] 59%|█████▉    | 1814/3086 [1:00:52<42:30,  2.01s/it] 59%|█████▉    | 1815/3086 [1:00:55<47:51,  2.26s/it] 59%|█████▉    | 1816/3086 [1:00:57<46:48,  2.21s/it] 59%|█████▉    | 1817/3086 [1:00:59<46:04,  2.18s/it] 59%|█████▉    | 1818/3086 [1:01:00<42:33,  2.01s/it] 59%|█████▉    | 1819/3086 [1:01:02<41:20,  1.96s/it] 59%|█████▉    | 1820/3086 [1:01:04<40:29,  1.92s/it]                                                     {'loss': 0.2712, 'grad_norm': 0.07786963880062103, 'learning_rate': 0.0002461438755670771, 'epoch': 0.59}
 59%|█████▉    | 1820/3086 [1:01:04<40:29,  1.92s/it] 59%|█████▉    | 1821/3086 [1:01:06<41:06,  1.95s/it] 59%|█████▉    | 1822/3086 [1:01:08<40:26,  1.92s/it] 59%|█████▉    | 1823/3086 [1:01:10<40:36,  1.93s/it] 59%|█████▉    | 1824/3086 [1:01:12<40:49,  1.94s/it] 59%|█████▉    | 1825/3086 [1:01:13<37:56,  1.81s/it] 59%|█████▉    | 1826/3086 [1:01:16<40:08,  1.91s/it] 59%|█████▉    | 1827/3086 [1:01:18<40:55,  1.95s/it] 59%|█████▉    | 1828/3086 [1:01:20<41:33,  1.98s/it] 59%|█████▉    | 1829/3086 [1:01:22<41:28,  1.98s/it] 59%|█████▉    | 1830/3086 [1:01:24<41:58,  2.01s/it]                                                     {'loss': 0.2706, 'grad_norm': 0.07947390526533127, 'learning_rate': 0.00024419961114711594, 'epoch': 0.59}
 59%|█████▉    | 1830/3086 [1:01:24<41:58,  2.01s/it] 59%|█████▉    | 1831/3086 [1:01:26<45:11,  2.16s/it] 59%|█████▉    | 1832/3086 [1:01:28<44:46,  2.14s/it] 59%|█████▉    | 1833/3086 [1:01:30<42:40,  2.04s/it] 59%|█████▉    | 1834/3086 [1:01:32<42:02,  2.01s/it] 59%|█████▉    | 1835/3086 [1:01:34<40:35,  1.95s/it] 59%|█████▉    | 1836/3086 [1:01:36<41:04,  1.97s/it] 60%|█████▉    | 1837/3086 [1:01:38<43:36,  2.09s/it] 60%|█████▉    | 1838/3086 [1:01:40<41:57,  2.02s/it] 60%|█████▉    | 1839/3086 [1:01:43<44:30,  2.14s/it] 60%|█████▉    | 1840/3086 [1:01:45<43:35,  2.10s/it]                                                     {'loss': 0.2745, 'grad_norm': 0.08083942532539368, 'learning_rate': 0.00024225534672715486, 'epoch': 0.6}
 60%|█████▉    | 1840/3086 [1:01:45<43:35,  2.10s/it] 60%|█████▉    | 1841/3086 [1:01:46<41:23,  1.99s/it] 60%|█████▉    | 1842/3086 [1:01:48<39:30,  1.91s/it] 60%|█████▉    | 1843/3086 [1:01:50<41:08,  1.99s/it] 60%|█████▉    | 1844/3086 [1:01:52<41:22,  2.00s/it] 60%|█████▉    | 1845/3086 [1:01:54<40:47,  1.97s/it] 60%|█████▉    | 1846/3086 [1:01:56<42:14,  2.04s/it] 60%|█████▉    | 1847/3086 [1:01:58<42:54,  2.08s/it] 60%|█████▉    | 1848/3086 [1:02:00<42:17,  2.05s/it] 60%|█████▉    | 1849/3086 [1:02:02<42:07,  2.04s/it] 60%|█████▉    | 1850/3086 [1:02:04<39:25,  1.91s/it]                                                     {'loss': 0.2643, 'grad_norm': 0.0798964872956276, 'learning_rate': 0.00024031108230719375, 'epoch': 0.6}
 60%|█████▉    | 1850/3086 [1:02:04<39:25,  1.91s/it] 60%|█████▉    | 1851/3086 [1:02:07<43:41,  2.12s/it] 60%|██████    | 1852/3086 [1:02:09<43:08,  2.10s/it] 60%|██████    | 1853/3086 [1:02:11<41:19,  2.01s/it] 60%|██████    | 1854/3086 [1:02:12<39:41,  1.93s/it] 60%|██████    | 1855/3086 [1:02:15<42:00,  2.05s/it] 60%|██████    | 1856/3086 [1:02:17<42:05,  2.05s/it] 60%|██████    | 1857/3086 [1:02:18<38:41,  1.89s/it] 60%|██████    | 1858/3086 [1:02:20<39:31,  1.93s/it] 60%|██████    | 1859/3086 [1:02:22<39:46,  1.94s/it] 60%|██████    | 1860/3086 [1:02:24<41:11,  2.02s/it]                                                     {'loss': 0.2758, 'grad_norm': 0.07375578582286835, 'learning_rate': 0.00023836681788723266, 'epoch': 0.6}
 60%|██████    | 1860/3086 [1:02:24<41:11,  2.02s/it] 60%|██████    | 1861/3086 [1:02:27<42:38,  2.09s/it] 60%|██████    | 1862/3086 [1:02:29<43:10,  2.12s/it] 60%|██████    | 1863/3086 [1:02:31<40:45,  2.00s/it] 60%|██████    | 1864/3086 [1:02:32<39:33,  1.94s/it] 60%|██████    | 1865/3086 [1:02:34<38:36,  1.90s/it] 60%|██████    | 1866/3086 [1:02:36<39:27,  1.94s/it] 60%|██████    | 1867/3086 [1:02:38<40:12,  1.98s/it] 61%|██████    | 1868/3086 [1:02:40<38:45,  1.91s/it] 61%|██████    | 1869/3086 [1:02:42<37:50,  1.87s/it] 61%|██████    | 1870/3086 [1:02:44<41:39,  2.06s/it]                                                     {'loss': 0.2687, 'grad_norm': 0.08292610943317413, 'learning_rate': 0.00023642255346727152, 'epoch': 0.61}
 61%|██████    | 1870/3086 [1:02:44<41:39,  2.06s/it] 61%|██████    | 1871/3086 [1:02:46<40:27,  2.00s/it] 61%|██████    | 1872/3086 [1:02:48<38:52,  1.92s/it] 61%|██████    | 1873/3086 [1:02:50<38:02,  1.88s/it] 61%|██████    | 1874/3086 [1:02:51<37:42,  1.87s/it] 61%|██████    | 1875/3086 [1:02:53<37:43,  1.87s/it] 61%|██████    | 1876/3086 [1:02:55<38:47,  1.92s/it] 61%|██████    | 1877/3086 [1:02:58<41:15,  2.05s/it] 61%|██████    | 1878/3086 [1:03:00<41:07,  2.04s/it] 61%|██████    | 1879/3086 [1:03:02<39:56,  1.99s/it] 61%|██████    | 1880/3086 [1:03:04<40:23,  2.01s/it]                                                     {'loss': 0.2695, 'grad_norm': 0.08355865627527237, 'learning_rate': 0.0002344782890473104, 'epoch': 0.61}
 61%|██████    | 1880/3086 [1:03:04<40:23,  2.01s/it] 61%|██████    | 1881/3086 [1:03:06<40:37,  2.02s/it] 61%|██████    | 1882/3086 [1:03:08<40:46,  2.03s/it] 61%|██████    | 1883/3086 [1:03:10<40:51,  2.04s/it] 61%|██████    | 1884/3086 [1:03:11<37:55,  1.89s/it] 61%|██████    | 1885/3086 [1:03:13<36:09,  1.81s/it] 61%|██████    | 1886/3086 [1:03:15<35:37,  1.78s/it] 61%|██████    | 1887/3086 [1:03:17<37:13,  1.86s/it] 61%|██████    | 1888/3086 [1:03:19<41:35,  2.08s/it] 61%|██████    | 1889/3086 [1:03:21<40:32,  2.03s/it] 61%|██████    | 1890/3086 [1:03:23<39:24,  1.98s/it]                                                     {'loss': 0.271, 'grad_norm': 0.08347919583320618, 'learning_rate': 0.0002325340246273493, 'epoch': 0.61}
 61%|██████    | 1890/3086 [1:03:23<39:24,  1.98s/it] 61%|██████▏   | 1891/3086 [1:03:26<42:59,  2.16s/it] 61%|██████▏   | 1892/3086 [1:03:28<43:00,  2.16s/it] 61%|██████▏   | 1893/3086 [1:03:29<39:40,  2.00s/it] 61%|██████▏   | 1894/3086 [1:03:31<38:37,  1.94s/it] 61%|██████▏   | 1895/3086 [1:03:33<38:05,  1.92s/it] 61%|██████▏   | 1896/3086 [1:03:35<39:32,  1.99s/it] 61%|██████▏   | 1897/3086 [1:03:37<37:05,  1.87s/it] 62%|██████▏   | 1898/3086 [1:03:39<38:27,  1.94s/it] 62%|██████▏   | 1899/3086 [1:03:41<39:23,  1.99s/it] 62%|██████▏   | 1900/3086 [1:03:43<39:23,  1.99s/it]                                                     {'loss': 0.265, 'grad_norm': 0.08044395595788956, 'learning_rate': 0.0002305897602073882, 'epoch': 0.62}
 62%|██████▏   | 1900/3086 [1:03:43<39:23,  1.99s/it] 62%|██████▏   | 1901/3086 [1:03:45<39:14,  1.99s/it] 62%|██████▏   | 1902/3086 [1:03:47<36:47,  1.86s/it] 62%|██████▏   | 1903/3086 [1:03:49<37:36,  1.91s/it] 62%|██████▏   | 1904/3086 [1:03:51<39:49,  2.02s/it] 62%|██████▏   | 1905/3086 [1:03:53<40:25,  2.05s/it] 62%|██████▏   | 1906/3086 [1:03:55<39:50,  2.03s/it] 62%|██████▏   | 1907/3086 [1:03:57<39:12,  2.00s/it] 62%|██████▏   | 1908/3086 [1:03:59<38:27,  1.96s/it] 62%|██████▏   | 1909/3086 [1:04:01<38:13,  1.95s/it] 62%|██████▏   | 1910/3086 [1:04:03<37:59,  1.94s/it]                                                     {'loss': 0.2684, 'grad_norm': 0.07665175199508667, 'learning_rate': 0.00022864549578742707, 'epoch': 0.62}
 62%|██████▏   | 1910/3086 [1:04:03<37:59,  1.94s/it] 62%|██████▏   | 1911/3086 [1:04:04<36:04,  1.84s/it] 62%|██████▏   | 1912/3086 [1:04:07<38:59,  1.99s/it] 62%|██████▏   | 1913/3086 [1:04:09<40:16,  2.06s/it] 62%|██████▏   | 1914/3086 [1:04:11<37:47,  1.93s/it] 62%|██████▏   | 1915/3086 [1:04:13<41:48,  2.14s/it] 62%|██████▏   | 1916/3086 [1:04:15<41:08,  2.11s/it] 62%|██████▏   | 1917/3086 [1:04:17<40:41,  2.09s/it] 62%|██████▏   | 1918/3086 [1:04:19<39:27,  2.03s/it] 62%|██████▏   | 1919/3086 [1:04:21<38:54,  2.00s/it] 62%|██████▏   | 1920/3086 [1:04:23<39:36,  2.04s/it]                                                     {'loss': 0.263, 'grad_norm': 0.0931662991642952, 'learning_rate': 0.00022670123136746595, 'epoch': 0.62}
 62%|██████▏   | 1920/3086 [1:04:23<39:36,  2.04s/it] 62%|██████▏   | 1921/3086 [1:04:25<40:19,  2.08s/it] 62%|██████▏   | 1922/3086 [1:04:27<38:48,  2.00s/it] 62%|██████▏   | 1923/3086 [1:04:29<37:30,  1.94s/it] 62%|██████▏   | 1924/3086 [1:04:31<38:14,  1.98s/it] 62%|██████▏   | 1925/3086 [1:04:33<35:51,  1.85s/it] 62%|██████▏   | 1926/3086 [1:04:34<34:58,  1.81s/it] 62%|██████▏   | 1927/3086 [1:04:37<37:59,  1.97s/it] 62%|██████▏   | 1928/3086 [1:04:39<38:41,  2.01s/it] 63%|██████▎   | 1929/3086 [1:04:41<38:24,  1.99s/it] 63%|██████▎   | 1930/3086 [1:04:42<37:04,  1.92s/it]                                                     {'loss': 0.2594, 'grad_norm': 0.08316001296043396, 'learning_rate': 0.00022475696694750484, 'epoch': 0.63}
 63%|██████▎   | 1930/3086 [1:04:42<37:04,  1.92s/it] 63%|██████▎   | 1931/3086 [1:04:44<35:58,  1.87s/it] 63%|██████▎   | 1932/3086 [1:04:46<35:38,  1.85s/it] 63%|██████▎   | 1933/3086 [1:04:48<35:06,  1.83s/it] 63%|██████▎   | 1934/3086 [1:04:50<36:14,  1.89s/it] 63%|██████▎   | 1935/3086 [1:04:52<36:23,  1.90s/it] 63%|██████▎   | 1936/3086 [1:04:53<35:02,  1.83s/it] 63%|██████▎   | 1937/3086 [1:04:55<34:16,  1.79s/it] 63%|██████▎   | 1938/3086 [1:04:57<36:09,  1.89s/it] 63%|██████▎   | 1939/3086 [1:04:59<37:17,  1.95s/it] 63%|██████▎   | 1940/3086 [1:05:01<35:20,  1.85s/it]                                                     {'loss': 0.2643, 'grad_norm': 0.07703862339258194, 'learning_rate': 0.00022281270252754373, 'epoch': 0.63}
 63%|██████▎   | 1940/3086 [1:05:01<35:20,  1.85s/it] 63%|██████▎   | 1941/3086 [1:05:03<37:16,  1.95s/it] 63%|██████▎   | 1942/3086 [1:05:05<34:09,  1.79s/it] 63%|██████▎   | 1943/3086 [1:05:06<34:41,  1.82s/it] 63%|██████▎   | 1944/3086 [1:05:08<35:28,  1.86s/it] 63%|██████▎   | 1945/3086 [1:05:11<39:03,  2.05s/it] 63%|██████▎   | 1946/3086 [1:05:13<38:18,  2.02s/it] 63%|██████▎   | 1947/3086 [1:05:15<36:52,  1.94s/it] 63%|██████▎   | 1948/3086 [1:05:17<38:25,  2.03s/it] 63%|██████▎   | 1949/3086 [1:05:19<38:14,  2.02s/it] 63%|██████▎   | 1950/3086 [1:05:21<37:18,  1.97s/it]                                                     {'loss': 0.2649, 'grad_norm': 0.08076145499944687, 'learning_rate': 0.0002208684381075826, 'epoch': 0.63}
 63%|██████▎   | 1950/3086 [1:05:21<37:18,  1.97s/it] 63%|██████▎   | 1951/3086 [1:05:23<36:56,  1.95s/it] 63%|██████▎   | 1952/3086 [1:05:24<36:25,  1.93s/it] 63%|██████▎   | 1953/3086 [1:05:26<36:22,  1.93s/it] 63%|██████▎   | 1954/3086 [1:05:28<35:32,  1.88s/it] 63%|██████▎   | 1955/3086 [1:05:31<40:34,  2.15s/it] 63%|██████▎   | 1956/3086 [1:05:33<37:27,  1.99s/it] 63%|██████▎   | 1957/3086 [1:05:35<38:41,  2.06s/it] 63%|██████▎   | 1958/3086 [1:05:37<39:25,  2.10s/it] 63%|██████▎   | 1959/3086 [1:05:39<40:19,  2.15s/it] 64%|██████▎   | 1960/3086 [1:05:41<37:16,  1.99s/it]                                                     {'loss': 0.2633, 'grad_norm': 0.08152692764997482, 'learning_rate': 0.0002189241736876215, 'epoch': 0.64}
 64%|██████▎   | 1960/3086 [1:05:41<37:16,  1.99s/it] 64%|██████▎   | 1961/3086 [1:05:43<39:06,  2.09s/it] 64%|██████▎   | 1962/3086 [1:05:45<37:00,  1.98s/it] 64%|██████▎   | 1963/3086 [1:05:47<36:38,  1.96s/it] 64%|██████▎   | 1964/3086 [1:05:49<39:29,  2.11s/it] 64%|██████▎   | 1965/3086 [1:05:52<44:55,  2.40s/it] 64%|██████▎   | 1966/3086 [1:05:54<41:45,  2.24s/it] 64%|██████▎   | 1967/3086 [1:05:56<39:39,  2.13s/it] 64%|██████▍   | 1968/3086 [1:05:58<37:09,  1.99s/it] 64%|██████▍   | 1969/3086 [1:06:00<40:07,  2.16s/it] 64%|██████▍   | 1970/3086 [1:06:02<38:48,  2.09s/it]                                                     {'loss': 0.2611, 'grad_norm': 0.0767548456788063, 'learning_rate': 0.0002169799092676604, 'epoch': 0.64}
 64%|██████▍   | 1970/3086 [1:06:02<38:48,  2.09s/it] 64%|██████▍   | 1971/3086 [1:06:04<36:37,  1.97s/it] 64%|██████▍   | 1972/3086 [1:06:06<39:16,  2.12s/it] 64%|██████▍   | 1973/3086 [1:06:08<37:03,  2.00s/it] 64%|██████▍   | 1974/3086 [1:06:10<38:43,  2.09s/it] 64%|██████▍   | 1975/3086 [1:06:12<38:15,  2.07s/it] 64%|██████▍   | 1976/3086 [1:06:14<38:31,  2.08s/it] 64%|██████▍   | 1977/3086 [1:06:16<36:36,  1.98s/it] 64%|██████▍   | 1978/3086 [1:06:18<37:44,  2.04s/it] 64%|██████▍   | 1979/3086 [1:06:20<36:34,  1.98s/it] 64%|██████▍   | 1980/3086 [1:06:23<38:24,  2.08s/it]                                                     {'loss': 0.2647, 'grad_norm': 0.06782799959182739, 'learning_rate': 0.00021503564484769928, 'epoch': 0.64}
 64%|██████▍   | 1980/3086 [1:06:23<38:24,  2.08s/it] 64%|██████▍   | 1981/3086 [1:06:24<37:15,  2.02s/it] 64%|██████▍   | 1982/3086 [1:06:27<40:09,  2.18s/it] 64%|██████▍   | 1983/3086 [1:06:29<37:36,  2.05s/it] 64%|██████▍   | 1984/3086 [1:06:31<40:42,  2.22s/it] 64%|██████▍   | 1985/3086 [1:06:33<38:10,  2.08s/it] 64%|██████▍   | 1986/3086 [1:06:35<38:00,  2.07s/it] 64%|██████▍   | 1987/3086 [1:06:37<38:26,  2.10s/it] 64%|██████▍   | 1988/3086 [1:06:40<39:46,  2.17s/it] 64%|██████▍   | 1989/3086 [1:06:42<40:02,  2.19s/it] 64%|██████▍   | 1990/3086 [1:06:44<39:23,  2.16s/it]                                                     {'loss': 0.2578, 'grad_norm': 0.07058263570070267, 'learning_rate': 0.00021309138042773814, 'epoch': 0.64}
 64%|██████▍   | 1990/3086 [1:06:44<39:23,  2.16s/it] 65%|██████▍   | 1991/3086 [1:06:46<39:37,  2.17s/it] 65%|██████▍   | 1992/3086 [1:06:48<39:31,  2.17s/it] 65%|██████▍   | 1993/3086 [1:06:50<39:16,  2.16s/it] 65%|██████▍   | 1994/3086 [1:06:53<39:22,  2.16s/it] 65%|██████▍   | 1995/3086 [1:06:54<36:29,  2.01s/it] 65%|██████▍   | 1996/3086 [1:06:56<36:34,  2.01s/it] 65%|██████▍   | 1997/3086 [1:06:58<35:04,  1.93s/it] 65%|██████▍   | 1998/3086 [1:07:00<36:05,  1.99s/it] 65%|██████▍   | 1999/3086 [1:07:02<37:12,  2.05s/it] 65%|██████▍   | 2000/3086 [1:07:05<39:49,  2.20s/it]                                                     {'loss': 0.2572, 'grad_norm': 0.09346554428339005, 'learning_rate': 0.00021114711600777705, 'epoch': 0.65}
 65%|██████▍   | 2000/3086 [1:07:05<39:49,  2.20s/it][INFO|trainer.py:3503] 2024-11-12 11:12:24,622 >> Saving model checkpoint to /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5large-qwe2.5-math7b-metamath/checkpoint-2000
[INFO|configuration_utils.py:472] 2024-11-12 11:12:24,629 >> Configuration saved in /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5large-qwe2.5-math7b-metamath/checkpoint-2000/config.json
[INFO|tokenization_utils_base.py:2684] 2024-11-12 11:12:24,749 >> tokenizer config file saved in /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5large-qwe2.5-math7b-metamath/checkpoint-2000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2024-11-12 11:12:24,752 >> Special tokens file saved in /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5large-qwe2.5-math7b-metamath/checkpoint-2000/special_tokens_map.json
 65%|██████▍   | 2001/3086 [1:07:13<1:13:35,  4.07s/it] 65%|██████▍   | 2002/3086 [1:07:16<1:05:02,  3.60s/it] 65%|██████▍   | 2003/3086 [1:07:18<57:27,  3.18s/it]   65%|██████▍   | 2004/3086 [1:07:20<50:48,  2.82s/it] 65%|██████▍   | 2005/3086 [1:07:22<45:44,  2.54s/it] 65%|██████▌   | 2006/3086 [1:07:24<41:54,  2.33s/it] 65%|██████▌   | 2007/3086 [1:07:26<40:13,  2.24s/it] 65%|██████▌   | 2008/3086 [1:07:27<37:05,  2.06s/it] 65%|██████▌   | 2009/3086 [1:07:29<35:30,  1.98s/it] 65%|██████▌   | 2010/3086 [1:07:31<33:58,  1.89s/it]                                                     {'loss': 0.2571, 'grad_norm': 0.07676488906145096, 'learning_rate': 0.00020920285158781594, 'epoch': 0.65}
 65%|██████▌   | 2010/3086 [1:07:31<33:58,  1.89s/it] 65%|██████▌   | 2011/3086 [1:07:33<33:27,  1.87s/it] 65%|██████▌   | 2012/3086 [1:07:35<32:57,  1.84s/it] 65%|██████▌   | 2013/3086 [1:07:36<32:20,  1.81s/it] 65%|██████▌   | 2014/3086 [1:07:39<35:44,  2.00s/it] 65%|██████▌   | 2015/3086 [1:07:41<34:51,  1.95s/it] 65%|██████▌   | 2016/3086 [1:07:43<35:09,  1.97s/it] 65%|██████▌   | 2017/3086 [1:07:45<37:41,  2.12s/it] 65%|██████▌   | 2018/3086 [1:07:47<36:47,  2.07s/it] 65%|██████▌   | 2019/3086 [1:07:49<34:46,  1.96s/it] 65%|██████▌   | 2020/3086 [1:07:51<35:36,  2.00s/it]                                                     {'loss': 0.2572, 'grad_norm': 0.07733015716075897, 'learning_rate': 0.0002072585871678548, 'epoch': 0.65}
 65%|██████▌   | 2020/3086 [1:07:51<35:36,  2.00s/it] 65%|██████▌   | 2021/3086 [1:07:53<36:30,  2.06s/it] 66%|██████▌   | 2022/3086 [1:07:55<35:51,  2.02s/it] 66%|██████▌   | 2023/3086 [1:07:57<36:47,  2.08s/it] 66%|██████▌   | 2024/3086 [1:07:59<34:48,  1.97s/it] 66%|██████▌   | 2025/3086 [1:08:01<36:15,  2.05s/it] 66%|██████▌   | 2026/3086 [1:08:03<36:38,  2.07s/it] 66%|██████▌   | 2027/3086 [1:08:05<35:05,  1.99s/it] 66%|██████▌   | 2028/3086 [1:08:07<33:19,  1.89s/it] 66%|██████▌   | 2029/3086 [1:08:09<33:52,  1.92s/it] 66%|██████▌   | 2030/3086 [1:08:10<32:10,  1.83s/it]                                                     {'loss': 0.2541, 'grad_norm': 0.07309948652982712, 'learning_rate': 0.00020531432274789368, 'epoch': 0.66}
 66%|██████▌   | 2030/3086 [1:08:10<32:10,  1.83s/it] 66%|██████▌   | 2031/3086 [1:08:12<31:42,  1.80s/it] 66%|██████▌   | 2032/3086 [1:08:14<31:23,  1.79s/it] 66%|██████▌   | 2033/3086 [1:08:16<32:13,  1.84s/it] 66%|██████▌   | 2034/3086 [1:08:17<31:48,  1.81s/it] 66%|██████▌   | 2035/3086 [1:08:20<34:02,  1.94s/it] 66%|██████▌   | 2036/3086 [1:08:22<35:09,  2.01s/it] 66%|██████▌   | 2037/3086 [1:08:24<36:11,  2.07s/it] 66%|██████▌   | 2038/3086 [1:08:26<37:16,  2.13s/it] 66%|██████▌   | 2039/3086 [1:08:28<34:48,  2.00s/it] 66%|██████▌   | 2040/3086 [1:08:30<34:24,  1.97s/it]                                                     {'loss': 0.2535, 'grad_norm': 0.07708477228879929, 'learning_rate': 0.00020337005832793257, 'epoch': 0.66}
 66%|██████▌   | 2040/3086 [1:08:30<34:24,  1.97s/it] 66%|██████▌   | 2041/3086 [1:08:32<34:44,  1.99s/it] 66%|██████▌   | 2042/3086 [1:08:34<33:06,  1.90s/it] 66%|██████▌   | 2043/3086 [1:08:36<33:55,  1.95s/it] 66%|██████▌   | 2044/3086 [1:08:38<35:53,  2.07s/it] 66%|██████▋   | 2045/3086 [1:08:40<33:57,  1.96s/it] 66%|██████▋   | 2046/3086 [1:08:42<34:08,  1.97s/it] 66%|██████▋   | 2047/3086 [1:08:44<35:48,  2.07s/it] 66%|██████▋   | 2048/3086 [1:08:46<37:32,  2.17s/it] 66%|██████▋   | 2049/3086 [1:08:49<39:48,  2.30s/it] 66%|██████▋   | 2050/3086 [1:08:52<40:36,  2.35s/it]                                                     {'loss': 0.2626, 'grad_norm': 0.07101864367723465, 'learning_rate': 0.00020142579390797149, 'epoch': 0.66}
 66%|██████▋   | 2050/3086 [1:08:52<40:36,  2.35s/it] 66%|██████▋   | 2051/3086 [1:08:54<40:08,  2.33s/it] 66%|██████▋   | 2052/3086 [1:08:56<37:09,  2.16s/it] 67%|██████▋   | 2053/3086 [1:08:57<35:27,  2.06s/it] 67%|██████▋   | 2054/3086 [1:08:59<33:43,  1.96s/it] 67%|██████▋   | 2055/3086 [1:09:02<35:48,  2.08s/it] 67%|██████▋   | 2056/3086 [1:09:04<37:18,  2.17s/it] 67%|██████▋   | 2057/3086 [1:09:06<34:33,  2.01s/it] 67%|██████▋   | 2058/3086 [1:09:07<33:38,  1.96s/it] 67%|██████▋   | 2059/3086 [1:09:09<32:53,  1.92s/it] 67%|██████▋   | 2060/3086 [1:09:13<40:12,  2.35s/it]                                                     {'loss': 0.257, 'grad_norm': 0.07674582302570343, 'learning_rate': 0.00019948152948801035, 'epoch': 0.67}
 67%|██████▋   | 2060/3086 [1:09:13<40:12,  2.35s/it] 67%|██████▋   | 2061/3086 [1:09:15<39:04,  2.29s/it] 67%|██████▋   | 2062/3086 [1:09:17<36:49,  2.16s/it] 67%|██████▋   | 2063/3086 [1:09:19<36:06,  2.12s/it] 67%|██████▋   | 2064/3086 [1:09:21<36:11,  2.13s/it] 67%|██████▋   | 2065/3086 [1:09:23<34:41,  2.04s/it] 67%|██████▋   | 2066/3086 [1:09:24<33:42,  1.98s/it] 67%|██████▋   | 2067/3086 [1:09:26<32:50,  1.93s/it] 67%|██████▋   | 2068/3086 [1:09:28<32:56,  1.94s/it] 67%|██████▋   | 2069/3086 [1:09:31<34:48,  2.05s/it] 67%|██████▋   | 2070/3086 [1:09:32<34:09,  2.02s/it]                                                     {'loss': 0.2564, 'grad_norm': 0.08404119312763214, 'learning_rate': 0.00019753726506804923, 'epoch': 0.67}
 67%|██████▋   | 2070/3086 [1:09:32<34:09,  2.02s/it] 67%|██████▋   | 2071/3086 [1:09:34<33:46,  2.00s/it] 67%|██████▋   | 2072/3086 [1:09:36<32:46,  1.94s/it] 67%|██████▋   | 2073/3086 [1:09:38<32:14,  1.91s/it] 67%|██████▋   | 2074/3086 [1:09:40<32:59,  1.96s/it] 67%|██████▋   | 2075/3086 [1:09:42<31:10,  1.85s/it] 67%|██████▋   | 2076/3086 [1:09:43<30:35,  1.82s/it] 67%|██████▋   | 2077/3086 [1:09:46<31:46,  1.89s/it] 67%|██████▋   | 2078/3086 [1:09:47<31:00,  1.85s/it] 67%|██████▋   | 2079/3086 [1:09:49<31:47,  1.89s/it] 67%|██████▋   | 2080/3086 [1:09:51<30:59,  1.85s/it]                                                     {'loss': 0.2579, 'grad_norm': 0.07586421072483063, 'learning_rate': 0.00019559300064808812, 'epoch': 0.67}
 67%|██████▋   | 2080/3086 [1:09:51<30:59,  1.85s/it] 67%|██████▋   | 2081/3086 [1:09:53<30:21,  1.81s/it] 67%|██████▋   | 2082/3086 [1:09:55<31:21,  1.87s/it] 67%|██████▋   | 2083/3086 [1:09:57<32:19,  1.93s/it] 68%|██████▊   | 2084/3086 [1:09:59<32:59,  1.98s/it] 68%|██████▊   | 2085/3086 [1:10:01<33:01,  1.98s/it] 68%|██████▊   | 2086/3086 [1:10:03<35:17,  2.12s/it] 68%|██████▊   | 2087/3086 [1:10:05<34:37,  2.08s/it] 68%|██████▊   | 2088/3086 [1:10:07<34:40,  2.08s/it] 68%|██████▊   | 2089/3086 [1:10:09<32:44,  1.97s/it] 68%|██████▊   | 2090/3086 [1:10:11<31:29,  1.90s/it]                                                     {'loss': 0.2572, 'grad_norm': 0.06933895498514175, 'learning_rate': 0.00019364873622812703, 'epoch': 0.68}
 68%|██████▊   | 2090/3086 [1:10:11<31:29,  1.90s/it] 68%|██████▊   | 2091/3086 [1:10:13<31:42,  1.91s/it] 68%|██████▊   | 2092/3086 [1:10:16<35:43,  2.16s/it] 68%|██████▊   | 2093/3086 [1:10:17<33:26,  2.02s/it] 68%|██████▊   | 2094/3086 [1:10:20<36:46,  2.22s/it] 68%|██████▊   | 2095/3086 [1:10:22<34:42,  2.10s/it] 68%|██████▊   | 2096/3086 [1:10:24<34:18,  2.08s/it] 68%|██████▊   | 2097/3086 [1:10:26<34:57,  2.12s/it] 68%|██████▊   | 2098/3086 [1:10:28<32:30,  1.97s/it] 68%|██████▊   | 2099/3086 [1:10:30<33:04,  2.01s/it] 68%|██████▊   | 2100/3086 [1:10:32<33:16,  2.03s/it]                                                     {'loss': 0.2583, 'grad_norm': 0.07312913984060287, 'learning_rate': 0.0001917044718081659, 'epoch': 0.68}
 68%|██████▊   | 2100/3086 [1:10:32<33:16,  2.03s/it] 68%|██████▊   | 2101/3086 [1:10:33<30:24,  1.85s/it] 68%|██████▊   | 2102/3086 [1:10:35<30:26,  1.86s/it] 68%|██████▊   | 2103/3086 [1:10:37<32:15,  1.97s/it] 68%|██████▊   | 2104/3086 [1:10:40<34:06,  2.08s/it] 68%|██████▊   | 2105/3086 [1:10:42<33:00,  2.02s/it] 68%|██████▊   | 2106/3086 [1:10:44<32:47,  2.01s/it] 68%|██████▊   | 2107/3086 [1:10:46<33:00,  2.02s/it] 68%|██████▊   | 2108/3086 [1:10:48<36:54,  2.26s/it] 68%|██████▊   | 2109/3086 [1:10:50<34:46,  2.14s/it] 68%|██████▊   | 2110/3086 [1:10:52<33:27,  2.06s/it]                                                     {'loss': 0.2546, 'grad_norm': 0.07128205895423889, 'learning_rate': 0.00018976020738820478, 'epoch': 0.68}
 68%|██████▊   | 2110/3086 [1:10:52<33:27,  2.06s/it] 68%|██████▊   | 2111/3086 [1:10:54<34:36,  2.13s/it] 68%|██████▊   | 2112/3086 [1:10:56<31:49,  1.96s/it] 68%|██████▊   | 2113/3086 [1:10:58<29:39,  1.83s/it] 69%|██████▊   | 2114/3086 [1:10:59<29:34,  1.83s/it] 69%|██████▊   | 2115/3086 [1:11:02<33:54,  2.10s/it] 69%|██████▊   | 2116/3086 [1:11:04<35:20,  2.19s/it] 69%|██████▊   | 2117/3086 [1:11:07<34:59,  2.17s/it] 69%|██████▊   | 2118/3086 [1:11:09<33:51,  2.10s/it] 69%|██████▊   | 2119/3086 [1:11:11<36:23,  2.26s/it] 69%|██████▊   | 2120/3086 [1:11:13<35:22,  2.20s/it]                                                     {'loss': 0.2568, 'grad_norm': 0.0774463638663292, 'learning_rate': 0.00018781594296824367, 'epoch': 0.69}
 69%|██████▊   | 2120/3086 [1:11:13<35:22,  2.20s/it] 69%|██████▊   | 2121/3086 [1:11:15<35:16,  2.19s/it] 69%|██████▉   | 2122/3086 [1:11:17<34:36,  2.15s/it] 69%|██████▉   | 2123/3086 [1:11:19<31:24,  1.96s/it] 69%|██████▉   | 2124/3086 [1:11:21<31:22,  1.96s/it] 69%|██████▉   | 2125/3086 [1:11:23<31:07,  1.94s/it] 69%|██████▉   | 2126/3086 [1:11:25<30:13,  1.89s/it] 69%|██████▉   | 2127/3086 [1:11:27<30:25,  1.90s/it] 69%|██████▉   | 2128/3086 [1:11:28<29:39,  1.86s/it] 69%|██████▉   | 2129/3086 [1:11:30<28:43,  1.80s/it] 69%|██████▉   | 2130/3086 [1:11:32<28:18,  1.78s/it]                                                     {'loss': 0.2589, 'grad_norm': 0.07402265071868896, 'learning_rate': 0.00018587167854828255, 'epoch': 0.69}
 69%|██████▉   | 2130/3086 [1:11:32<28:18,  1.78s/it] 69%|██████▉   | 2131/3086 [1:11:34<32:51,  2.06s/it] 69%|██████▉   | 2132/3086 [1:11:37<33:24,  2.10s/it] 69%|██████▉   | 2133/3086 [1:11:38<32:32,  2.05s/it] 69%|██████▉   | 2134/3086 [1:11:41<33:43,  2.13s/it] 69%|██████▉   | 2135/3086 [1:11:42<31:07,  1.96s/it] 69%|██████▉   | 2136/3086 [1:11:44<31:01,  1.96s/it] 69%|██████▉   | 2137/3086 [1:11:46<31:43,  2.01s/it] 69%|██████▉   | 2138/3086 [1:11:48<31:28,  1.99s/it] 69%|██████▉   | 2139/3086 [1:11:50<30:59,  1.96s/it] 69%|██████▉   | 2140/3086 [1:11:53<32:25,  2.06s/it]                                                     {'loss': 0.2523, 'grad_norm': 0.07679087668657303, 'learning_rate': 0.00018392741412832141, 'epoch': 0.69}
 69%|██████▉   | 2140/3086 [1:11:53<32:25,  2.06s/it] 69%|██████▉   | 2141/3086 [1:11:54<31:20,  1.99s/it] 69%|██████▉   | 2142/3086 [1:11:57<34:09,  2.17s/it] 69%|██████▉   | 2143/3086 [1:11:59<31:35,  2.01s/it] 69%|██████▉   | 2144/3086 [1:12:01<33:28,  2.13s/it] 70%|██████▉   | 2145/3086 [1:12:04<35:49,  2.28s/it] 70%|██████▉   | 2146/3086 [1:12:05<33:31,  2.14s/it] 70%|██████▉   | 2147/3086 [1:12:07<30:59,  1.98s/it] 70%|██████▉   | 2148/3086 [1:12:09<30:49,  1.97s/it] 70%|██████▉   | 2149/3086 [1:12:11<32:26,  2.08s/it] 70%|██████▉   | 2150/3086 [1:12:13<32:01,  2.05s/it]                                                     {'loss': 0.2525, 'grad_norm': 0.07531891763210297, 'learning_rate': 0.00018198314970836033, 'epoch': 0.7}
 70%|██████▉   | 2150/3086 [1:12:13<32:01,  2.05s/it] 70%|██████▉   | 2151/3086 [1:12:15<31:19,  2.01s/it] 70%|██████▉   | 2152/3086 [1:12:17<31:08,  2.00s/it] 70%|██████▉   | 2153/3086 [1:12:19<30:10,  1.94s/it] 70%|██████▉   | 2154/3086 [1:12:21<29:33,  1.90s/it] 70%|██████▉   | 2155/3086 [1:12:23<29:22,  1.89s/it] 70%|██████▉   | 2156/3086 [1:12:25<32:17,  2.08s/it] 70%|██████▉   | 2157/3086 [1:12:27<32:39,  2.11s/it] 70%|██████▉   | 2158/3086 [1:12:30<34:26,  2.23s/it] 70%|██████▉   | 2159/3086 [1:12:32<34:07,  2.21s/it] 70%|██████▉   | 2160/3086 [1:12:34<31:39,  2.05s/it]                                                     {'loss': 0.2561, 'grad_norm': 0.07541077584028244, 'learning_rate': 0.00018003888528839922, 'epoch': 0.7}
 70%|██████▉   | 2160/3086 [1:12:34<31:39,  2.05s/it] 70%|███████   | 2161/3086 [1:12:36<32:44,  2.12s/it] 70%|███████   | 2162/3086 [1:12:38<32:42,  2.12s/it] 70%|███████   | 2163/3086 [1:12:41<34:47,  2.26s/it] 70%|███████   | 2164/3086 [1:12:43<34:24,  2.24s/it] 70%|███████   | 2165/3086 [1:12:45<34:08,  2.22s/it] 70%|███████   | 2166/3086 [1:12:47<32:38,  2.13s/it] 70%|███████   | 2167/3086 [1:12:49<33:35,  2.19s/it] 70%|███████   | 2168/3086 [1:12:52<33:13,  2.17s/it] 70%|███████   | 2169/3086 [1:12:53<31:09,  2.04s/it] 70%|███████   | 2170/3086 [1:12:56<34:33,  2.26s/it]                                                     {'loss': 0.2555, 'grad_norm': 0.06987379491329193, 'learning_rate': 0.00017809462086843808, 'epoch': 0.7}
 70%|███████   | 2170/3086 [1:12:56<34:33,  2.26s/it] 70%|███████   | 2171/3086 [1:12:58<31:27,  2.06s/it] 70%|███████   | 2172/3086 [1:13:00<31:53,  2.09s/it] 70%|███████   | 2173/3086 [1:13:02<34:12,  2.25s/it] 70%|███████   | 2174/3086 [1:13:05<33:53,  2.23s/it] 70%|███████   | 2175/3086 [1:13:07<34:23,  2.27s/it] 71%|███████   | 2176/3086 [1:13:09<32:13,  2.13s/it] 71%|███████   | 2177/3086 [1:13:10<30:22,  2.01s/it] 71%|███████   | 2178/3086 [1:13:13<30:56,  2.04s/it] 71%|███████   | 2179/3086 [1:13:15<31:12,  2.06s/it] 71%|███████   | 2180/3086 [1:13:17<30:50,  2.04s/it]                                                     {'loss': 0.2542, 'grad_norm': 0.0749213919043541, 'learning_rate': 0.00017615035644847696, 'epoch': 0.71}
 71%|███████   | 2180/3086 [1:13:17<30:50,  2.04s/it] 71%|███████   | 2181/3086 [1:13:19<31:41,  2.10s/it] 71%|███████   | 2182/3086 [1:13:21<31:15,  2.07s/it] 71%|███████   | 2183/3086 [1:13:24<35:40,  2.37s/it] 71%|███████   | 2184/3086 [1:13:26<32:10,  2.14s/it] 71%|███████   | 2185/3086 [1:13:28<31:41,  2.11s/it] 71%|███████   | 2186/3086 [1:13:29<29:16,  1.95s/it] 71%|███████   | 2187/3086 [1:13:31<29:11,  1.95s/it] 71%|███████   | 2188/3086 [1:13:33<27:57,  1.87s/it] 71%|███████   | 2189/3086 [1:13:35<29:08,  1.95s/it] 71%|███████   | 2190/3086 [1:13:37<27:55,  1.87s/it]                                                     {'loss': 0.2466, 'grad_norm': 0.07264291495084763, 'learning_rate': 0.00017420609202851588, 'epoch': 0.71}
 71%|███████   | 2190/3086 [1:13:37<27:55,  1.87s/it] 71%|███████   | 2191/3086 [1:13:38<26:57,  1.81s/it] 71%|███████   | 2192/3086 [1:13:40<26:11,  1.76s/it] 71%|███████   | 2193/3086 [1:13:42<25:59,  1.75s/it] 71%|███████   | 2194/3086 [1:13:44<26:27,  1.78s/it] 71%|███████   | 2195/3086 [1:13:45<26:01,  1.75s/it] 71%|███████   | 2196/3086 [1:13:47<26:50,  1.81s/it] 71%|███████   | 2197/3086 [1:13:50<30:40,  2.07s/it] 71%|███████   | 2198/3086 [1:13:52<31:31,  2.13s/it] 71%|███████▏  | 2199/3086 [1:13:54<30:55,  2.09s/it] 71%|███████▏  | 2200/3086 [1:13:56<29:55,  2.03s/it]                                                     {'loss': 0.2557, 'grad_norm': 0.0864947959780693, 'learning_rate': 0.00017226182760855476, 'epoch': 0.71}
 71%|███████▏  | 2200/3086 [1:13:56<29:55,  2.03s/it] 71%|███████▏  | 2201/3086 [1:13:58<27:44,  1.88s/it] 71%|███████▏  | 2202/3086 [1:13:59<27:17,  1.85s/it] 71%|███████▏  | 2203/3086 [1:14:02<29:28,  2.00s/it] 71%|███████▏  | 2204/3086 [1:14:04<31:00,  2.11s/it] 71%|███████▏  | 2205/3086 [1:14:07<33:10,  2.26s/it] 71%|███████▏  | 2206/3086 [1:14:09<35:03,  2.39s/it] 72%|███████▏  | 2207/3086 [1:14:12<33:57,  2.32s/it] 72%|███████▏  | 2208/3086 [1:14:14<32:34,  2.23s/it] 72%|███████▏  | 2209/3086 [1:14:16<33:11,  2.27s/it] 72%|███████▏  | 2210/3086 [1:14:18<31:09,  2.13s/it]                                                     {'loss': 0.2556, 'grad_norm': 0.07201400399208069, 'learning_rate': 0.00017031756318859362, 'epoch': 0.72}
 72%|███████▏  | 2210/3086 [1:14:18<31:09,  2.13s/it] 72%|███████▏  | 2211/3086 [1:14:20<31:22,  2.15s/it] 72%|███████▏  | 2212/3086 [1:14:22<28:55,  1.99s/it] 72%|███████▏  | 2213/3086 [1:14:23<28:23,  1.95s/it] 72%|███████▏  | 2214/3086 [1:14:25<28:36,  1.97s/it] 72%|███████▏  | 2215/3086 [1:14:28<30:26,  2.10s/it] 72%|███████▏  | 2216/3086 [1:14:30<30:17,  2.09s/it] 72%|███████▏  | 2217/3086 [1:14:32<31:11,  2.15s/it] 72%|███████▏  | 2218/3086 [1:14:34<30:06,  2.08s/it] 72%|███████▏  | 2219/3086 [1:14:36<29:07,  2.02s/it] 72%|███████▏  | 2220/3086 [1:14:38<28:26,  1.97s/it]                                                     {'loss': 0.2539, 'grad_norm': 0.07131166011095047, 'learning_rate': 0.0001683732987686325, 'epoch': 0.72}
 72%|███████▏  | 2220/3086 [1:14:38<28:26,  1.97s/it] 72%|███████▏  | 2221/3086 [1:14:40<27:20,  1.90s/it] 72%|███████▏  | 2222/3086 [1:14:42<28:33,  1.98s/it] 72%|███████▏  | 2223/3086 [1:14:44<27:50,  1.94s/it] 72%|███████▏  | 2224/3086 [1:14:45<27:03,  1.88s/it] 72%|███████▏  | 2225/3086 [1:14:48<29:17,  2.04s/it] 72%|███████▏  | 2226/3086 [1:14:50<29:05,  2.03s/it] 72%|███████▏  | 2227/3086 [1:14:52<28:39,  2.00s/it] 72%|███████▏  | 2228/3086 [1:14:53<27:22,  1.91s/it] 72%|███████▏  | 2229/3086 [1:14:55<27:31,  1.93s/it] 72%|███████▏  | 2230/3086 [1:14:58<29:02,  2.04s/it]                                                     {'loss': 0.2504, 'grad_norm': 0.08257416635751724, 'learning_rate': 0.0001664290343486714, 'epoch': 0.72}
 72%|███████▏  | 2230/3086 [1:14:58<29:02,  2.04s/it] 72%|███████▏  | 2231/3086 [1:15:00<28:39,  2.01s/it] 72%|███████▏  | 2232/3086 [1:15:02<28:53,  2.03s/it] 72%|███████▏  | 2233/3086 [1:15:04<29:28,  2.07s/it] 72%|███████▏  | 2234/3086 [1:15:06<28:12,  1.99s/it] 72%|███████▏  | 2235/3086 [1:15:08<30:31,  2.15s/it] 72%|███████▏  | 2236/3086 [1:15:10<28:52,  2.04s/it] 72%|███████▏  | 2237/3086 [1:15:12<28:45,  2.03s/it] 73%|███████▎  | 2238/3086 [1:15:14<26:53,  1.90s/it] 73%|███████▎  | 2239/3086 [1:15:16<30:20,  2.15s/it] 73%|███████▎  | 2240/3086 [1:15:18<29:58,  2.13s/it]                                                     {'loss': 0.2529, 'grad_norm': 0.0647849589586258, 'learning_rate': 0.0001644847699287103, 'epoch': 0.73}
 73%|███████▎  | 2240/3086 [1:15:18<29:58,  2.13s/it] 73%|███████▎  | 2241/3086 [1:15:20<29:52,  2.12s/it] 73%|███████▎  | 2242/3086 [1:15:22<27:20,  1.94s/it] 73%|███████▎  | 2243/3086 [1:15:24<29:20,  2.09s/it] 73%|███████▎  | 2244/3086 [1:15:26<27:41,  1.97s/it] 73%|███████▎  | 2245/3086 [1:15:28<28:32,  2.04s/it] 73%|███████▎  | 2246/3086 [1:15:31<29:51,  2.13s/it] 73%|███████▎  | 2247/3086 [1:15:32<27:43,  1.98s/it] 73%|███████▎  | 2248/3086 [1:15:34<28:39,  2.05s/it] 73%|███████▎  | 2249/3086 [1:15:37<29:19,  2.10s/it] 73%|███████▎  | 2250/3086 [1:15:39<28:44,  2.06s/it]                                                     {'loss': 0.2466, 'grad_norm': 0.06895264983177185, 'learning_rate': 0.00016254050550874917, 'epoch': 0.73}
 73%|███████▎  | 2250/3086 [1:15:39<28:44,  2.06s/it] 73%|███████▎  | 2251/3086 [1:15:41<27:45,  1.99s/it] 73%|███████▎  | 2252/3086 [1:15:43<29:24,  2.12s/it] 73%|███████▎  | 2253/3086 [1:15:45<28:03,  2.02s/it] 73%|███████▎  | 2254/3086 [1:15:47<31:06,  2.24s/it] 73%|███████▎  | 2255/3086 [1:15:50<31:07,  2.25s/it] 73%|███████▎  | 2256/3086 [1:15:52<29:21,  2.12s/it] 73%|███████▎  | 2257/3086 [1:15:54<29:19,  2.12s/it] 73%|███████▎  | 2258/3086 [1:15:56<30:15,  2.19s/it] 73%|███████▎  | 2259/3086 [1:15:59<31:44,  2.30s/it] 73%|███████▎  | 2260/3086 [1:16:00<28:38,  2.08s/it]                                                     {'loss': 0.2492, 'grad_norm': 0.07450161129236221, 'learning_rate': 0.00016059624108878806, 'epoch': 0.73}
 73%|███████▎  | 2260/3086 [1:16:00<28:38,  2.08s/it] 73%|███████▎  | 2261/3086 [1:16:02<28:17,  2.06s/it] 73%|███████▎  | 2262/3086 [1:16:05<30:01,  2.19s/it] 73%|███████▎  | 2263/3086 [1:16:07<29:02,  2.12s/it] 73%|███████▎  | 2264/3086 [1:16:09<28:44,  2.10s/it] 73%|███████▎  | 2265/3086 [1:16:12<31:45,  2.32s/it] 73%|███████▎  | 2266/3086 [1:16:13<29:39,  2.17s/it] 73%|███████▎  | 2267/3086 [1:16:15<28:11,  2.07s/it] 73%|███████▎  | 2268/3086 [1:16:17<26:22,  1.93s/it] 74%|███████▎  | 2269/3086 [1:16:19<26:04,  1.92s/it] 74%|███████▎  | 2270/3086 [1:16:20<25:11,  1.85s/it]                                                     {'loss': 0.2555, 'grad_norm': 0.07944843173027039, 'learning_rate': 0.00015865197666882695, 'epoch': 0.74}
 74%|███████▎  | 2270/3086 [1:16:20<25:11,  1.85s/it] 74%|███████▎  | 2271/3086 [1:16:22<25:24,  1.87s/it] 74%|███████▎  | 2272/3086 [1:16:24<26:01,  1.92s/it] 74%|███████▎  | 2273/3086 [1:16:26<25:47,  1.90s/it] 74%|███████▎  | 2274/3086 [1:16:28<24:13,  1.79s/it] 74%|███████▎  | 2275/3086 [1:16:30<25:10,  1.86s/it] 74%|███████▍  | 2276/3086 [1:16:31<24:02,  1.78s/it] 74%|███████▍  | 2277/3086 [1:16:33<22:59,  1.70s/it] 74%|███████▍  | 2278/3086 [1:16:34<22:47,  1.69s/it] 74%|███████▍  | 2279/3086 [1:16:37<24:05,  1.79s/it] 74%|███████▍  | 2280/3086 [1:16:38<24:04,  1.79s/it]                                                     {'loss': 0.2497, 'grad_norm': 0.06882113963365555, 'learning_rate': 0.00015670771224886586, 'epoch': 0.74}
 74%|███████▍  | 2280/3086 [1:16:38<24:04,  1.79s/it] 74%|███████▍  | 2281/3086 [1:16:41<25:45,  1.92s/it] 74%|███████▍  | 2282/3086 [1:16:42<25:38,  1.91s/it] 74%|███████▍  | 2283/3086 [1:16:44<25:14,  1.89s/it] 74%|███████▍  | 2284/3086 [1:16:46<25:56,  1.94s/it] 74%|███████▍  | 2285/3086 [1:16:48<26:00,  1.95s/it] 74%|███████▍  | 2286/3086 [1:16:50<24:49,  1.86s/it] 74%|███████▍  | 2287/3086 [1:16:52<23:59,  1.80s/it] 74%|███████▍  | 2288/3086 [1:16:54<25:13,  1.90s/it] 74%|███████▍  | 2289/3086 [1:16:55<24:21,  1.83s/it] 74%|███████▍  | 2290/3086 [1:16:57<24:32,  1.85s/it]                                                     {'loss': 0.248, 'grad_norm': 0.07662844657897949, 'learning_rate': 0.00015476344782890472, 'epoch': 0.74}
 74%|███████▍  | 2290/3086 [1:16:57<24:32,  1.85s/it] 74%|███████▍  | 2291/3086 [1:16:59<24:02,  1.81s/it] 74%|███████▍  | 2292/3086 [1:17:01<24:57,  1.89s/it] 74%|███████▍  | 2293/3086 [1:17:03<24:30,  1.85s/it] 74%|███████▍  | 2294/3086 [1:17:05<25:37,  1.94s/it] 74%|███████▍  | 2295/3086 [1:17:06<23:46,  1.80s/it] 74%|███████▍  | 2296/3086 [1:17:08<24:26,  1.86s/it] 74%|███████▍  | 2297/3086 [1:17:11<26:04,  1.98s/it] 74%|███████▍  | 2298/3086 [1:17:13<26:51,  2.04s/it] 74%|███████▍  | 2299/3086 [1:17:15<27:37,  2.11s/it] 75%|███████▍  | 2300/3086 [1:17:17<26:20,  2.01s/it]                                                     {'loss': 0.253, 'grad_norm': 0.08181441575288773, 'learning_rate': 0.0001528191834089436, 'epoch': 0.75}
 75%|███████▍  | 2300/3086 [1:17:17<26:20,  2.01s/it] 75%|███████▍  | 2301/3086 [1:17:19<24:59,  1.91s/it] 75%|███████▍  | 2302/3086 [1:17:20<24:16,  1.86s/it] 75%|███████▍  | 2303/3086 [1:17:22<25:12,  1.93s/it] 75%|███████▍  | 2304/3086 [1:17:24<25:08,  1.93s/it] 75%|███████▍  | 2305/3086 [1:17:26<24:24,  1.87s/it] 75%|███████▍  | 2306/3086 [1:17:28<25:38,  1.97s/it] 75%|███████▍  | 2307/3086 [1:17:30<25:07,  1.94s/it] 75%|███████▍  | 2308/3086 [1:17:32<24:08,  1.86s/it] 75%|███████▍  | 2309/3086 [1:17:34<24:42,  1.91s/it] 75%|███████▍  | 2310/3086 [1:17:36<24:07,  1.87s/it]                                                     {'loss': 0.2477, 'grad_norm': 0.07629606872797012, 'learning_rate': 0.0001508749189889825, 'epoch': 0.75}
 75%|███████▍  | 2310/3086 [1:17:36<24:07,  1.87s/it] 75%|███████▍  | 2311/3086 [1:17:37<23:38,  1.83s/it] 75%|███████▍  | 2312/3086 [1:17:39<23:05,  1.79s/it] 75%|███████▍  | 2313/3086 [1:17:41<24:07,  1.87s/it] 75%|███████▍  | 2314/3086 [1:17:43<24:23,  1.90s/it] 75%|███████▌  | 2315/3086 [1:17:45<24:00,  1.87s/it] 75%|███████▌  | 2316/3086 [1:17:47<23:53,  1.86s/it] 75%|███████▌  | 2317/3086 [1:17:48<23:05,  1.80s/it] 75%|███████▌  | 2318/3086 [1:17:50<23:18,  1.82s/it] 75%|███████▌  | 2319/3086 [1:17:53<25:23,  1.99s/it] 75%|███████▌  | 2320/3086 [1:17:54<24:38,  1.93s/it]                                                     {'loss': 0.2512, 'grad_norm': 0.07099491357803345, 'learning_rate': 0.00014893065456902138, 'epoch': 0.75}
 75%|███████▌  | 2320/3086 [1:17:54<24:38,  1.93s/it] 75%|███████▌  | 2321/3086 [1:17:57<24:55,  1.95s/it] 75%|███████▌  | 2322/3086 [1:17:59<25:07,  1.97s/it] 75%|███████▌  | 2323/3086 [1:18:00<23:35,  1.86s/it] 75%|███████▌  | 2324/3086 [1:18:02<23:20,  1.84s/it] 75%|███████▌  | 2325/3086 [1:18:04<25:55,  2.04s/it] 75%|███████▌  | 2326/3086 [1:18:06<25:43,  2.03s/it] 75%|███████▌  | 2327/3086 [1:18:09<26:47,  2.12s/it] 75%|███████▌  | 2328/3086 [1:18:11<26:35,  2.10s/it] 75%|███████▌  | 2329/3086 [1:18:13<26:37,  2.11s/it] 76%|███████▌  | 2330/3086 [1:18:15<25:33,  2.03s/it]                                                     {'loss': 0.2498, 'grad_norm': 0.08014911413192749, 'learning_rate': 0.00014698639014906024, 'epoch': 0.76}
 76%|███████▌  | 2330/3086 [1:18:15<25:33,  2.03s/it] 76%|███████▌  | 2331/3086 [1:18:17<26:48,  2.13s/it] 76%|███████▌  | 2332/3086 [1:18:19<26:26,  2.10s/it] 76%|███████▌  | 2333/3086 [1:18:21<26:17,  2.09s/it] 76%|███████▌  | 2334/3086 [1:18:24<26:54,  2.15s/it] 76%|███████▌  | 2335/3086 [1:18:25<26:00,  2.08s/it] 76%|███████▌  | 2336/3086 [1:18:27<24:09,  1.93s/it] 76%|███████▌  | 2337/3086 [1:18:29<25:15,  2.02s/it] 76%|███████▌  | 2338/3086 [1:18:31<24:29,  1.96s/it] 76%|███████▌  | 2339/3086 [1:18:33<24:28,  1.97s/it] 76%|███████▌  | 2340/3086 [1:18:35<24:21,  1.96s/it]                                                     {'loss': 0.2478, 'grad_norm': 0.07161164283752441, 'learning_rate': 0.00014504212572909915, 'epoch': 0.76}
 76%|███████▌  | 2340/3086 [1:18:35<24:21,  1.96s/it] 76%|███████▌  | 2341/3086 [1:18:37<24:04,  1.94s/it] 76%|███████▌  | 2342/3086 [1:18:39<25:11,  2.03s/it] 76%|███████▌  | 2343/3086 [1:18:41<24:45,  2.00s/it] 76%|███████▌  | 2344/3086 [1:18:43<25:24,  2.06s/it] 76%|███████▌  | 2345/3086 [1:18:45<25:02,  2.03s/it] 76%|███████▌  | 2346/3086 [1:18:47<24:04,  1.95s/it] 76%|███████▌  | 2347/3086 [1:18:49<25:42,  2.09s/it] 76%|███████▌  | 2348/3086 [1:18:52<26:22,  2.14s/it] 76%|███████▌  | 2349/3086 [1:18:54<26:55,  2.19s/it] 76%|███████▌  | 2350/3086 [1:18:56<26:09,  2.13s/it]                                                     {'loss': 0.2433, 'grad_norm': 0.0721312016248703, 'learning_rate': 0.00014309786130913801, 'epoch': 0.76}
 76%|███████▌  | 2350/3086 [1:18:56<26:09,  2.13s/it] 76%|███████▌  | 2351/3086 [1:18:58<24:38,  2.01s/it] 76%|███████▌  | 2352/3086 [1:19:00<23:54,  1.95s/it] 76%|███████▌  | 2353/3086 [1:19:01<23:50,  1.95s/it] 76%|███████▋  | 2354/3086 [1:19:03<22:58,  1.88s/it] 76%|███████▋  | 2355/3086 [1:19:05<23:43,  1.95s/it] 76%|███████▋  | 2356/3086 [1:19:08<24:59,  2.05s/it] 76%|███████▋  | 2357/3086 [1:19:09<23:25,  1.93s/it] 76%|███████▋  | 2358/3086 [1:19:11<23:16,  1.92s/it] 76%|███████▋  | 2359/3086 [1:19:13<23:41,  1.96s/it] 76%|███████▋  | 2360/3086 [1:19:16<27:36,  2.28s/it]                                                     {'loss': 0.2456, 'grad_norm': 0.07684633135795593, 'learning_rate': 0.00014115359688917693, 'epoch': 0.76}
 76%|███████▋  | 2360/3086 [1:19:16<27:36,  2.28s/it] 77%|███████▋  | 2361/3086 [1:19:18<25:39,  2.12s/it] 77%|███████▋  | 2362/3086 [1:19:20<25:28,  2.11s/it] 77%|███████▋  | 2363/3086 [1:19:22<25:28,  2.11s/it] 77%|███████▋  | 2364/3086 [1:19:24<24:35,  2.04s/it] 77%|███████▋  | 2365/3086 [1:19:26<24:00,  2.00s/it] 77%|███████▋  | 2366/3086 [1:19:28<24:29,  2.04s/it] 77%|███████▋  | 2367/3086 [1:19:30<24:30,  2.04s/it] 77%|███████▋  | 2368/3086 [1:19:32<24:55,  2.08s/it] 77%|███████▋  | 2369/3086 [1:19:34<23:54,  2.00s/it] 77%|███████▋  | 2370/3086 [1:19:36<23:38,  1.98s/it]                                                     {'loss': 0.24, 'grad_norm': 0.07214751094579697, 'learning_rate': 0.0001392093324692158, 'epoch': 0.77}
 77%|███████▋  | 2370/3086 [1:19:36<23:38,  1.98s/it] 77%|███████▋  | 2371/3086 [1:19:39<26:52,  2.25s/it] 77%|███████▋  | 2372/3086 [1:19:41<25:29,  2.14s/it] 77%|███████▋  | 2373/3086 [1:19:43<25:02,  2.11s/it] 77%|███████▋  | 2374/3086 [1:19:45<25:45,  2.17s/it] 77%|███████▋  | 2375/3086 [1:19:47<25:10,  2.12s/it] 77%|███████▋  | 2376/3086 [1:19:49<25:30,  2.16s/it] 77%|███████▋  | 2377/3086 [1:19:52<25:20,  2.15s/it] 77%|███████▋  | 2378/3086 [1:19:54<24:57,  2.11s/it] 77%|███████▋  | 2379/3086 [1:19:56<24:55,  2.12s/it] 77%|███████▋  | 2380/3086 [1:19:58<24:20,  2.07s/it]                                                     {'loss': 0.2448, 'grad_norm': 0.07016647607088089, 'learning_rate': 0.0001372650680492547, 'epoch': 0.77}
 77%|███████▋  | 2380/3086 [1:19:58<24:20,  2.07s/it] 77%|███████▋  | 2381/3086 [1:19:59<23:24,  1.99s/it] 77%|███████▋  | 2382/3086 [1:20:02<25:53,  2.21s/it] 77%|███████▋  | 2383/3086 [1:20:04<24:34,  2.10s/it] 77%|███████▋  | 2384/3086 [1:20:06<23:01,  1.97s/it] 77%|███████▋  | 2385/3086 [1:20:08<22:39,  1.94s/it] 77%|███████▋  | 2386/3086 [1:20:10<22:40,  1.94s/it] 77%|███████▋  | 2387/3086 [1:20:11<22:19,  1.92s/it] 77%|███████▋  | 2388/3086 [1:20:13<21:45,  1.87s/it] 77%|███████▋  | 2389/3086 [1:20:15<21:32,  1.85s/it] 77%|███████▋  | 2390/3086 [1:20:16<20:23,  1.76s/it]                                                     {'loss': 0.2407, 'grad_norm': 0.0716203972697258, 'learning_rate': 0.00013532080362929356, 'epoch': 0.77}
 77%|███████▋  | 2390/3086 [1:20:16<20:23,  1.76s/it] 77%|███████▋  | 2391/3086 [1:20:19<21:50,  1.89s/it] 78%|███████▊  | 2392/3086 [1:20:21<23:28,  2.03s/it] 78%|███████▊  | 2393/3086 [1:20:23<22:23,  1.94s/it] 78%|███████▊  | 2394/3086 [1:20:25<22:06,  1.92s/it] 78%|███████▊  | 2395/3086 [1:20:27<22:05,  1.92s/it] 78%|███████▊  | 2396/3086 [1:20:29<23:38,  2.06s/it] 78%|███████▊  | 2397/3086 [1:20:31<22:43,  1.98s/it] 78%|███████▊  | 2398/3086 [1:20:33<23:51,  2.08s/it] 78%|███████▊  | 2399/3086 [1:20:35<23:48,  2.08s/it] 78%|███████▊  | 2400/3086 [1:20:37<22:31,  1.97s/it]                                                     {'loss': 0.2497, 'grad_norm': 0.09258244186639786, 'learning_rate': 0.00013337653920933245, 'epoch': 0.78}
 78%|███████▊  | 2400/3086 [1:20:37<22:31,  1.97s/it] 78%|███████▊  | 2401/3086 [1:20:38<21:20,  1.87s/it] 78%|███████▊  | 2402/3086 [1:20:40<21:23,  1.88s/it] 78%|███████▊  | 2403/3086 [1:20:42<21:42,  1.91s/it] 78%|███████▊  | 2404/3086 [1:20:44<21:32,  1.90s/it] 78%|███████▊  | 2405/3086 [1:20:46<20:52,  1.84s/it] 78%|███████▊  | 2406/3086 [1:20:47<19:58,  1.76s/it] 78%|███████▊  | 2407/3086 [1:20:49<20:05,  1.77s/it] 78%|███████▊  | 2408/3086 [1:20:51<19:59,  1.77s/it] 78%|███████▊  | 2409/3086 [1:20:55<26:04,  2.31s/it] 78%|███████▊  | 2410/3086 [1:20:56<23:48,  2.11s/it]                                                     {'loss': 0.2396, 'grad_norm': 0.07507946342229843, 'learning_rate': 0.00013143227478937134, 'epoch': 0.78}
 78%|███████▊  | 2410/3086 [1:20:56<23:48,  2.11s/it] 78%|███████▊  | 2411/3086 [1:20:59<24:55,  2.22s/it] 78%|███████▊  | 2412/3086 [1:21:01<26:32,  2.36s/it] 78%|███████▊  | 2413/3086 [1:21:04<26:07,  2.33s/it] 78%|███████▊  | 2414/3086 [1:21:06<24:36,  2.20s/it] 78%|███████▊  | 2415/3086 [1:21:07<22:55,  2.05s/it] 78%|███████▊  | 2416/3086 [1:21:09<22:08,  1.98s/it] 78%|███████▊  | 2417/3086 [1:21:11<21:01,  1.89s/it] 78%|███████▊  | 2418/3086 [1:21:12<19:46,  1.78s/it] 78%|███████▊  | 2419/3086 [1:21:14<20:26,  1.84s/it] 78%|███████▊  | 2420/3086 [1:21:16<20:16,  1.83s/it]                                                     {'loss': 0.2478, 'grad_norm': 0.06661518663167953, 'learning_rate': 0.00012948801036941022, 'epoch': 0.78}
 78%|███████▊  | 2420/3086 [1:21:16<20:16,  1.83s/it] 78%|███████▊  | 2421/3086 [1:21:18<20:18,  1.83s/it] 78%|███████▊  | 2422/3086 [1:21:20<20:10,  1.82s/it] 79%|███████▊  | 2423/3086 [1:21:22<20:14,  1.83s/it] 79%|███████▊  | 2424/3086 [1:21:23<20:27,  1.85s/it] 79%|███████▊  | 2425/3086 [1:21:26<21:33,  1.96s/it] 79%|███████▊  | 2426/3086 [1:21:28<22:03,  2.01s/it] 79%|███████▊  | 2427/3086 [1:21:30<21:07,  1.92s/it] 79%|███████▊  | 2428/3086 [1:21:31<20:42,  1.89s/it] 79%|███████▊  | 2429/3086 [1:21:33<20:45,  1.90s/it] 79%|███████▊  | 2430/3086 [1:21:35<20:11,  1.85s/it]                                                     {'loss': 0.2452, 'grad_norm': 0.07110261917114258, 'learning_rate': 0.0001275437459494491, 'epoch': 0.79}
 79%|███████▊  | 2430/3086 [1:21:35<20:11,  1.85s/it] 79%|███████▉  | 2431/3086 [1:21:37<19:49,  1.82s/it] 79%|███████▉  | 2432/3086 [1:21:39<22:46,  2.09s/it] 79%|███████▉  | 2433/3086 [1:21:42<23:40,  2.18s/it] 79%|███████▉  | 2434/3086 [1:21:44<23:10,  2.13s/it] 79%|███████▉  | 2435/3086 [1:21:46<24:37,  2.27s/it] 79%|███████▉  | 2436/3086 [1:21:48<23:10,  2.14s/it] 79%|███████▉  | 2437/3086 [1:21:50<22:00,  2.04s/it] 79%|███████▉  | 2438/3086 [1:21:52<21:12,  1.96s/it] 79%|███████▉  | 2439/3086 [1:21:54<20:33,  1.91s/it] 79%|███████▉  | 2440/3086 [1:21:56<20:36,  1.91s/it]                                                     {'loss': 0.2456, 'grad_norm': 0.06824333220720291, 'learning_rate': 0.000125599481529488, 'epoch': 0.79}
 79%|███████▉  | 2440/3086 [1:21:56<20:36,  1.91s/it] 79%|███████▉  | 2441/3086 [1:21:58<23:07,  2.15s/it] 79%|███████▉  | 2442/3086 [1:22:00<21:02,  1.96s/it] 79%|███████▉  | 2443/3086 [1:22:02<20:49,  1.94s/it] 79%|███████▉  | 2444/3086 [1:22:04<21:18,  1.99s/it] 79%|███████▉  | 2445/3086 [1:22:06<21:33,  2.02s/it] 79%|███████▉  | 2446/3086 [1:22:08<23:25,  2.20s/it] 79%|███████▉  | 2447/3086 [1:22:11<24:11,  2.27s/it] 79%|███████▉  | 2448/3086 [1:22:13<23:16,  2.19s/it] 79%|███████▉  | 2449/3086 [1:22:15<21:19,  2.01s/it] 79%|███████▉  | 2450/3086 [1:22:16<20:27,  1.93s/it]                                                     {'loss': 0.2492, 'grad_norm': 0.08580043911933899, 'learning_rate': 0.00012365521710952688, 'epoch': 0.79}
 79%|███████▉  | 2450/3086 [1:22:16<20:27,  1.93s/it] 79%|███████▉  | 2451/3086 [1:22:18<19:17,  1.82s/it] 79%|███████▉  | 2452/3086 [1:22:20<20:59,  1.99s/it] 79%|███████▉  | 2453/3086 [1:22:22<20:58,  1.99s/it] 80%|███████▉  | 2454/3086 [1:22:25<23:17,  2.21s/it] 80%|███████▉  | 2455/3086 [1:22:27<23:00,  2.19s/it] 80%|███████▉  | 2456/3086 [1:22:30<24:25,  2.33s/it] 80%|███████▉  | 2457/3086 [1:22:32<23:58,  2.29s/it] 80%|███████▉  | 2458/3086 [1:22:34<21:58,  2.10s/it] 80%|███████▉  | 2459/3086 [1:22:36<22:39,  2.17s/it] 80%|███████▉  | 2460/3086 [1:22:38<21:49,  2.09s/it]                                                     {'loss': 0.2445, 'grad_norm': 0.07008690387010574, 'learning_rate': 0.00012171095268956577, 'epoch': 0.8}
 80%|███████▉  | 2460/3086 [1:22:38<21:49,  2.09s/it] 80%|███████▉  | 2461/3086 [1:22:40<23:10,  2.22s/it] 80%|███████▉  | 2462/3086 [1:22:43<23:41,  2.28s/it] 80%|███████▉  | 2463/3086 [1:22:44<21:52,  2.11s/it] 80%|███████▉  | 2464/3086 [1:22:46<21:00,  2.03s/it] 80%|███████▉  | 2465/3086 [1:22:48<20:01,  1.93s/it] 80%|███████▉  | 2466/3086 [1:22:50<19:12,  1.86s/it] 80%|███████▉  | 2467/3086 [1:22:52<19:11,  1.86s/it] 80%|███████▉  | 2468/3086 [1:22:54<20:07,  1.95s/it] 80%|████████  | 2469/3086 [1:22:56<21:07,  2.05s/it] 80%|████████  | 2470/3086 [1:22:58<21:03,  2.05s/it]                                                     {'loss': 0.2512, 'grad_norm': 0.0712592601776123, 'learning_rate': 0.00011976668826960466, 'epoch': 0.8}
 80%|████████  | 2470/3086 [1:22:58<21:03,  2.05s/it] 80%|████████  | 2471/3086 [1:23:00<21:26,  2.09s/it] 80%|████████  | 2472/3086 [1:23:02<21:34,  2.11s/it] 80%|████████  | 2473/3086 [1:23:05<21:43,  2.13s/it] 80%|████████  | 2474/3086 [1:23:07<21:32,  2.11s/it] 80%|████████  | 2475/3086 [1:23:08<20:03,  1.97s/it] 80%|████████  | 2476/3086 [1:23:10<19:29,  1.92s/it] 80%|████████  | 2477/3086 [1:23:12<20:07,  1.98s/it] 80%|████████  | 2478/3086 [1:23:14<19:17,  1.90s/it] 80%|████████  | 2479/3086 [1:23:16<18:58,  1.88s/it] 80%|████████  | 2480/3086 [1:23:18<19:21,  1.92s/it]                                                     {'loss': 0.2396, 'grad_norm': 0.07190513610839844, 'learning_rate': 0.00011782242384964355, 'epoch': 0.8}
 80%|████████  | 2480/3086 [1:23:18<19:21,  1.92s/it] 80%|████████  | 2481/3086 [1:23:20<20:47,  2.06s/it] 80%|████████  | 2482/3086 [1:23:22<20:49,  2.07s/it] 80%|████████  | 2483/3086 [1:23:24<20:32,  2.04s/it] 80%|████████  | 2484/3086 [1:23:26<20:25,  2.04s/it] 81%|████████  | 2485/3086 [1:23:29<21:06,  2.11s/it] 81%|████████  | 2486/3086 [1:23:31<21:09,  2.12s/it] 81%|████████  | 2487/3086 [1:23:33<21:03,  2.11s/it] 81%|████████  | 2488/3086 [1:23:35<21:02,  2.11s/it] 81%|████████  | 2489/3086 [1:23:37<22:06,  2.22s/it] 81%|████████  | 2490/3086 [1:23:39<20:54,  2.10s/it]                                                     {'loss': 0.2428, 'grad_norm': 0.062137700617313385, 'learning_rate': 0.00011587815942968243, 'epoch': 0.81}
 81%|████████  | 2490/3086 [1:23:39<20:54,  2.10s/it] 81%|████████  | 2491/3086 [1:23:41<21:20,  2.15s/it] 81%|████████  | 2492/3086 [1:23:43<20:32,  2.07s/it] 81%|████████  | 2493/3086 [1:23:45<19:43,  1.99s/it] 81%|████████  | 2494/3086 [1:23:47<20:26,  2.07s/it] 81%|████████  | 2495/3086 [1:23:49<20:27,  2.08s/it] 81%|████████  | 2496/3086 [1:23:52<20:32,  2.09s/it] 81%|████████  | 2497/3086 [1:23:54<20:26,  2.08s/it] 81%|████████  | 2498/3086 [1:23:56<19:52,  2.03s/it] 81%|████████  | 2499/3086 [1:23:57<18:43,  1.91s/it] 81%|████████  | 2500/3086 [1:23:59<18:08,  1.86s/it]                                                     {'loss': 0.2408, 'grad_norm': 0.07237725704908371, 'learning_rate': 0.0001139338950097213, 'epoch': 0.81}
 81%|████████  | 2500/3086 [1:23:59<18:08,  1.86s/it] 81%|████████  | 2501/3086 [1:24:01<19:17,  1.98s/it] 81%|████████  | 2502/3086 [1:24:03<19:15,  1.98s/it] 81%|████████  | 2503/3086 [1:24:05<19:18,  1.99s/it] 81%|████████  | 2504/3086 [1:24:07<18:40,  1.92s/it] 81%|████████  | 2505/3086 [1:24:09<18:38,  1.92s/it] 81%|████████  | 2506/3086 [1:24:11<18:58,  1.96s/it] 81%|████████  | 2507/3086 [1:24:13<19:58,  2.07s/it] 81%|████████▏ | 2508/3086 [1:24:16<21:15,  2.21s/it] 81%|████████▏ | 2509/3086 [1:24:18<20:18,  2.11s/it] 81%|████████▏ | 2510/3086 [1:24:20<20:28,  2.13s/it]                                                     {'loss': 0.2425, 'grad_norm': 0.07597585767507553, 'learning_rate': 0.00011198963058976019, 'epoch': 0.81}
 81%|████████▏ | 2510/3086 [1:24:20<20:28,  2.13s/it] 81%|████████▏ | 2511/3086 [1:24:22<21:22,  2.23s/it] 81%|████████▏ | 2512/3086 [1:24:24<20:33,  2.15s/it] 81%|████████▏ | 2513/3086 [1:24:26<19:01,  1.99s/it] 81%|████████▏ | 2514/3086 [1:24:28<17:56,  1.88s/it] 81%|████████▏ | 2515/3086 [1:24:29<17:21,  1.82s/it] 82%|████████▏ | 2516/3086 [1:24:32<18:56,  1.99s/it] 82%|████████▏ | 2517/3086 [1:24:34<19:32,  2.06s/it] 82%|████████▏ | 2518/3086 [1:24:36<18:42,  1.98s/it] 82%|████████▏ | 2519/3086 [1:24:37<17:31,  1.85s/it] 82%|████████▏ | 2520/3086 [1:24:39<16:50,  1.79s/it]                                                     {'loss': 0.2407, 'grad_norm': 0.0800345242023468, 'learning_rate': 0.00011004536616979908, 'epoch': 0.82}
 82%|████████▏ | 2520/3086 [1:24:39<16:50,  1.79s/it] 82%|████████▏ | 2521/3086 [1:24:41<16:39,  1.77s/it] 82%|████████▏ | 2522/3086 [1:24:43<17:33,  1.87s/it] 82%|████████▏ | 2523/3086 [1:24:45<18:07,  1.93s/it] 82%|████████▏ | 2524/3086 [1:24:47<18:04,  1.93s/it] 82%|████████▏ | 2525/3086 [1:24:49<18:30,  1.98s/it] 82%|████████▏ | 2526/3086 [1:24:51<18:02,  1.93s/it] 82%|████████▏ | 2527/3086 [1:24:53<19:12,  2.06s/it] 82%|████████▏ | 2528/3086 [1:24:56<20:53,  2.25s/it] 82%|████████▏ | 2529/3086 [1:24:58<21:03,  2.27s/it] 82%|████████▏ | 2530/3086 [1:25:00<20:48,  2.25s/it]                                                     {'loss': 0.2386, 'grad_norm': 0.07348711043596268, 'learning_rate': 0.00010810110174983797, 'epoch': 0.82}
 82%|████████▏ | 2530/3086 [1:25:00<20:48,  2.25s/it] 82%|████████▏ | 2531/3086 [1:25:02<19:47,  2.14s/it] 82%|████████▏ | 2532/3086 [1:25:04<18:31,  2.01s/it] 82%|████████▏ | 2533/3086 [1:25:05<17:37,  1.91s/it] 82%|████████▏ | 2534/3086 [1:25:07<17:56,  1.95s/it] 82%|████████▏ | 2535/3086 [1:25:10<19:12,  2.09s/it] 82%|████████▏ | 2536/3086 [1:25:12<19:23,  2.12s/it] 82%|████████▏ | 2537/3086 [1:25:14<19:26,  2.13s/it] 82%|████████▏ | 2538/3086 [1:25:16<18:52,  2.07s/it] 82%|████████▏ | 2539/3086 [1:25:18<17:48,  1.95s/it] 82%|████████▏ | 2540/3086 [1:25:20<19:01,  2.09s/it]                                                     {'loss': 0.2456, 'grad_norm': 0.07129879295825958, 'learning_rate': 0.00010615683732987685, 'epoch': 0.82}
 82%|████████▏ | 2540/3086 [1:25:20<19:01,  2.09s/it] 82%|████████▏ | 2541/3086 [1:25:22<17:34,  1.93s/it] 82%|████████▏ | 2542/3086 [1:25:24<18:11,  2.01s/it] 82%|████████▏ | 2543/3086 [1:25:26<17:10,  1.90s/it] 82%|████████▏ | 2544/3086 [1:25:28<18:03,  2.00s/it] 82%|████████▏ | 2545/3086 [1:25:29<17:01,  1.89s/it] 83%|████████▎ | 2546/3086 [1:25:31<17:18,  1.92s/it] 83%|████████▎ | 2547/3086 [1:25:34<17:49,  1.98s/it] 83%|████████▎ | 2548/3086 [1:25:36<19:21,  2.16s/it] 83%|████████▎ | 2549/3086 [1:25:38<18:30,  2.07s/it] 83%|████████▎ | 2550/3086 [1:25:40<17:27,  1.96s/it]                                                     {'loss': 0.2462, 'grad_norm': 0.07861009240150452, 'learning_rate': 0.00010421257290991574, 'epoch': 0.83}
 83%|████████▎ | 2550/3086 [1:25:40<17:27,  1.96s/it] 83%|████████▎ | 2551/3086 [1:25:42<17:03,  1.91s/it] 83%|████████▎ | 2552/3086 [1:25:43<16:23,  1.84s/it] 83%|████████▎ | 2553/3086 [1:25:45<16:12,  1.83s/it] 83%|████████▎ | 2554/3086 [1:25:48<18:16,  2.06s/it] 83%|████████▎ | 2555/3086 [1:25:50<18:44,  2.12s/it] 83%|████████▎ | 2556/3086 [1:25:52<19:03,  2.16s/it] 83%|████████▎ | 2557/3086 [1:25:54<18:19,  2.08s/it] 83%|████████▎ | 2558/3086 [1:25:56<18:31,  2.10s/it] 83%|████████▎ | 2559/3086 [1:25:58<17:46,  2.02s/it] 83%|████████▎ | 2560/3086 [1:26:00<17:29,  1.99s/it]                                                     {'loss': 0.2409, 'grad_norm': 0.06612736731767654, 'learning_rate': 0.00010226830848995461, 'epoch': 0.83}
 83%|████████▎ | 2560/3086 [1:26:00<17:29,  1.99s/it] 83%|████████▎ | 2561/3086 [1:26:02<17:08,  1.96s/it] 83%|████████▎ | 2562/3086 [1:26:04<16:39,  1.91s/it] 83%|████████▎ | 2563/3086 [1:26:05<16:35,  1.90s/it] 83%|████████▎ | 2564/3086 [1:26:07<16:02,  1.84s/it] 83%|████████▎ | 2565/3086 [1:26:09<16:28,  1.90s/it] 83%|████████▎ | 2566/3086 [1:26:11<15:37,  1.80s/it] 83%|████████▎ | 2567/3086 [1:26:13<17:19,  2.00s/it] 83%|████████▎ | 2568/3086 [1:26:15<17:21,  2.01s/it] 83%|████████▎ | 2569/3086 [1:26:17<16:53,  1.96s/it] 83%|████████▎ | 2570/3086 [1:26:19<16:06,  1.87s/it]                                                     {'loss': 0.2383, 'grad_norm': 0.08708310127258301, 'learning_rate': 0.00010032404406999351, 'epoch': 0.83}
 83%|████████▎ | 2570/3086 [1:26:19<16:06,  1.87s/it] 83%|████████▎ | 2571/3086 [1:26:21<16:24,  1.91s/it] 83%|████████▎ | 2572/3086 [1:26:23<15:59,  1.87s/it] 83%|████████▎ | 2573/3086 [1:26:24<15:54,  1.86s/it] 83%|████████▎ | 2574/3086 [1:26:26<15:39,  1.84s/it] 83%|████████▎ | 2575/3086 [1:26:29<17:12,  2.02s/it] 83%|████████▎ | 2576/3086 [1:26:31<18:17,  2.15s/it] 84%|████████▎ | 2577/3086 [1:26:33<17:48,  2.10s/it] 84%|████████▎ | 2578/3086 [1:26:35<18:20,  2.17s/it] 84%|████████▎ | 2579/3086 [1:26:37<17:22,  2.06s/it] 84%|████████▎ | 2580/3086 [1:26:39<15:57,  1.89s/it]                                                     {'loss': 0.2523, 'grad_norm': 0.08004926890134811, 'learning_rate': 9.837977965003239e-05, 'epoch': 0.84}
 84%|████████▎ | 2580/3086 [1:26:39<15:57,  1.89s/it] 84%|████████▎ | 2581/3086 [1:26:40<15:02,  1.79s/it] 84%|████████▎ | 2582/3086 [1:26:42<15:20,  1.83s/it] 84%|████████▎ | 2583/3086 [1:26:44<16:14,  1.94s/it] 84%|████████▎ | 2584/3086 [1:26:46<16:36,  1.98s/it] 84%|████████▍ | 2585/3086 [1:26:49<17:52,  2.14s/it] 84%|████████▍ | 2586/3086 [1:26:51<18:01,  2.16s/it] 84%|████████▍ | 2587/3086 [1:26:53<17:26,  2.10s/it] 84%|████████▍ | 2588/3086 [1:26:55<15:46,  1.90s/it] 84%|████████▍ | 2589/3086 [1:26:57<16:59,  2.05s/it] 84%|████████▍ | 2590/3086 [1:26:59<16:18,  1.97s/it]                                                     {'loss': 0.2426, 'grad_norm': 0.07941071689128876, 'learning_rate': 9.643551523007129e-05, 'epoch': 0.84}
 84%|████████▍ | 2590/3086 [1:26:59<16:18,  1.97s/it] 84%|████████▍ | 2591/3086 [1:27:01<16:00,  1.94s/it] 84%|████████▍ | 2592/3086 [1:27:02<15:30,  1.88s/it] 84%|████████▍ | 2593/3086 [1:27:04<14:42,  1.79s/it] 84%|████████▍ | 2594/3086 [1:27:06<15:43,  1.92s/it] 84%|████████▍ | 2595/3086 [1:27:09<17:05,  2.09s/it] 84%|████████▍ | 2596/3086 [1:27:11<16:59,  2.08s/it] 84%|████████▍ | 2597/3086 [1:27:13<17:31,  2.15s/it] 84%|████████▍ | 2598/3086 [1:27:15<16:17,  2.00s/it] 84%|████████▍ | 2599/3086 [1:27:16<15:40,  1.93s/it] 84%|████████▍ | 2600/3086 [1:27:18<15:00,  1.85s/it]                                                     {'loss': 0.2408, 'grad_norm': 0.07362264394760132, 'learning_rate': 9.449125081011016e-05, 'epoch': 0.84}
 84%|████████▍ | 2600/3086 [1:27:18<15:00,  1.85s/it] 84%|████████▍ | 2601/3086 [1:27:20<15:54,  1.97s/it] 84%|████████▍ | 2602/3086 [1:27:22<15:41,  1.94s/it] 84%|████████▍ | 2603/3086 [1:27:24<14:38,  1.82s/it] 84%|████████▍ | 2604/3086 [1:27:26<15:18,  1.91s/it] 84%|████████▍ | 2605/3086 [1:27:28<16:18,  2.03s/it] 84%|████████▍ | 2606/3086 [1:27:30<16:19,  2.04s/it] 84%|████████▍ | 2607/3086 [1:27:33<17:04,  2.14s/it] 85%|████████▍ | 2608/3086 [1:27:35<17:00,  2.14s/it] 85%|████████▍ | 2609/3086 [1:27:37<16:15,  2.05s/it] 85%|████████▍ | 2610/3086 [1:27:38<14:57,  1.88s/it]                                                     {'loss': 0.2426, 'grad_norm': 0.07207276672124863, 'learning_rate': 9.254698639014906e-05, 'epoch': 0.85}
 85%|████████▍ | 2610/3086 [1:27:38<14:57,  1.88s/it] 85%|████████▍ | 2611/3086 [1:27:41<16:31,  2.09s/it] 85%|████████▍ | 2612/3086 [1:27:43<16:57,  2.15s/it] 85%|████████▍ | 2613/3086 [1:27:45<15:59,  2.03s/it] 85%|████████▍ | 2614/3086 [1:27:47<15:30,  1.97s/it] 85%|████████▍ | 2615/3086 [1:27:48<15:11,  1.94s/it] 85%|████████▍ | 2616/3086 [1:27:50<15:02,  1.92s/it] 85%|████████▍ | 2617/3086 [1:27:52<14:46,  1.89s/it] 85%|████████▍ | 2618/3086 [1:27:54<15:06,  1.94s/it] 85%|████████▍ | 2619/3086 [1:27:56<15:02,  1.93s/it] 85%|████████▍ | 2620/3086 [1:27:58<15:45,  2.03s/it]                                                     {'loss': 0.2442, 'grad_norm': 0.07067937403917313, 'learning_rate': 9.060272197018794e-05, 'epoch': 0.85}
 85%|████████▍ | 2620/3086 [1:27:58<15:45,  2.03s/it] 85%|████████▍ | 2621/3086 [1:28:00<14:47,  1.91s/it] 85%|████████▍ | 2622/3086 [1:28:02<14:11,  1.84s/it] 85%|████████▍ | 2623/3086 [1:28:04<15:08,  1.96s/it] 85%|████████▌ | 2624/3086 [1:28:06<14:34,  1.89s/it] 85%|████████▌ | 2625/3086 [1:28:08<15:52,  2.07s/it] 85%|████████▌ | 2626/3086 [1:28:10<15:32,  2.03s/it] 85%|████████▌ | 2627/3086 [1:28:12<16:06,  2.10s/it] 85%|████████▌ | 2628/3086 [1:28:14<14:54,  1.95s/it] 85%|████████▌ | 2629/3086 [1:28:17<16:27,  2.16s/it] 85%|████████▌ | 2630/3086 [1:28:18<15:31,  2.04s/it]                                                     {'loss': 0.2404, 'grad_norm': 0.08177004754543304, 'learning_rate': 8.865845755022684e-05, 'epoch': 0.85}
 85%|████████▌ | 2630/3086 [1:28:18<15:31,  2.04s/it] 85%|████████▌ | 2631/3086 [1:28:20<14:28,  1.91s/it] 85%|████████▌ | 2632/3086 [1:28:22<14:45,  1.95s/it] 85%|████████▌ | 2633/3086 [1:28:24<14:02,  1.86s/it] 85%|████████▌ | 2634/3086 [1:28:26<14:58,  1.99s/it] 85%|████████▌ | 2635/3086 [1:28:28<15:31,  2.07s/it] 85%|████████▌ | 2636/3086 [1:28:30<15:13,  2.03s/it] 85%|████████▌ | 2637/3086 [1:28:32<14:53,  1.99s/it] 85%|████████▌ | 2638/3086 [1:28:34<14:22,  1.93s/it] 86%|████████▌ | 2639/3086 [1:28:36<14:12,  1.91s/it] 86%|████████▌ | 2640/3086 [1:28:38<14:52,  2.00s/it]                                                     {'loss': 0.246, 'grad_norm': 0.07929342985153198, 'learning_rate': 8.671419313026571e-05, 'epoch': 0.86}
 86%|████████▌ | 2640/3086 [1:28:38<14:52,  2.00s/it] 86%|████████▌ | 2641/3086 [1:28:40<14:34,  1.97s/it] 86%|████████▌ | 2642/3086 [1:28:42<15:38,  2.11s/it] 86%|████████▌ | 2643/3086 [1:28:44<14:13,  1.93s/it] 86%|████████▌ | 2644/3086 [1:28:46<14:12,  1.93s/it] 86%|████████▌ | 2645/3086 [1:28:47<13:54,  1.89s/it] 86%|████████▌ | 2646/3086 [1:28:49<13:39,  1.86s/it] 86%|████████▌ | 2647/3086 [1:28:51<13:24,  1.83s/it] 86%|████████▌ | 2648/3086 [1:28:53<13:37,  1.87s/it] 86%|████████▌ | 2649/3086 [1:28:55<13:41,  1.88s/it] 86%|████████▌ | 2650/3086 [1:28:57<15:24,  2.12s/it]                                                     {'loss': 0.2427, 'grad_norm': 0.0791608914732933, 'learning_rate': 8.47699287103046e-05, 'epoch': 0.86}
 86%|████████▌ | 2650/3086 [1:28:57<15:24,  2.12s/it] 86%|████████▌ | 2651/3086 [1:29:00<15:25,  2.13s/it] 86%|████████▌ | 2652/3086 [1:29:01<14:40,  2.03s/it] 86%|████████▌ | 2653/3086 [1:29:04<14:45,  2.04s/it] 86%|████████▌ | 2654/3086 [1:29:05<14:29,  2.01s/it] 86%|████████▌ | 2655/3086 [1:29:08<15:59,  2.23s/it] 86%|████████▌ | 2656/3086 [1:29:10<15:20,  2.14s/it] 86%|████████▌ | 2657/3086 [1:29:12<15:29,  2.17s/it] 86%|████████▌ | 2658/3086 [1:29:15<16:08,  2.26s/it] 86%|████████▌ | 2659/3086 [1:29:17<16:45,  2.36s/it] 86%|████████▌ | 2660/3086 [1:29:19<15:34,  2.19s/it]                                                     {'loss': 0.245, 'grad_norm': 0.07992852479219437, 'learning_rate': 8.282566429034348e-05, 'epoch': 0.86}
 86%|████████▌ | 2660/3086 [1:29:19<15:34,  2.19s/it] 86%|████████▌ | 2661/3086 [1:29:21<14:37,  2.06s/it] 86%|████████▋ | 2662/3086 [1:29:23<15:12,  2.15s/it] 86%|████████▋ | 2663/3086 [1:29:25<15:02,  2.13s/it] 86%|████████▋ | 2664/3086 [1:29:27<14:10,  2.02s/it] 86%|████████▋ | 2665/3086 [1:29:29<13:15,  1.89s/it] 86%|████████▋ | 2666/3086 [1:29:30<12:52,  1.84s/it] 86%|████████▋ | 2667/3086 [1:29:33<13:32,  1.94s/it] 86%|████████▋ | 2668/3086 [1:29:35<13:35,  1.95s/it] 86%|████████▋ | 2669/3086 [1:29:37<13:31,  1.95s/it] 87%|████████▋ | 2670/3086 [1:29:39<14:03,  2.03s/it]                                                     {'loss': 0.2431, 'grad_norm': 0.07370655983686447, 'learning_rate': 8.088139987038236e-05, 'epoch': 0.87}
 87%|████████▋ | 2670/3086 [1:29:39<14:03,  2.03s/it] 87%|████████▋ | 2671/3086 [1:29:41<15:02,  2.18s/it] 87%|████████▋ | 2672/3086 [1:29:43<14:17,  2.07s/it] 87%|████████▋ | 2673/3086 [1:29:45<14:02,  2.04s/it] 87%|████████▋ | 2674/3086 [1:29:47<13:58,  2.04s/it] 87%|████████▋ | 2675/3086 [1:29:50<14:56,  2.18s/it] 87%|████████▋ | 2676/3086 [1:29:51<14:02,  2.06s/it] 87%|████████▋ | 2677/3086 [1:29:53<13:28,  1.98s/it] 87%|████████▋ | 2678/3086 [1:29:56<14:14,  2.09s/it] 87%|████████▋ | 2679/3086 [1:29:57<13:21,  1.97s/it] 87%|████████▋ | 2680/3086 [1:29:59<13:28,  1.99s/it]                                                     {'loss': 0.2412, 'grad_norm': 0.08030896633863449, 'learning_rate': 7.893713545042126e-05, 'epoch': 0.87}
 87%|████████▋ | 2680/3086 [1:29:59<13:28,  1.99s/it] 87%|████████▋ | 2681/3086 [1:30:02<14:51,  2.20s/it] 87%|████████▋ | 2682/3086 [1:30:04<14:01,  2.08s/it] 87%|████████▋ | 2683/3086 [1:30:07<15:28,  2.30s/it] 87%|████████▋ | 2684/3086 [1:30:08<14:33,  2.17s/it] 87%|████████▋ | 2685/3086 [1:30:10<13:25,  2.01s/it] 87%|████████▋ | 2686/3086 [1:30:12<13:06,  1.97s/it] 87%|████████▋ | 2687/3086 [1:30:14<13:17,  2.00s/it] 87%|████████▋ | 2688/3086 [1:30:16<13:18,  2.01s/it] 87%|████████▋ | 2689/3086 [1:30:18<12:24,  1.88s/it] 87%|████████▋ | 2690/3086 [1:30:20<12:34,  1.90s/it]                                                     {'loss': 0.2363, 'grad_norm': 0.0635087639093399, 'learning_rate': 7.699287103046013e-05, 'epoch': 0.87}
 87%|████████▋ | 2690/3086 [1:30:20<12:34,  1.90s/it] 87%|████████▋ | 2691/3086 [1:30:21<12:01,  1.83s/it] 87%|████████▋ | 2692/3086 [1:30:23<12:45,  1.94s/it] 87%|████████▋ | 2693/3086 [1:30:26<13:06,  2.00s/it] 87%|████████▋ | 2694/3086 [1:30:28<13:39,  2.09s/it] 87%|████████▋ | 2695/3086 [1:30:30<13:23,  2.05s/it] 87%|████████▋ | 2696/3086 [1:30:32<12:53,  1.98s/it] 87%|████████▋ | 2697/3086 [1:30:34<12:39,  1.95s/it] 87%|████████▋ | 2698/3086 [1:30:35<12:33,  1.94s/it] 87%|████████▋ | 2699/3086 [1:30:37<12:26,  1.93s/it] 87%|████████▋ | 2700/3086 [1:30:39<12:08,  1.89s/it]                                                     {'loss': 0.2502, 'grad_norm': 0.07386669516563416, 'learning_rate': 7.504860661049902e-05, 'epoch': 0.87}
 87%|████████▋ | 2700/3086 [1:30:39<12:08,  1.89s/it] 88%|████████▊ | 2701/3086 [1:30:41<12:11,  1.90s/it] 88%|████████▊ | 2702/3086 [1:30:43<12:06,  1.89s/it] 88%|████████▊ | 2703/3086 [1:30:45<11:40,  1.83s/it] 88%|████████▊ | 2704/3086 [1:30:47<11:52,  1.87s/it] 88%|████████▊ | 2705/3086 [1:30:49<12:12,  1.92s/it] 88%|████████▊ | 2706/3086 [1:30:51<12:14,  1.93s/it] 88%|████████▊ | 2707/3086 [1:30:53<13:00,  2.06s/it] 88%|████████▊ | 2708/3086 [1:30:55<12:04,  1.92s/it] 88%|████████▊ | 2709/3086 [1:30:56<11:49,  1.88s/it] 88%|████████▊ | 2710/3086 [1:30:58<11:14,  1.79s/it]                                                     {'loss': 0.2437, 'grad_norm': 0.08004176616668701, 'learning_rate': 7.31043421905379e-05, 'epoch': 0.88}
 88%|████████▊ | 2710/3086 [1:30:58<11:14,  1.79s/it] 88%|████████▊ | 2711/3086 [1:31:00<11:40,  1.87s/it] 88%|████████▊ | 2712/3086 [1:31:02<11:30,  1.85s/it] 88%|████████▊ | 2713/3086 [1:31:04<11:47,  1.90s/it] 88%|████████▊ | 2714/3086 [1:31:06<11:39,  1.88s/it] 88%|████████▊ | 2715/3086 [1:31:07<11:03,  1.79s/it] 88%|████████▊ | 2716/3086 [1:31:09<11:00,  1.79s/it] 88%|████████▊ | 2717/3086 [1:31:11<11:40,  1.90s/it] 88%|████████▊ | 2718/3086 [1:31:13<11:09,  1.82s/it] 88%|████████▊ | 2719/3086 [1:31:15<11:27,  1.87s/it] 88%|████████▊ | 2720/3086 [1:31:17<11:20,  1.86s/it]                                                     {'loss': 0.2415, 'grad_norm': 0.06588630378246307, 'learning_rate': 7.116007777057679e-05, 'epoch': 0.88}
 88%|████████▊ | 2720/3086 [1:31:17<11:20,  1.86s/it] 88%|████████▊ | 2721/3086 [1:31:18<11:21,  1.87s/it] 88%|████████▊ | 2722/3086 [1:31:20<10:58,  1.81s/it] 88%|████████▊ | 2723/3086 [1:31:22<11:09,  1.84s/it] 88%|████████▊ | 2724/3086 [1:31:24<11:06,  1.84s/it] 88%|████████▊ | 2725/3086 [1:31:26<11:23,  1.89s/it] 88%|████████▊ | 2726/3086 [1:31:28<10:57,  1.83s/it] 88%|████████▊ | 2727/3086 [1:31:30<11:26,  1.91s/it] 88%|████████▊ | 2728/3086 [1:31:31<10:50,  1.82s/it] 88%|████████▊ | 2729/3086 [1:31:33<11:01,  1.85s/it] 88%|████████▊ | 2730/3086 [1:31:35<11:39,  1.96s/it]                                                     {'loss': 0.2436, 'grad_norm': 0.07192802429199219, 'learning_rate': 6.921581335061568e-05, 'epoch': 0.88}
 88%|████████▊ | 2730/3086 [1:31:35<11:39,  1.96s/it] 88%|████████▊ | 2731/3086 [1:31:37<11:31,  1.95s/it] 89%|████████▊ | 2732/3086 [1:31:40<12:08,  2.06s/it] 89%|████████▊ | 2733/3086 [1:31:42<11:49,  2.01s/it] 89%|████████▊ | 2734/3086 [1:31:43<11:14,  1.92s/it] 89%|████████▊ | 2735/3086 [1:31:46<11:45,  2.01s/it] 89%|████████▊ | 2736/3086 [1:31:48<12:57,  2.22s/it] 89%|████████▊ | 2737/3086 [1:31:50<12:03,  2.07s/it] 89%|████████▊ | 2738/3086 [1:31:53<13:04,  2.25s/it] 89%|████████▉ | 2739/3086 [1:31:55<13:05,  2.27s/it] 89%|████████▉ | 2740/3086 [1:31:57<12:47,  2.22s/it]                                                     {'loss': 0.2434, 'grad_norm': 0.072693832218647, 'learning_rate': 6.727154893065455e-05, 'epoch': 0.89}
 89%|████████▉ | 2740/3086 [1:31:57<12:47,  2.22s/it] 89%|████████▉ | 2741/3086 [1:31:59<12:52,  2.24s/it] 89%|████████▉ | 2742/3086 [1:32:01<11:46,  2.05s/it] 89%|████████▉ | 2743/3086 [1:32:03<12:06,  2.12s/it] 89%|████████▉ | 2744/3086 [1:32:05<12:03,  2.11s/it] 89%|████████▉ | 2745/3086 [1:32:07<10:56,  1.92s/it] 89%|████████▉ | 2746/3086 [1:32:09<10:50,  1.91s/it] 89%|████████▉ | 2747/3086 [1:32:11<11:09,  1.97s/it] 89%|████████▉ | 2748/3086 [1:32:13<10:38,  1.89s/it] 89%|████████▉ | 2749/3086 [1:32:15<11:10,  1.99s/it] 89%|████████▉ | 2750/3086 [1:32:17<11:02,  1.97s/it]                                                     {'loss': 0.2406, 'grad_norm': 0.06984426826238632, 'learning_rate': 6.532728451069344e-05, 'epoch': 0.89}
 89%|████████▉ | 2750/3086 [1:32:17<11:02,  1.97s/it] 89%|████████▉ | 2751/3086 [1:32:18<10:39,  1.91s/it] 89%|████████▉ | 2752/3086 [1:32:20<10:41,  1.92s/it] 89%|████████▉ | 2753/3086 [1:32:22<10:18,  1.86s/it] 89%|████████▉ | 2754/3086 [1:32:24<10:36,  1.92s/it] 89%|████████▉ | 2755/3086 [1:32:26<11:01,  2.00s/it] 89%|████████▉ | 2756/3086 [1:32:29<11:28,  2.09s/it] 89%|████████▉ | 2757/3086 [1:32:31<11:37,  2.12s/it] 89%|████████▉ | 2758/3086 [1:32:33<11:29,  2.10s/it] 89%|████████▉ | 2759/3086 [1:32:35<11:11,  2.05s/it] 89%|████████▉ | 2760/3086 [1:32:37<11:08,  2.05s/it]                                                     {'loss': 0.2386, 'grad_norm': 0.08074313402175903, 'learning_rate': 6.338302009073233e-05, 'epoch': 0.89}
 89%|████████▉ | 2760/3086 [1:32:37<11:08,  2.05s/it] 89%|████████▉ | 2761/3086 [1:32:39<10:55,  2.02s/it] 90%|████████▉ | 2762/3086 [1:32:41<10:35,  1.96s/it] 90%|████████▉ | 2763/3086 [1:32:42<10:11,  1.89s/it] 90%|████████▉ | 2764/3086 [1:32:44<09:46,  1.82s/it] 90%|████████▉ | 2765/3086 [1:32:46<09:54,  1.85s/it] 90%|████████▉ | 2766/3086 [1:32:49<11:26,  2.15s/it] 90%|████████▉ | 2767/3086 [1:32:51<11:25,  2.15s/it] 90%|████████▉ | 2768/3086 [1:32:53<11:34,  2.18s/it] 90%|████████▉ | 2769/3086 [1:32:55<10:49,  2.05s/it] 90%|████████▉ | 2770/3086 [1:32:57<10:12,  1.94s/it]                                                     {'loss': 0.2444, 'grad_norm': 0.08010202646255493, 'learning_rate': 6.143875567077121e-05, 'epoch': 0.9}
 90%|████████▉ | 2770/3086 [1:32:57<10:12,  1.94s/it] 90%|████████▉ | 2771/3086 [1:32:59<10:25,  1.98s/it] 90%|████████▉ | 2772/3086 [1:33:01<10:06,  1.93s/it] 90%|████████▉ | 2773/3086 [1:33:02<09:31,  1.83s/it] 90%|████████▉ | 2774/3086 [1:33:04<09:31,  1.83s/it] 90%|████████▉ | 2775/3086 [1:33:06<09:54,  1.91s/it] 90%|████████▉ | 2776/3086 [1:33:08<09:25,  1.82s/it] 90%|████████▉ | 2777/3086 [1:33:10<10:12,  1.98s/it] 90%|█████████ | 2778/3086 [1:33:12<10:13,  1.99s/it] 90%|█████████ | 2779/3086 [1:33:14<10:12,  1.99s/it] 90%|█████████ | 2780/3086 [1:33:16<10:08,  1.99s/it]                                                     {'loss': 0.2409, 'grad_norm': 0.07466897368431091, 'learning_rate': 5.94944912508101e-05, 'epoch': 0.9}
 90%|█████████ | 2780/3086 [1:33:16<10:08,  1.99s/it] 90%|█████████ | 2781/3086 [1:33:18<09:25,  1.85s/it] 90%|█████████ | 2782/3086 [1:33:19<09:08,  1.81s/it] 90%|█████████ | 2783/3086 [1:33:21<09:21,  1.85s/it] 90%|█████████ | 2784/3086 [1:33:23<09:26,  1.88s/it] 90%|█████████ | 2785/3086 [1:33:25<09:13,  1.84s/it] 90%|█████████ | 2786/3086 [1:33:27<09:16,  1.85s/it] 90%|█████████ | 2787/3086 [1:33:29<09:29,  1.90s/it] 90%|█████████ | 2788/3086 [1:33:31<09:18,  1.87s/it] 90%|█████████ | 2789/3086 [1:33:33<09:37,  1.95s/it] 90%|█████████ | 2790/3086 [1:33:35<09:46,  1.98s/it]                                                     {'loss': 0.2373, 'grad_norm': 0.08394363522529602, 'learning_rate': 5.755022683084899e-05, 'epoch': 0.9}
 90%|█████████ | 2790/3086 [1:33:35<09:46,  1.98s/it] 90%|█████████ | 2791/3086 [1:33:37<09:54,  2.01s/it] 90%|█████████ | 2792/3086 [1:33:39<09:30,  1.94s/it] 91%|█████████ | 2793/3086 [1:33:41<09:38,  1.97s/it] 91%|█████████ | 2794/3086 [1:33:42<09:12,  1.89s/it] 91%|█████████ | 2795/3086 [1:33:44<09:24,  1.94s/it] 91%|█████████ | 2796/3086 [1:33:46<09:20,  1.93s/it] 91%|█████████ | 2797/3086 [1:33:49<09:52,  2.05s/it] 91%|█████████ | 2798/3086 [1:33:51<10:11,  2.12s/it] 91%|█████████ | 2799/3086 [1:33:53<09:52,  2.06s/it] 91%|█████████ | 2800/3086 [1:33:55<09:29,  1.99s/it]                                                     {'loss': 0.235, 'grad_norm': 0.08476627618074417, 'learning_rate': 5.5605962410887875e-05, 'epoch': 0.91}
 91%|█████████ | 2800/3086 [1:33:55<09:29,  1.99s/it] 91%|█████████ | 2801/3086 [1:33:56<08:54,  1.87s/it] 91%|█████████ | 2802/3086 [1:33:58<08:36,  1.82s/it] 91%|█████████ | 2803/3086 [1:34:00<08:20,  1.77s/it] 91%|█████████ | 2804/3086 [1:34:02<08:29,  1.81s/it] 91%|█████████ | 2805/3086 [1:34:04<09:15,  1.98s/it] 91%|█████████ | 2806/3086 [1:34:06<09:30,  2.04s/it] 91%|█████████ | 2807/3086 [1:34:09<09:58,  2.15s/it] 91%|█████████ | 2808/3086 [1:34:11<09:59,  2.16s/it] 91%|█████████ | 2809/3086 [1:34:13<09:33,  2.07s/it] 91%|█████████ | 2810/3086 [1:34:14<09:11,  2.00s/it]                                                     {'loss': 0.243, 'grad_norm': 0.07554317265748978, 'learning_rate': 5.366169799092676e-05, 'epoch': 0.91}
 91%|█████████ | 2810/3086 [1:34:14<09:11,  2.00s/it] 91%|█████████ | 2811/3086 [1:34:16<08:39,  1.89s/it] 91%|█████████ | 2812/3086 [1:34:18<08:39,  1.90s/it] 91%|█████████ | 2813/3086 [1:34:20<09:04,  2.00s/it] 91%|█████████ | 2814/3086 [1:34:22<08:38,  1.91s/it] 91%|█████████ | 2815/3086 [1:34:24<08:40,  1.92s/it] 91%|█████████▏| 2816/3086 [1:34:26<08:34,  1.91s/it] 91%|█████████▏| 2817/3086 [1:34:28<08:47,  1.96s/it] 91%|█████████▏| 2818/3086 [1:34:30<09:07,  2.04s/it] 91%|█████████▏| 2819/3086 [1:34:32<09:07,  2.05s/it] 91%|█████████▏| 2820/3086 [1:34:34<08:55,  2.01s/it]                                                     {'loss': 0.2465, 'grad_norm': 0.07239843159914017, 'learning_rate': 5.171743357096565e-05, 'epoch': 0.91}
 91%|█████████▏| 2820/3086 [1:34:34<08:55,  2.01s/it] 91%|█████████▏| 2821/3086 [1:34:36<08:52,  2.01s/it] 91%|█████████▏| 2822/3086 [1:34:38<08:50,  2.01s/it] 91%|█████████▏| 2823/3086 [1:34:40<09:02,  2.06s/it] 92%|█████████▏| 2824/3086 [1:34:42<08:53,  2.04s/it] 92%|█████████▏| 2825/3086 [1:34:44<08:42,  2.00s/it] 92%|█████████▏| 2826/3086 [1:34:46<08:10,  1.88s/it] 92%|█████████▏| 2827/3086 [1:34:48<08:28,  1.96s/it] 92%|█████████▏| 2828/3086 [1:34:50<08:02,  1.87s/it] 92%|█████████▏| 2829/3086 [1:34:51<07:57,  1.86s/it] 92%|█████████▏| 2830/3086 [1:34:53<07:35,  1.78s/it]                                                     {'loss': 0.2405, 'grad_norm': 0.08755800127983093, 'learning_rate': 4.9773169151004536e-05, 'epoch': 0.92}
 92%|█████████▏| 2830/3086 [1:34:53<07:35,  1.78s/it] 92%|█████████▏| 2831/3086 [1:34:55<08:28,  1.99s/it] 92%|█████████▏| 2832/3086 [1:34:57<08:13,  1.94s/it] 92%|█████████▏| 2833/3086 [1:34:59<08:16,  1.96s/it] 92%|█████████▏| 2834/3086 [1:35:02<08:44,  2.08s/it] 92%|█████████▏| 2835/3086 [1:35:03<08:19,  1.99s/it] 92%|█████████▏| 2836/3086 [1:35:06<08:44,  2.10s/it] 92%|█████████▏| 2837/3086 [1:35:07<08:00,  1.93s/it] 92%|█████████▏| 2838/3086 [1:35:10<08:30,  2.06s/it] 92%|█████████▏| 2839/3086 [1:35:12<08:31,  2.07s/it] 92%|█████████▏| 2840/3086 [1:35:14<08:28,  2.07s/it]                                                     {'loss': 0.2376, 'grad_norm': 0.07518039643764496, 'learning_rate': 4.7828904731043416e-05, 'epoch': 0.92}
 92%|█████████▏| 2840/3086 [1:35:14<08:28,  2.07s/it] 92%|█████████▏| 2841/3086 [1:35:16<08:20,  2.04s/it] 92%|█████████▏| 2842/3086 [1:35:18<08:11,  2.01s/it] 92%|█████████▏| 2843/3086 [1:35:20<08:15,  2.04s/it] 92%|█████████▏| 2844/3086 [1:35:22<08:40,  2.15s/it] 92%|█████████▏| 2845/3086 [1:35:24<08:28,  2.11s/it] 92%|█████████▏| 2846/3086 [1:35:27<08:34,  2.15s/it] 92%|█████████▏| 2847/3086 [1:35:28<08:02,  2.02s/it] 92%|█████████▏| 2848/3086 [1:35:30<07:48,  1.97s/it] 92%|█████████▏| 2849/3086 [1:35:32<07:30,  1.90s/it] 92%|█████████▏| 2850/3086 [1:35:33<07:04,  1.80s/it]                                                     {'loss': 0.2417, 'grad_norm': 0.07926224917173386, 'learning_rate': 4.58846403110823e-05, 'epoch': 0.92}
 92%|█████████▏| 2850/3086 [1:35:33<07:04,  1.80s/it] 92%|█████████▏| 2851/3086 [1:35:35<07:04,  1.81s/it] 92%|█████████▏| 2852/3086 [1:35:37<07:25,  1.90s/it] 92%|█████████▏| 2853/3086 [1:35:39<07:39,  1.97s/it] 92%|█████████▏| 2854/3086 [1:35:42<08:23,  2.17s/it] 93%|█████████▎| 2855/3086 [1:35:44<07:49,  2.03s/it] 93%|█████████▎| 2856/3086 [1:35:46<08:31,  2.22s/it] 93%|█████████▎| 2857/3086 [1:35:48<08:10,  2.14s/it] 93%|█████████▎| 2858/3086 [1:35:50<07:43,  2.03s/it] 93%|█████████▎| 2859/3086 [1:35:52<07:05,  1.88s/it] 93%|█████████▎| 2860/3086 [1:35:54<07:24,  1.96s/it]                                                     {'loss': 0.2431, 'grad_norm': 0.074427530169487, 'learning_rate': 4.394037589112119e-05, 'epoch': 0.93}
 93%|█████████▎| 2860/3086 [1:35:54<07:24,  1.96s/it] 93%|█████████▎| 2861/3086 [1:35:56<07:13,  1.93s/it] 93%|█████████▎| 2862/3086 [1:35:58<07:25,  1.99s/it] 93%|█████████▎| 2863/3086 [1:36:00<07:33,  2.03s/it] 93%|█████████▎| 2864/3086 [1:36:02<07:42,  2.08s/it] 93%|█████████▎| 2865/3086 [1:36:04<07:24,  2.01s/it] 93%|█████████▎| 2866/3086 [1:36:06<07:11,  1.96s/it] 93%|█████████▎| 2867/3086 [1:36:08<07:07,  1.95s/it] 93%|█████████▎| 2868/3086 [1:36:10<06:57,  1.91s/it] 93%|█████████▎| 2869/3086 [1:36:12<07:29,  2.07s/it] 93%|█████████▎| 2870/3086 [1:36:15<07:57,  2.21s/it]                                                     {'loss': 0.2366, 'grad_norm': 0.08009637147188187, 'learning_rate': 4.199611147116008e-05, 'epoch': 0.93}
 93%|█████████▎| 2870/3086 [1:36:15<07:57,  2.21s/it] 93%|█████████▎| 2871/3086 [1:36:17<07:46,  2.17s/it] 93%|█████████▎| 2872/3086 [1:36:19<07:53,  2.21s/it] 93%|█████████▎| 2873/3086 [1:36:21<07:41,  2.17s/it] 93%|█████████▎| 2874/3086 [1:36:23<07:43,  2.19s/it] 93%|█████████▎| 2875/3086 [1:36:25<07:24,  2.11s/it] 93%|█████████▎| 2876/3086 [1:36:27<07:02,  2.01s/it] 93%|█████████▎| 2877/3086 [1:36:29<06:59,  2.01s/it] 93%|█████████▎| 2878/3086 [1:36:31<07:02,  2.03s/it] 93%|█████████▎| 2879/3086 [1:36:33<06:50,  1.98s/it] 93%|█████████▎| 2880/3086 [1:36:35<06:34,  1.92s/it]                                                     {'loss': 0.2393, 'grad_norm': 0.06715533137321472, 'learning_rate': 4.0051847051198964e-05, 'epoch': 0.93}
 93%|█████████▎| 2880/3086 [1:36:35<06:34,  1.92s/it] 93%|█████████▎| 2881/3086 [1:36:37<06:37,  1.94s/it] 93%|█████████▎| 2882/3086 [1:36:39<06:38,  1.96s/it] 93%|█████████▎| 2883/3086 [1:36:40<06:04,  1.79s/it] 93%|█████████▎| 2884/3086 [1:36:43<06:44,  2.00s/it] 93%|█████████▎| 2885/3086 [1:36:45<06:45,  2.02s/it] 94%|█████████▎| 2886/3086 [1:36:47<06:50,  2.05s/it] 94%|█████████▎| 2887/3086 [1:36:49<06:55,  2.09s/it] 94%|█████████▎| 2888/3086 [1:36:51<06:44,  2.04s/it] 94%|█████████▎| 2889/3086 [1:36:53<06:52,  2.09s/it] 94%|█████████▎| 2890/3086 [1:36:55<06:49,  2.09s/it]                                                     {'loss': 0.2416, 'grad_norm': 0.07446558773517609, 'learning_rate': 3.810758263123785e-05, 'epoch': 0.94}
 94%|█████████▎| 2890/3086 [1:36:55<06:49,  2.09s/it] 94%|█████████▎| 2891/3086 [1:36:57<06:39,  2.05s/it] 94%|█████████▎| 2892/3086 [1:36:59<06:43,  2.08s/it] 94%|█████████▎| 2893/3086 [1:37:01<06:33,  2.04s/it] 94%|█████████▍| 2894/3086 [1:37:03<06:32,  2.05s/it] 94%|█████████▍| 2895/3086 [1:37:06<06:49,  2.15s/it] 94%|█████████▍| 2896/3086 [1:37:08<07:09,  2.26s/it] 94%|█████████▍| 2897/3086 [1:37:10<06:34,  2.09s/it] 94%|█████████▍| 2898/3086 [1:37:12<06:49,  2.18s/it] 94%|█████████▍| 2899/3086 [1:37:15<07:03,  2.27s/it] 94%|█████████▍| 2900/3086 [1:37:16<06:29,  2.09s/it]                                                     {'loss': 0.239, 'grad_norm': 0.06700164079666138, 'learning_rate': 3.616331821127673e-05, 'epoch': 0.94}
 94%|█████████▍| 2900/3086 [1:37:16<06:29,  2.09s/it] 94%|█████████▍| 2901/3086 [1:37:18<06:14,  2.02s/it] 94%|█████████▍| 2902/3086 [1:37:20<06:16,  2.05s/it] 94%|█████████▍| 2903/3086 [1:37:23<06:16,  2.06s/it] 94%|█████████▍| 2904/3086 [1:37:24<05:41,  1.87s/it] 94%|█████████▍| 2905/3086 [1:37:26<05:47,  1.92s/it] 94%|█████████▍| 2906/3086 [1:37:28<06:00,  2.00s/it] 94%|█████████▍| 2907/3086 [1:37:30<05:39,  1.90s/it] 94%|█████████▍| 2908/3086 [1:37:32<06:01,  2.03s/it] 94%|█████████▍| 2909/3086 [1:37:34<05:53,  2.00s/it] 94%|█████████▍| 2910/3086 [1:37:36<06:12,  2.12s/it]                                                     {'loss': 0.2364, 'grad_norm': 0.06709403544664383, 'learning_rate': 3.421905379131562e-05, 'epoch': 0.94}
 94%|█████████▍| 2910/3086 [1:37:36<06:12,  2.12s/it] 94%|█████████▍| 2911/3086 [1:37:38<05:46,  1.98s/it] 94%|█████████▍| 2912/3086 [1:37:41<06:04,  2.09s/it] 94%|█████████▍| 2913/3086 [1:37:42<05:37,  1.95s/it] 94%|█████████▍| 2914/3086 [1:37:44<05:53,  2.06s/it] 94%|█████████▍| 2915/3086 [1:37:46<05:45,  2.02s/it] 94%|█████████▍| 2916/3086 [1:37:49<05:59,  2.11s/it] 95%|█████████▍| 2917/3086 [1:37:50<05:40,  2.01s/it] 95%|█████████▍| 2918/3086 [1:37:53<05:42,  2.04s/it] 95%|█████████▍| 2919/3086 [1:37:54<05:26,  1.95s/it] 95%|█████████▍| 2920/3086 [1:37:56<05:15,  1.90s/it]                                                     {'loss': 0.2396, 'grad_norm': 0.07679631561040878, 'learning_rate': 3.2274789371354506e-05, 'epoch': 0.95}
 95%|█████████▍| 2920/3086 [1:37:56<05:15,  1.90s/it] 95%|█████████▍| 2921/3086 [1:37:58<05:18,  1.93s/it] 95%|█████████▍| 2922/3086 [1:38:00<05:09,  1.89s/it] 95%|█████████▍| 2923/3086 [1:38:02<05:36,  2.06s/it] 95%|█████████▍| 2924/3086 [1:38:04<05:27,  2.02s/it] 95%|█████████▍| 2925/3086 [1:38:06<05:29,  2.05s/it] 95%|█████████▍| 2926/3086 [1:38:08<05:16,  1.98s/it] 95%|█████████▍| 2927/3086 [1:38:10<05:25,  2.05s/it] 95%|█████████▍| 2928/3086 [1:38:12<05:11,  1.97s/it] 95%|█████████▍| 2929/3086 [1:38:14<05:08,  1.97s/it] 95%|█████████▍| 2930/3086 [1:38:16<05:16,  2.03s/it]                                                     {'loss': 0.2432, 'grad_norm': 0.07303281873464584, 'learning_rate': 3.033052495139339e-05, 'epoch': 0.95}
 95%|█████████▍| 2930/3086 [1:38:16<05:16,  2.03s/it] 95%|█████████▍| 2931/3086 [1:38:18<05:14,  2.03s/it] 95%|█████████▌| 2932/3086 [1:38:20<05:16,  2.06s/it] 95%|█████████▌| 2933/3086 [1:38:22<04:53,  1.92s/it] 95%|█████████▌| 2934/3086 [1:38:24<04:44,  1.87s/it] 95%|█████████▌| 2935/3086 [1:38:26<04:49,  1.92s/it] 95%|█████████▌| 2936/3086 [1:38:28<04:40,  1.87s/it] 95%|█████████▌| 2937/3086 [1:38:29<04:34,  1.84s/it] 95%|█████████▌| 2938/3086 [1:38:31<04:33,  1.85s/it] 95%|█████████▌| 2939/3086 [1:38:33<04:47,  1.96s/it] 95%|█████████▌| 2940/3086 [1:38:35<04:45,  1.95s/it]                                                     {'loss': 0.2457, 'grad_norm': 0.0695025771856308, 'learning_rate': 2.8386260531432273e-05, 'epoch': 0.95}
 95%|█████████▌| 2940/3086 [1:38:35<04:45,  1.95s/it] 95%|█████████▌| 2941/3086 [1:38:37<04:42,  1.95s/it] 95%|█████████▌| 2942/3086 [1:38:40<05:06,  2.13s/it] 95%|█████████▌| 2943/3086 [1:38:42<04:47,  2.01s/it] 95%|█████████▌| 2944/3086 [1:38:43<04:30,  1.91s/it] 95%|█████████▌| 2945/3086 [1:38:45<04:36,  1.96s/it] 95%|█████████▌| 2946/3086 [1:38:47<04:21,  1.87s/it] 95%|█████████▌| 2947/3086 [1:38:49<04:34,  1.98s/it] 96%|█████████▌| 2948/3086 [1:38:51<04:34,  1.99s/it] 96%|█████████▌| 2949/3086 [1:38:53<04:21,  1.91s/it] 96%|█████████▌| 2950/3086 [1:38:55<04:19,  1.91s/it]                                                     {'loss': 0.2395, 'grad_norm': 0.06844978034496307, 'learning_rate': 2.6441996111471156e-05, 'epoch': 0.96}
 96%|█████████▌| 2950/3086 [1:38:55<04:19,  1.91s/it] 96%|█████████▌| 2951/3086 [1:38:57<04:25,  1.97s/it] 96%|█████████▌| 2952/3086 [1:38:59<04:22,  1.96s/it] 96%|█████████▌| 2953/3086 [1:39:01<04:12,  1.90s/it] 96%|█████████▌| 2954/3086 [1:39:03<04:22,  1.99s/it] 96%|█████████▌| 2955/3086 [1:39:05<04:34,  2.10s/it] 96%|█████████▌| 2956/3086 [1:39:07<04:25,  2.04s/it] 96%|█████████▌| 2957/3086 [1:39:09<04:21,  2.03s/it] 96%|█████████▌| 2958/3086 [1:39:11<04:14,  1.99s/it] 96%|█████████▌| 2959/3086 [1:39:13<04:23,  2.08s/it] 96%|█████████▌| 2960/3086 [1:39:15<04:20,  2.07s/it]                                                     {'loss': 0.2342, 'grad_norm': 0.06493648141622543, 'learning_rate': 2.4497731691510043e-05, 'epoch': 0.96}
 96%|█████████▌| 2960/3086 [1:39:15<04:20,  2.07s/it] 96%|█████████▌| 2961/3086 [1:39:17<04:09,  2.00s/it] 96%|█████████▌| 2962/3086 [1:39:19<04:08,  2.00s/it] 96%|█████████▌| 2963/3086 [1:39:22<04:26,  2.16s/it] 96%|█████████▌| 2964/3086 [1:39:24<04:14,  2.09s/it] 96%|█████████▌| 2965/3086 [1:39:25<04:00,  1.99s/it] 96%|█████████▌| 2966/3086 [1:39:27<03:49,  1.91s/it] 96%|█████████▌| 2967/3086 [1:39:29<03:35,  1.81s/it] 96%|█████████▌| 2968/3086 [1:39:31<03:38,  1.86s/it] 96%|█████████▌| 2969/3086 [1:39:32<03:28,  1.78s/it] 96%|█████████▌| 2970/3086 [1:39:35<03:44,  1.93s/it]                                                     {'loss': 0.2412, 'grad_norm': 0.070719413459301, 'learning_rate': 2.2553467271548927e-05, 'epoch': 0.96}
 96%|█████████▌| 2970/3086 [1:39:35<03:44,  1.93s/it] 96%|█████████▋| 2971/3086 [1:39:37<03:47,  1.98s/it] 96%|█████████▋| 2972/3086 [1:39:39<03:51,  2.03s/it] 96%|█████████▋| 2973/3086 [1:39:41<03:39,  1.94s/it] 96%|█████████▋| 2974/3086 [1:39:43<03:52,  2.08s/it] 96%|█████████▋| 2975/3086 [1:39:45<03:43,  2.01s/it] 96%|█████████▋| 2976/3086 [1:39:47<03:40,  2.00s/it] 96%|█████████▋| 2977/3086 [1:39:49<03:48,  2.10s/it] 97%|█████████▋| 2978/3086 [1:39:51<03:46,  2.10s/it] 97%|█████████▋| 2979/3086 [1:39:54<03:53,  2.18s/it] 97%|█████████▋| 2980/3086 [1:39:55<03:36,  2.04s/it]                                                     {'loss': 0.2359, 'grad_norm': 0.06702500581741333, 'learning_rate': 2.0609202851587814e-05, 'epoch': 0.97}
 97%|█████████▋| 2980/3086 [1:39:55<03:36,  2.04s/it] 97%|█████████▋| 2981/3086 [1:39:57<03:24,  1.94s/it] 97%|█████████▋| 2982/3086 [1:39:59<03:35,  2.07s/it] 97%|█████████▋| 2983/3086 [1:40:02<03:36,  2.10s/it] 97%|█████████▋| 2984/3086 [1:40:04<03:30,  2.07s/it] 97%|█████████▋| 2985/3086 [1:40:05<03:20,  1.99s/it] 97%|█████████▋| 2986/3086 [1:40:09<04:05,  2.46s/it] 97%|█████████▋| 2987/3086 [1:40:11<03:38,  2.21s/it] 97%|█████████▋| 2988/3086 [1:40:12<03:25,  2.10s/it] 97%|█████████▋| 2989/3086 [1:40:15<03:31,  2.18s/it] 97%|█████████▋| 2990/3086 [1:40:17<03:19,  2.08s/it]                                                     {'loss': 0.2427, 'grad_norm': 0.07860542088747025, 'learning_rate': 1.86649384316267e-05, 'epoch': 0.97}
 97%|█████████▋| 2990/3086 [1:40:17<03:19,  2.08s/it] 97%|█████████▋| 2991/3086 [1:40:18<03:04,  1.95s/it] 97%|█████████▋| 2992/3086 [1:40:20<03:00,  1.92s/it] 97%|█████████▋| 2993/3086 [1:40:22<03:06,  2.00s/it] 97%|█████████▋| 2994/3086 [1:40:25<03:18,  2.16s/it] 97%|█████████▋| 2995/3086 [1:40:27<03:22,  2.22s/it] 97%|█████████▋| 2996/3086 [1:40:29<03:17,  2.19s/it] 97%|█████████▋| 2997/3086 [1:40:31<03:11,  2.15s/it] 97%|█████████▋| 2998/3086 [1:40:33<03:06,  2.11s/it] 97%|█████████▋| 2999/3086 [1:40:35<02:58,  2.05s/it] 97%|█████████▋| 3000/3086 [1:40:37<02:56,  2.05s/it]                                                     {'loss': 0.2387, 'grad_norm': 0.07923340052366257, 'learning_rate': 1.6720674011665585e-05, 'epoch': 0.97}
 97%|█████████▋| 3000/3086 [1:40:37<02:56,  2.05s/it] 97%|█████████▋| 3001/3086 [1:40:39<02:52,  2.02s/it] 97%|█████████▋| 3002/3086 [1:40:42<03:01,  2.17s/it] 97%|█████████▋| 3003/3086 [1:40:44<02:59,  2.16s/it] 97%|█████████▋| 3004/3086 [1:40:46<02:55,  2.13s/it] 97%|█████████▋| 3005/3086 [1:40:48<02:50,  2.10s/it] 97%|█████████▋| 3006/3086 [1:40:50<02:36,  1.96s/it] 97%|█████████▋| 3007/3086 [1:40:51<02:27,  1.87s/it] 97%|█████████▋| 3008/3086 [1:40:54<02:45,  2.12s/it] 98%|█████████▊| 3009/3086 [1:40:56<02:40,  2.09s/it] 98%|█████████▊| 3010/3086 [1:40:59<02:51,  2.25s/it]                                                     {'loss': 0.2347, 'grad_norm': 0.06346406042575836, 'learning_rate': 1.477640959170447e-05, 'epoch': 0.98}
 98%|█████████▊| 3010/3086 [1:40:59<02:51,  2.25s/it] 98%|█████████▊| 3011/3086 [1:41:01<02:41,  2.15s/it] 98%|█████████▊| 3012/3086 [1:41:03<02:35,  2.10s/it] 98%|█████████▊| 3013/3086 [1:41:05<02:35,  2.13s/it] 98%|█████████▊| 3014/3086 [1:41:07<02:28,  2.06s/it] 98%|█████████▊| 3015/3086 [1:41:09<02:29,  2.11s/it] 98%|█████████▊| 3016/3086 [1:41:11<02:19,  2.00s/it] 98%|█████████▊| 3017/3086 [1:41:12<02:07,  1.85s/it] 98%|█████████▊| 3018/3086 [1:41:14<02:06,  1.87s/it] 98%|█████████▊| 3019/3086 [1:41:16<02:06,  1.89s/it] 98%|█████████▊| 3020/3086 [1:41:18<02:09,  1.97s/it]                                                     {'loss': 0.2364, 'grad_norm': 0.0721043050289154, 'learning_rate': 1.2832145171743355e-05, 'epoch': 0.98}
 98%|█████████▊| 3020/3086 [1:41:18<02:09,  1.97s/it] 98%|█████████▊| 3021/3086 [1:41:20<02:06,  1.94s/it] 98%|█████████▊| 3022/3086 [1:41:22<02:03,  1.94s/it] 98%|█████████▊| 3023/3086 [1:41:24<01:59,  1.89s/it] 98%|█████████▊| 3024/3086 [1:41:26<02:00,  1.94s/it] 98%|█████████▊| 3025/3086 [1:41:28<02:00,  1.97s/it] 98%|█████████▊| 3026/3086 [1:41:30<01:53,  1.89s/it] 98%|█████████▊| 3027/3086 [1:41:32<01:58,  2.02s/it] 98%|█████████▊| 3028/3086 [1:41:34<01:54,  1.97s/it] 98%|█████████▊| 3029/3086 [1:41:36<01:52,  1.97s/it] 98%|█████████▊| 3030/3086 [1:41:38<01:49,  1.95s/it]                                                     {'loss': 0.2373, 'grad_norm': 0.06748548150062561, 'learning_rate': 1.0887880751782242e-05, 'epoch': 0.98}
 98%|█████████▊| 3030/3086 [1:41:38<01:49,  1.95s/it] 98%|█████████▊| 3031/3086 [1:41:41<02:06,  2.29s/it] 98%|█████████▊| 3032/3086 [1:41:43<02:08,  2.38s/it] 98%|█████████▊| 3033/3086 [1:41:45<02:00,  2.27s/it] 98%|█████████▊| 3034/3086 [1:41:47<01:52,  2.16s/it] 98%|█████████▊| 3035/3086 [1:41:49<01:46,  2.09s/it] 98%|█████████▊| 3036/3086 [1:41:51<01:39,  1.99s/it] 98%|█████████▊| 3037/3086 [1:41:53<01:38,  2.00s/it] 98%|█████████▊| 3038/3086 [1:41:55<01:32,  1.93s/it] 98%|█████████▊| 3039/3086 [1:41:57<01:31,  1.94s/it] 99%|█████████▊| 3040/3086 [1:41:59<01:29,  1.94s/it]                                                     {'loss': 0.2404, 'grad_norm': 0.06823308765888214, 'learning_rate': 8.943616331821126e-06, 'epoch': 0.99}
 99%|█████████▊| 3040/3086 [1:41:59<01:29,  1.94s/it] 99%|█████████▊| 3041/3086 [1:42:00<01:23,  1.85s/it] 99%|█████████▊| 3042/3086 [1:42:02<01:22,  1.87s/it] 99%|█████████▊| 3043/3086 [1:42:04<01:16,  1.78s/it] 99%|█████████▊| 3044/3086 [1:42:06<01:26,  2.07s/it] 99%|█████████▊| 3045/3086 [1:42:08<01:20,  1.95s/it] 99%|█████████▊| 3046/3086 [1:42:11<01:25,  2.15s/it] 99%|█████████▊| 3047/3086 [1:42:13<01:21,  2.09s/it] 99%|█████████▉| 3048/3086 [1:42:14<01:12,  1.91s/it] 99%|█████████▉| 3049/3086 [1:42:16<01:07,  1.82s/it] 99%|█████████▉| 3050/3086 [1:42:18<01:07,  1.87s/it]                                                     {'loss': 0.2359, 'grad_norm': 0.06471405178308487, 'learning_rate': 6.999351911860012e-06, 'epoch': 0.99}
 99%|█████████▉| 3050/3086 [1:42:18<01:07,  1.87s/it] 99%|█████████▉| 3051/3086 [1:42:20<01:04,  1.85s/it] 99%|█████████▉| 3052/3086 [1:42:21<01:02,  1.84s/it] 99%|█████████▉| 3053/3086 [1:42:23<01:01,  1.88s/it] 99%|█████████▉| 3054/3086 [1:42:25<00:55,  1.73s/it] 99%|█████████▉| 3055/3086 [1:42:27<00:56,  1.81s/it] 99%|█████████▉| 3056/3086 [1:42:29<00:56,  1.87s/it] 99%|█████████▉| 3057/3086 [1:42:31<00:59,  2.04s/it] 99%|█████████▉| 3058/3086 [1:42:33<00:54,  1.96s/it] 99%|█████████▉| 3059/3086 [1:42:34<00:49,  1.82s/it] 99%|█████████▉| 3060/3086 [1:42:37<00:49,  1.92s/it]                                                     {'loss': 0.2354, 'grad_norm': 0.06643934547901154, 'learning_rate': 5.055087491898897e-06, 'epoch': 0.99}
 99%|█████████▉| 3060/3086 [1:42:37<00:49,  1.92s/it] 99%|█████████▉| 3061/3086 [1:42:38<00:45,  1.84s/it] 99%|█████████▉| 3062/3086 [1:42:40<00:45,  1.91s/it] 99%|█████████▉| 3063/3086 [1:42:42<00:40,  1.78s/it] 99%|█████████▉| 3064/3086 [1:42:43<00:38,  1.74s/it] 99%|█████████▉| 3065/3086 [1:42:46<00:40,  1.94s/it] 99%|█████████▉| 3066/3086 [1:42:48<00:40,  2.05s/it] 99%|█████████▉| 3067/3086 [1:42:50<00:36,  1.93s/it] 99%|█████████▉| 3068/3086 [1:42:52<00:35,  1.96s/it] 99%|█████████▉| 3069/3086 [1:42:53<00:31,  1.83s/it] 99%|█████████▉| 3070/3086 [1:42:55<00:30,  1.90s/it]                                                     {'loss': 0.2344, 'grad_norm': 0.06177617982029915, 'learning_rate': 3.1108230719377835e-06, 'epoch': 0.99}
 99%|█████████▉| 3070/3086 [1:42:55<00:30,  1.90s/it]100%|█████████▉| 3071/3086 [1:42:58<00:31,  2.08s/it]100%|█████████▉| 3072/3086 [1:43:00<00:29,  2.10s/it]100%|█████████▉| 3073/3086 [1:43:02<00:27,  2.09s/it]100%|█████████▉| 3074/3086 [1:43:05<00:26,  2.17s/it]100%|█████████▉| 3075/3086 [1:43:07<00:23,  2.16s/it]100%|█████████▉| 3076/3086 [1:43:09<00:20,  2.07s/it]100%|█████████▉| 3077/3086 [1:43:11<00:18,  2.08s/it]100%|█████████▉| 3078/3086 [1:43:13<00:17,  2.24s/it]100%|█████████▉| 3079/3086 [1:43:15<00:14,  2.14s/it]100%|█████████▉| 3080/3086 [1:43:17<00:11,  2.00s/it]                                                     {'loss': 0.2388, 'grad_norm': 0.07740677148103714, 'learning_rate': 1.1665586519766688e-06, 'epoch': 1.0}
100%|█████████▉| 3080/3086 [1:43:17<00:11,  2.00s/it]100%|█████████▉| 3081/3086 [1:43:19<00:09,  1.98s/it]100%|█████████▉| 3082/3086 [1:43:21<00:08,  2.11s/it]100%|█████████▉| 3083/3086 [1:43:23<00:06,  2.13s/it]100%|█████████▉| 3084/3086 [1:43:25<00:04,  2.07s/it]100%|█████████▉| 3085/3086 [1:43:27<00:02,  2.09s/it]100%|██████████| 3086/3086 [1:43:29<00:00,  1.95s/it][INFO|trainer.py:3503] 2024-11-12 11:48:48,264 >> Saving model checkpoint to /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5large-qwe2.5-math7b-metamath/checkpoint-3086
[INFO|configuration_utils.py:472] 2024-11-12 11:48:48,271 >> Configuration saved in /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5large-qwe2.5-math7b-metamath/checkpoint-3086/config.json
[INFO|tokenization_utils_base.py:2684] 2024-11-12 11:48:48,407 >> tokenizer config file saved in /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5large-qwe2.5-math7b-metamath/checkpoint-3086/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2024-11-12 11:48:48,409 >> Special tokens file saved in /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5large-qwe2.5-math7b-metamath/checkpoint-3086/special_tokens_map.json
[INFO|trainer.py:2394] 2024-11-12 11:48:48,823 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                     {'train_runtime': 6215.7591, 'train_samples_per_second': 63.548, 'train_steps_per_second': 0.496, 'train_loss': 0.4006362625012055, 'epoch': 1.0}
100%|██████████| 3086/3086 [1:43:35<00:00,  1.95s/it]100%|██████████| 3086/3086 [1:43:35<00:00,  2.01s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:3503] 2024-11-12 11:48:54,056 >> Saving model checkpoint to /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5large-qwe2.5-math7b-metamath
[INFO|configuration_utils.py:472] 2024-11-12 11:48:54,061 >> Configuration saved in /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5large-qwe2.5-math7b-metamath/config.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|tokenization_utils_base.py:2684] 2024-11-12 11:48:54,130 >> tokenizer config file saved in /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5large-qwe2.5-math7b-metamath/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2024-11-12 11:48:54,132 >> Special tokens file saved in /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5large-qwe2.5-math7b-metamath/special_tokens_map.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
***** train metrics *****
  epoch                    =        1.0
  total_flos               =        0GF
  train_loss               =     0.4006
  train_runtime            = 1:43:35.75
  train_samples            =     395000
  train_samples_per_second =     63.548
  train_steps_per_second   =      0.496
[INFO|modelcard.py:449] 2024-11-12 11:48:54,572 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'dataset': {'name': '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json', 'type': '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json'}}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[2024-11-12 11:48:55,742] [INFO] [launch.py:351:main] Process 250 exits successfully.
[2024-11-12 11:48:56,744] [INFO] [launch.py:351:main] Process 252 exits successfully.
[2024-11-12 11:48:56,744] [INFO] [launch.py:351:main] Process 246 exits successfully.
[2024-11-12 11:48:57,747] [INFO] [launch.py:351:main] Process 251 exits successfully.
[2024-11-12 11:48:57,747] [INFO] [launch.py:351:main] Process 247 exits successfully.
[2024-11-12 11:48:58,749] [INFO] [launch.py:351:main] Process 245 exits successfully.
[2024-11-12 11:48:59,750] [INFO] [launch.py:351:main] Process 249 exits successfully.
[2024-11-12 11:48:59,750] [INFO] [launch.py:351:main] Process 248 exits successfully.
