{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model_utils import ImplicitTransBridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/16/2024 17:45:00 - INFO - src.model_utils.modeling_itb - Small LM A model size: 494.032768 M\n",
      "10/16/2024 17:45:00 - INFO - src.model_utils.modeling_itb - mapping a layer size: 3.214848 M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ImplicitTransBridge(\n",
       "  (llm): Qwen2ForCausalLM(\n",
       "    (model): Qwen2Model(\n",
       "      (embed_tokens): Embedding(151936, 896)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Qwen2DecoderLayer(\n",
       "          (self_attn): Qwen2SdpaAttention(\n",
       "            (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "            (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "            (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "            (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "            (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm()\n",
       "          (post_attention_layernorm): Qwen2RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       "  )\n",
       "  (llm_embedding_layer): Embedding(151936, 896)\n",
       "  (slm_a): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "  )\n",
       "  (mapping_a): Mapping(\n",
       "    (mlp): MLP(\n",
       "      (linear1): Linear(in_features=896, out_features=1792, bias=True)\n",
       "      (linear2): Linear(in_features=1792, out_features=896, bias=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "llm_path = \"/home/nfs04/model/Qwen2.5/Qwen2.5-0.5B-Instruct\"\n",
    "llm_path_a = \"/home/nfs04/model/Qwen2.5/Qwen2.5-0.5B-Instruct\"\n",
    "llm_path_b = \"/home/nfs04/model/Qwen2.5/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_path)\n",
    "tokenizer.add_special_tokens({\"bos_token\": \"<bos>\"})\n",
    "model = ImplicitTransBridge(llm_path, max_gen_len=10, llm_bos_token_id=tokenizer.bos_token_id,\n",
    "                            llm_pad_token_id=tokenizer.pad_token_id, stage=0,\n",
    "                            slm_path_a=llm_path_a, slm_path_b=llm_path_b)\n",
    "model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx  = \"我喜欢你\"\n",
    "ids_xx = tokenizer([xx], return_tensors=\"pt\")\n",
    "prompt = \"Translate from Chinese to English: \"\n",
    "ids_prompt = tokenizer([prompt], return_tensors=\"pt\")\n",
    "en = \"I like you\"\n",
    "ids_en = tokenizer([en], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    }
   ],
   "source": [
    "res  = model(input_ids_prompt=ids_prompt['input_ids'], attention_mask_prompt=ids_prompt['attention_mask'],\n",
    "                      input_ids_xx=ids_xx['input_ids'], attention_mask_xx=ids_xx['attention_mask'],\n",
    "                      input_ids_en=ids_en['input_ids'], attention_mask_en=ids_en['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13.9015,  9.8175,  5.6632,  ..., -1.6009, -1.6007, -1.6008],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.logits[0][13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[84160]], 'attention_mask': [[1]]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['sss'], add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PreTrainedTokenizerBase.pad() got an unexpected keyword argument 'add_special_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: PreTrainedTokenizerBase.pad() got an unexpected keyword argument 'add_special_tokens'"
     ]
    }
   ],
   "source": [
    "tokenizer.pad({\"input_ids\": [[12,3]]}, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"./stage0_isen.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./is_en.en.txt\", \"w\") as f:\n",
    "    for d in data:\n",
    "        f.write(d['hyp'] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    40,   1075,    498, 151643, 151643, 151643, 151643, 151643],\n",
       "        [    45,  52091,    287,    374,    279,   6722,    315,   5616]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_en['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx  = [\"我喜欢你\", \"南京是中国的首都\"]\n",
    "ids_xx = tokenizer(xx, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "prompt = \"Translate from Chinese to English: \"\n",
    "ids_prompt = tokenizer([prompt] * 2, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "en = [\"I like you\", \"Nanjing is the capital of China\"]\n",
    "ids_en = tokenizer(en, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "res_a  = model(input_ids_prompt=ids_prompt['input_ids'], attention_mask_prompt=ids_prompt['attention_mask'],\n",
    "                      input_ids_xx=ids_xx['input_ids'], attention_mask_xx=ids_xx['attention_mask'],\n",
    "                      input_ids_en=ids_en['input_ids'], attention_mask_en=ids_en['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/nfs02/wangzj/dataset/opus-100-corpus/v1.0/supervised/en-is/opus.en-is-train.is\", \"r\") as f:\n",
    "    xx = []\n",
    "    for k in f.readlines():\n",
    "        xx.append(k.strip())\n",
    "with open(\"/home/nfs02/wangzj/dataset/opus-100-corpus/v1.0/supervised/en-is/opus.en-is-train.en\", \"r\") as f:\n",
    "    en = []\n",
    "    for k in f.readlines():\n",
    "        en.append(k.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/opus_en_is.jsonl\", \"w\") as f:\n",
    "    import json\n",
    "    for a, b in zip(xx, en):\n",
    "        f.write(json.dumps({\"src\": b, \"tgt\": a}, ensure_ascii=False) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
