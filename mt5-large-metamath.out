[2024-11-12 03:35:03,908] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
[2024-11-12 03:35:05,373] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-11-12 03:35:05,374] [INFO] [runner.py:568:main] cmd = /root/work/huangxin/envs/hs/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=9901 --enable_each_rank_log=None /root/work/huangxin/nanda/ImplicitTransBridge-master/train.py --deepspeed /root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json --dataset_name /root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json --preprocessing_num_workers 64 --llm_path /root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct --slm_path_a /root/work/huangxin/nanda/models/google/mt5-large --slm_path_b /root/work/huangxin/nanda/models/google/mt5-large --stage 0 --output_dir /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/xlmr-qwe2.5-math7b-metamath --do_train --max_seq_length 1024 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --learning_rate 2e-4 --num_train_epochs 1 --save_only_model --logging_steps 10 --save_steps 2000 --seed 42 --overwrite_output_dir --bf16
[2024-11-12 03:35:06,683] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[2024-11-12 03:35:08,097] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-11-12 03:35:08,097] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-11-12 03:35:08,097] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-11-12 03:35:08,097] [INFO] [launch.py:164:main] dist_world_size=8
[2024-11-12 03:35:08,097] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-11-12 03:35:08,110] [INFO] [launch.py:256:main] process 245 spawned with command: ['/root/work/huangxin/envs/hs/bin/python', '-u', '/root/work/huangxin/nanda/ImplicitTransBridge-master/train.py', '--local_rank=0', '--deepspeed', '/root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json', '--dataset_name', '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json', '--preprocessing_num_workers', '64', '--llm_path', '/root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct', '--slm_path_a', '/root/work/huangxin/nanda/models/google/mt5-large', '--slm_path_b', '/root/work/huangxin/nanda/models/google/mt5-large', '--stage', '0', '--output_dir', '/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/xlmr-qwe2.5-math7b-metamath', '--do_train', '--max_seq_length', '1024', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-4', '--num_train_epochs', '1', '--save_only_model', '--logging_steps', '10', '--save_steps', '2000', '--seed', '42', '--overwrite_output_dir', '--bf16']
[2024-11-12 03:35:08,120] [INFO] [launch.py:256:main] process 246 spawned with command: ['/root/work/huangxin/envs/hs/bin/python', '-u', '/root/work/huangxin/nanda/ImplicitTransBridge-master/train.py', '--local_rank=1', '--deepspeed', '/root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json', '--dataset_name', '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json', '--preprocessing_num_workers', '64', '--llm_path', '/root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct', '--slm_path_a', '/root/work/huangxin/nanda/models/google/mt5-large', '--slm_path_b', '/root/work/huangxin/nanda/models/google/mt5-large', '--stage', '0', '--output_dir', '/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/xlmr-qwe2.5-math7b-metamath', '--do_train', '--max_seq_length', '1024', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-4', '--num_train_epochs', '1', '--save_only_model', '--logging_steps', '10', '--save_steps', '2000', '--seed', '42', '--overwrite_output_dir', '--bf16']
[2024-11-12 03:35:08,130] [INFO] [launch.py:256:main] process 247 spawned with command: ['/root/work/huangxin/envs/hs/bin/python', '-u', '/root/work/huangxin/nanda/ImplicitTransBridge-master/train.py', '--local_rank=2', '--deepspeed', '/root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json', '--dataset_name', '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json', '--preprocessing_num_workers', '64', '--llm_path', '/root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct', '--slm_path_a', '/root/work/huangxin/nanda/models/google/mt5-large', '--slm_path_b', '/root/work/huangxin/nanda/models/google/mt5-large', '--stage', '0', '--output_dir', '/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/xlmr-qwe2.5-math7b-metamath', '--do_train', '--max_seq_length', '1024', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-4', '--num_train_epochs', '1', '--save_only_model', '--logging_steps', '10', '--save_steps', '2000', '--seed', '42', '--overwrite_output_dir', '--bf16']
[2024-11-12 03:35:08,140] [INFO] [launch.py:256:main] process 248 spawned with command: ['/root/work/huangxin/envs/hs/bin/python', '-u', '/root/work/huangxin/nanda/ImplicitTransBridge-master/train.py', '--local_rank=3', '--deepspeed', '/root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json', '--dataset_name', '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json', '--preprocessing_num_workers', '64', '--llm_path', '/root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct', '--slm_path_a', '/root/work/huangxin/nanda/models/google/mt5-large', '--slm_path_b', '/root/work/huangxin/nanda/models/google/mt5-large', '--stage', '0', '--output_dir', '/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/xlmr-qwe2.5-math7b-metamath', '--do_train', '--max_seq_length', '1024', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-4', '--num_train_epochs', '1', '--save_only_model', '--logging_steps', '10', '--save_steps', '2000', '--seed', '42', '--overwrite_output_dir', '--bf16']
[2024-11-12 03:35:08,148] [INFO] [launch.py:256:main] process 249 spawned with command: ['/root/work/huangxin/envs/hs/bin/python', '-u', '/root/work/huangxin/nanda/ImplicitTransBridge-master/train.py', '--local_rank=4', '--deepspeed', '/root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json', '--dataset_name', '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json', '--preprocessing_num_workers', '64', '--llm_path', '/root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct', '--slm_path_a', '/root/work/huangxin/nanda/models/google/mt5-large', '--slm_path_b', '/root/work/huangxin/nanda/models/google/mt5-large', '--stage', '0', '--output_dir', '/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/xlmr-qwe2.5-math7b-metamath', '--do_train', '--max_seq_length', '1024', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-4', '--num_train_epochs', '1', '--save_only_model', '--logging_steps', '10', '--save_steps', '2000', '--seed', '42', '--overwrite_output_dir', '--bf16']
[2024-11-12 03:35:08,157] [INFO] [launch.py:256:main] process 250 spawned with command: ['/root/work/huangxin/envs/hs/bin/python', '-u', '/root/work/huangxin/nanda/ImplicitTransBridge-master/train.py', '--local_rank=5', '--deepspeed', '/root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json', '--dataset_name', '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json', '--preprocessing_num_workers', '64', '--llm_path', '/root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct', '--slm_path_a', '/root/work/huangxin/nanda/models/google/mt5-large', '--slm_path_b', '/root/work/huangxin/nanda/models/google/mt5-large', '--stage', '0', '--output_dir', '/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/xlmr-qwe2.5-math7b-metamath', '--do_train', '--max_seq_length', '1024', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-4', '--num_train_epochs', '1', '--save_only_model', '--logging_steps', '10', '--save_steps', '2000', '--seed', '42', '--overwrite_output_dir', '--bf16']
[2024-11-12 03:35:08,164] [INFO] [launch.py:256:main] process 251 spawned with command: ['/root/work/huangxin/envs/hs/bin/python', '-u', '/root/work/huangxin/nanda/ImplicitTransBridge-master/train.py', '--local_rank=6', '--deepspeed', '/root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json', '--dataset_name', '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json', '--preprocessing_num_workers', '64', '--llm_path', '/root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct', '--slm_path_a', '/root/work/huangxin/nanda/models/google/mt5-large', '--slm_path_b', '/root/work/huangxin/nanda/models/google/mt5-large', '--stage', '0', '--output_dir', '/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/xlmr-qwe2.5-math7b-metamath', '--do_train', '--max_seq_length', '1024', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-4', '--num_train_epochs', '1', '--save_only_model', '--logging_steps', '10', '--save_steps', '2000', '--seed', '42', '--overwrite_output_dir', '--bf16']
[2024-11-12 03:35:08,173] [INFO] [launch.py:256:main] process 252 spawned with command: ['/root/work/huangxin/envs/hs/bin/python', '-u', '/root/work/huangxin/nanda/ImplicitTransBridge-master/train.py', '--local_rank=7', '--deepspeed', '/root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json', '--dataset_name', '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json', '--preprocessing_num_workers', '64', '--llm_path', '/root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct', '--slm_path_a', '/root/work/huangxin/nanda/models/google/mt5-large', '--slm_path_b', '/root/work/huangxin/nanda/models/google/mt5-large', '--stage', '0', '--output_dir', '/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/xlmr-qwe2.5-math7b-metamath', '--do_train', '--max_seq_length', '1024', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-4', '--num_train_epochs', '1', '--save_only_model', '--logging_steps', '10', '--save_steps', '2000', '--seed', '42', '--overwrite_output_dir', '--bf16']
[2024-11-12 03:35:11,734] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-12 03:35:11,744] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-12 03:35:11,755] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-12 03:35:11,832] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-12 03:35:11,839] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-12 03:35:11,848] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-12 03:35:11,865] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-12 03:35:11,898] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2024-11-12 03:35:12,335] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-12 03:35:12,340] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[2024-11-12 03:35:12,367] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[2024-11-12 03:35:12,447] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-12 03:35:12,481] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-12 03:35:12,507] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-12 03:35:12,519] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-12 03:35:12,520] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-11-12 03:35:12,524] [INFO] [comm.py:637:init_distributed] cdb=None
11/12/2024 03:35:13 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
11/12/2024 03:35:13 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=no,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0002,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/xlmr-qwe2.5-math7b-metamath/runs/Nov12_03-35-11_dt-3f438f9c1ce34127b03dfa1fd050d2d6-master-9ea38ad98224-0,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/xlmr-qwe2.5-math7b-metamath,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/xlmr-qwe2.5-math7b-metamath,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=2000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|tokenization_utils_base.py:2267] 2024-11-12 03:35:13,792 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2267] 2024-11-12 03:35:13,792 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2267] 2024-11-12 03:35:13,792 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2267] 2024-11-12 03:35:13,792 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2267] 2024-11-12 03:35:13,792 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2267] 2024-11-12 03:35:13,792 >> loading file tokenizer_config.json
11/12/2024 03:35:13 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, 16-bits training: False
11/12/2024 03:35:13 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, 16-bits training: False
11/12/2024 03:35:13 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1, distributed training: True, 16-bits training: False
11/12/2024 03:35:13 - WARNING - __main__ - Process rank: 7, device: cuda:7, n_gpu: 1, distributed training: True, 16-bits training: False
11/12/2024 03:35:13 - WARNING - __main__ - Process rank: 6, device: cuda:6, n_gpu: 1, distributed training: True, 16-bits training: False
11/12/2024 03:35:13 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
11/12/2024 03:35:13 - WARNING - __main__ - Process rank: 5, device: cuda:5, n_gpu: 1, distributed training: True, 16-bits training: False
[INFO|tokenization_utils_base.py:2513] 2024-11-12 03:35:14,006 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:731] 2024-11-12 03:35:14,009 >> loading configuration file /root/work/huangxin/nanda/models/google/mt5-large/config.json
[INFO|configuration_utils.py:800] 2024-11-12 03:35:14,015 >> Model config MT5Config {
  "_name_or_path": "/root/work/huangxin/nanda/models/google/mt5-large",
  "architectures": [
    "MT5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 2816,
  "d_kv": 64,
  "d_model": 1024,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "mt5",
  "num_decoder_layers": 24,
  "num_heads": 16,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "tokenizer_class": "T5Tokenizer",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 250112
}

[INFO|tokenization_utils_base.py:2267] 2024-11-12 03:35:14,016 >> loading file spiece.model
[INFO|tokenization_utils_base.py:2267] 2024-11-12 03:35:14,016 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2267] 2024-11-12 03:35:14,016 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2267] 2024-11-12 03:35:14,016 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2267] 2024-11-12 03:35:14,016 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:731] 2024-11-12 03:35:14,017 >> loading configuration file /root/work/huangxin/nanda/models/google/mt5-large/config.json
[INFO|configuration_utils.py:800] 2024-11-12 03:35:14,017 >> Model config MT5Config {
  "_name_or_path": "/root/work/huangxin/nanda/models/google/mt5-large",
  "architectures": [
    "MT5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 2816,
  "d_kv": 64,
  "d_model": 1024,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "mt5",
  "num_decoder_layers": 24,
  "num_heads": 16,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "tokenizer_class": "T5Tokenizer",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 250112
}

[WARNING|logging.py:328] 2024-11-12 03:35:14,403 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[WARNING|logging.py:328] 2024-11-12 03:35:14,409 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[WARNING|logging.py:328] 2024-11-12 03:35:14,445 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[WARNING|logging.py:328] 2024-11-12 03:35:14,460 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[WARNING|logging.py:328] 2024-11-12 03:35:14,468 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[WARNING|logging.py:328] 2024-11-12 03:35:14,472 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[WARNING|logging.py:328] 2024-11-12 03:35:14,477 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[WARNING|logging.py:328] 2024-11-12 03:35:14,480 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
/root/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
[INFO|configuration_utils.py:731] 2024-11-12 03:35:15,460 >> loading configuration file /root/work/huangxin/nanda/models/google/mt5-large/config.json
[INFO|configuration_utils.py:800] 2024-11-12 03:35:15,461 >> Model config MT5Config {
  "_name_or_path": "/root/work/huangxin/nanda/models/google/mt5-large",
  "architectures": [
    "MT5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 2816,
  "d_kv": 64,
  "d_model": 1024,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "mt5",
  "num_decoder_layers": 24,
  "num_heads": 16,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "tokenizer_class": "T5Tokenizer",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 250112
}

/root/.local/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][INFO|configuration_utils.py:731] 2024-11-12 03:35:16,013 >> loading configuration file /root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct/config.json
[INFO|configuration_utils.py:800] 2024-11-12 03:35:16,014 >> Model config Qwen2Config {
  "_name_or_path": "/root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 4096,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_theta": 10000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|modeling_utils.py:3675] 2024-11-12 03:35:16,016 >> loading weights file /root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct/model.safetensors.index.json
[INFO|configuration_utils.py:1038] 2024-11-12 03:35:16,018 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.49s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.31s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.60s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.73s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.63s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.22s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.05s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.09s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.30s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.43s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.86s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.73s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.08s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.28s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.44s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.86s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.03s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.33s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.94s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.17s/it]
[INFO|modeling_utils.py:4507] 2024-11-12 03:35:30,074 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4515] 2024-11-12 03:35:30,074 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:991] 2024-11-12 03:35:30,077 >> loading configuration file /root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct/generation_config.json
[INFO|configuration_utils.py:1038] 2024-11-12 03:35:30,077 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643
}

[INFO|configuration_utils.py:731] 2024-11-12 03:35:30,080 >> loading configuration file /root/work/huangxin/nanda/models/google/mt5-large/config.json
[INFO|configuration_utils.py:800] 2024-11-12 03:35:30,080 >> Model config MT5Config {
  "_name_or_path": "/root/work/huangxin/nanda/models/google/mt5-large",
  "architectures": [
    "MT5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 2816,
  "d_kv": 64,
  "d_model": 1024,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "mt5",
  "num_decoder_layers": 24,
  "num_heads": 16,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "tokenizer_class": "T5Tokenizer",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 250112
}

[INFO|modeling_utils.py:3675] 2024-11-12 03:35:30,123 >> loading weights file /root/work/huangxin/nanda/models/google/mt5-large/pytorch_model.bin
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.83s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.14s/it]
[INFO|modeling_utils.py:4497] 2024-11-12 03:35:32,286 >> Some weights of the model checkpoint at /root/work/huangxin/nanda/models/google/mt5-large were not used when initializing MT5Model: ['lm_head.weight']
- This IS expected if you are initializing MT5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MT5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:4515] 2024-11-12 03:35:32,286 >> All the weights of MT5Model were initialized from the model checkpoint at /root/work/huangxin/nanda/models/google/mt5-large.
If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5Model for predictions without further training.
11/12/2024 03:35:32 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
11/12/2024 03:35:32 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
11/12/2024 03:35:32 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
11/12/2024 03:35:32 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
11/12/2024 03:35:32 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
11/12/2024 03:35:32 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
Loading checkpoint shards:  25%|██▌       | 1/4 [00:10<00:32, 10.96s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:11<00:09,  4.90s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:02,  2.96s/it]11/12/2024 03:35:34 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
11/12/2024 03:35:34 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
Generating train split: 0 examples [00:00, ? examples/s]11/12/2024 03:35:34 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
11/12/2024 03:35:34 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
Using custom data configuration default-5110b371feb00e53
11/12/2024 03:35:34 - INFO - datasets.builder - Using custom data configuration default-5110b371feb00e53
Loading Dataset Infos from /root/.local/lib/python3.9/site-packages/datasets/packaged_modules/json
11/12/2024 03:35:34 - INFO - datasets.info - Loading Dataset Infos from /root/.local/lib/python3.9/site-packages/datasets/packaged_modules/json
Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.20s/it]
11/12/2024 03:35:35 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
11/12/2024 03:35:35 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
11/12/2024 03:35:37 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
11/12/2024 03:35:37 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
11/12/2024 03:35:39 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
11/12/2024 03:35:39 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  5.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.26s/it]
11/12/2024 03:35:41 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
11/12/2024 03:35:41 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
11/12/2024 03:35:43 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
11/12/2024 03:35:43 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
Generating train split: 395000 examples [00:10, 36600.17 examples/s]Generating train split: 395000 examples [00:10, 36482.06 examples/s]
Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5110b371feb00e53/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
11/12/2024 03:35:45 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5110b371feb00e53/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5110b371feb00e53/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
11/12/2024 03:35:45 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5110b371feb00e53/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
Map:   0%|          | 0/395000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-5110b371feb00e53/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d9c32650a4c419a6.arrow
11/12/2024 03:35:45 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-5110b371feb00e53/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-d9c32650a4c419a6.arrow
Map:   0%|          | 1000/395000 [00:00<00:49, 7894.71 examples/s]Map:   1%|          | 3000/395000 [00:00<00:41, 9402.26 examples/s]Map:   1%|▏         | 5000/395000 [00:00<00:39, 9852.58 examples/s]Map:   2%|▏         | 7000/395000 [00:00<00:38, 10052.57 examples/s]Map:   2%|▏         | 9000/395000 [00:00<00:38, 10095.22 examples/s]Map:   3%|▎         | 11000/395000 [00:01<00:39, 9601.72 examples/s]Map:   3%|▎         | 13000/395000 [00:01<00:39, 9747.63 examples/s]Map:   4%|▍         | 15000/395000 [00:01<00:38, 9980.36 examples/s]Map:   4%|▍         | 16000/395000 [00:01<00:37, 9977.17 examples/s]Map:   5%|▍         | 18000/395000 [00:01<00:37, 10019.61 examples/s]Map:   5%|▌         | 20000/395000 [00:02<00:36, 10161.63 examples/s]Map:   6%|▌         | 22000/395000 [00:02<00:36, 10236.67 examples/s]Map:   6%|▌         | 24000/395000 [00:02<00:43, 8560.68 examples/s] Map:   7%|▋         | 26000/395000 [00:02<00:40, 9061.91 examples/s]Map:   7%|▋         | 28000/395000 [00:02<00:38, 9491.94 examples/s]Map:   8%|▊         | 30000/395000 [00:03<00:37, 9845.41 examples/s]Map:   8%|▊         | 32000/395000 [00:03<00:36, 10029.61 examples/s]Map:   9%|▊         | 34000/395000 [00:03<00:35, 10226.75 examples/s]Map:   9%|▉         | 36000/395000 [00:03<00:34, 10336.82 examples/s]Map:  10%|▉         | 38000/395000 [00:03<00:34, 10452.25 examples/s]Map:  10%|█         | 40000/395000 [00:04<00:34, 10165.99 examples/s]Map:  11%|█         | 42000/395000 [00:04<00:35, 9856.64 examples/s] Map:  11%|█         | 44000/395000 [00:04<00:34, 10053.85 examples/s]Map:  12%|█▏        | 46000/395000 [00:04<00:34, 10135.01 examples/s]Map:  12%|█▏        | 48000/395000 [00:04<00:34, 10156.00 examples/s]Map:  13%|█▎        | 50000/395000 [00:05<00:35, 9850.79 examples/s] Map:  13%|█▎        | 51000/395000 [00:05<00:34, 9866.95 examples/s]Map:  13%|█▎        | 52000/395000 [00:05<00:45, 7512.74 examples/s]Map:  14%|█▎        | 54000/395000 [00:05<00:40, 8432.34 examples/s]Map:  14%|█▍        | 56000/395000 [00:05<00:37, 9028.10 examples/s]Map:  15%|█▍        | 58000/395000 [00:05<00:35, 9530.40 examples/s]Map:  15%|█▌        | 60000/395000 [00:06<00:33, 9881.29 examples/s]Map:  16%|█▌        | 62000/395000 [00:06<00:32, 10177.33 examples/s]Map:  16%|█▌        | 64000/395000 [00:06<00:32, 10237.22 examples/s]Map:  17%|█▋        | 66000/395000 [00:06<00:33, 9848.19 examples/s] Map:  17%|█▋        | 68000/395000 [00:06<00:32, 10057.65 examples/s]Map:  18%|█▊        | 70000/395000 [00:07<00:31, 10281.51 examples/s]Map:  18%|█▊        | 72000/395000 [00:07<00:32, 10072.26 examples/s]Map:  19%|█▊        | 74000/395000 [00:07<00:31, 10088.48 examples/s]Map:  19%|█▉        | 76000/395000 [00:07<00:31, 10214.79 examples/s]Map:  20%|█▉        | 78000/395000 [00:07<00:30, 10350.46 examples/s]Map:  20%|██        | 80000/395000 [00:08<00:36, 8641.40 examples/s] Map:  21%|██        | 82000/395000 [00:08<00:34, 9142.11 examples/s]Map:  21%|██▏       | 84000/395000 [00:08<00:32, 9531.19 examples/s]Map:  22%|██▏       | 86000/395000 [00:08<00:31, 9840.54 examples/s]Map:  22%|██▏       | 88000/395000 [00:09<00:30, 10084.69 examples/s]Map:  23%|██▎       | 90000/395000 [00:09<00:29, 10294.55 examples/s]Map:  23%|██▎       | 92000/395000 [00:09<00:29, 10389.85 examples/s]Map:  24%|██▍       | 94000/395000 [00:09<00:29, 10343.06 examples/s]Map:  24%|██▍       | 96000/395000 [00:09<00:28, 10462.43 examples/s]Map:  25%|██▍       | 98000/395000 [00:09<00:28, 10490.71 examples/s]Map:  25%|██▌       | 100000/395000 [00:10<00:28, 10407.92 examples/s]Map:  26%|██▌       | 102000/395000 [00:10<00:28, 10444.48 examples/s]Map:  26%|██▋       | 104000/395000 [00:10<00:33, 8772.43 examples/s] Map:  27%|██▋       | 106000/395000 [00:10<00:31, 9132.83 examples/s]Map:  27%|██▋       | 107000/395000 [00:10<00:31, 9120.49 examples/s]Map:  27%|██▋       | 108000/395000 [00:11<00:31, 9145.17 examples/s]Map:  28%|██▊       | 109000/395000 [00:11<00:31, 9172.59 examples/s]Map:  28%|██▊       | 110000/395000 [00:11<00:30, 9243.04 examples/s]Map:  28%|██▊       | 111000/395000 [00:11<00:30, 9274.84 examples/s]Map:  28%|██▊       | 112000/395000 [00:11<00:30, 9394.99 examples/s]Map:  29%|██▉       | 114000/395000 [00:11<00:29, 9632.14 examples/s]Map:  29%|██▉       | 116000/395000 [00:11<00:28, 9884.85 examples/s]Map:  30%|██▉       | 118000/395000 [00:12<00:27, 10202.58 examples/s]Map:  30%|███       | 120000/395000 [00:12<00:26, 10440.00 examples/s]Map:  31%|███       | 122000/395000 [00:12<00:25, 10546.65 examples/s]Map:  31%|███▏      | 124000/395000 [00:12<00:25, 10645.68 examples/s]Map:  32%|███▏      | 126000/395000 [00:12<00:32, 8389.07 examples/s] Map:  32%|███▏      | 128000/395000 [00:13<00:29, 8992.23 examples/s]Map:  33%|███▎      | 130000/395000 [00:13<00:27, 9481.74 examples/s]Map:  33%|███▎      | 132000/395000 [00:13<00:26, 10000.56 examples/s]Map:  34%|███▍      | 134000/395000 [00:13<00:25, 10304.94 examples/s]Map:  34%|███▍      | 136000/395000 [00:13<00:24, 10566.59 examples/s]Map:  35%|███▍      | 138000/395000 [00:14<00:23, 10785.38 examples/s]Map:  35%|███▌      | 140000/395000 [00:14<00:23, 10921.39 examples/s]Map:  36%|███▌      | 142000/395000 [00:14<00:22, 11042.72 examples/s]Map:  36%|███▋      | 144000/395000 [00:14<00:22, 11146.54 examples/s]Map:  37%|███▋      | 146000/395000 [00:14<00:22, 11109.85 examples/s]Map:  37%|███▋      | 148000/395000 [00:15<00:27, 9035.80 examples/s] Map:  38%|███▊      | 150000/395000 [00:15<00:25, 9532.55 examples/s]Map:  38%|███▊      | 152000/395000 [00:15<00:24, 9879.85 examples/s]Map:  39%|███▉      | 154000/395000 [00:15<00:23, 10078.02 examples/s]Map:  39%|███▉      | 156000/395000 [00:15<00:23, 10148.40 examples/s]Map:  40%|████      | 158000/395000 [00:16<00:23, 10162.97 examples/s]Map:  41%|████      | 160000/395000 [00:16<00:23, 10140.01 examples/s]Map:  41%|████      | 162000/395000 [00:16<00:22, 10476.51 examples/s]Map:  42%|████▏     | 164000/395000 [00:16<00:21, 10716.87 examples/s]Map:  42%|████▏     | 166000/395000 [00:16<00:21, 10841.10 examples/s]Map:  43%|████▎     | 168000/395000 [00:17<00:24, 9115.51 examples/s] Map:  43%|████▎     | 170000/395000 [00:17<00:23, 9680.29 examples/s]Map:  44%|████▎     | 172000/395000 [00:17<00:23, 9681.80 examples/s]Map:  44%|████▍     | 174000/395000 [00:17<00:21, 10148.92 examples/s]Map:  45%|████▍     | 176000/395000 [00:17<00:21, 10057.33 examples/s]Map:  45%|████▌     | 178000/395000 [00:17<00:20, 10417.07 examples/s]Map:  46%|████▌     | 180000/395000 [00:18<00:20, 10586.42 examples/s]Map:  46%|████▌     | 182000/395000 [00:18<00:20, 10309.59 examples/s]Map:  47%|████▋     | 184000/395000 [00:18<00:20, 10079.91 examples/s]Map:  47%|████▋     | 186000/395000 [00:18<00:20, 10105.39 examples/s]Map:  48%|████▊     | 188000/395000 [00:19<00:23, 8693.47 examples/s] Map:  48%|████▊     | 190000/395000 [00:19<00:22, 9216.09 examples/s]Map:  49%|████▊     | 192000/395000 [00:19<00:20, 9788.94 examples/s]Map:  49%|████▉     | 194000/395000 [00:19<00:19, 10060.59 examples/s]Map:  50%|████▉     | 196000/395000 [00:19<00:19, 10433.00 examples/s]Map:  50%|█████     | 198000/395000 [00:19<00:18, 10711.83 examples/s]Map:  51%|█████     | 200000/395000 [00:20<00:17, 10898.64 examples/s]Map:  51%|█████     | 202000/395000 [00:20<00:17, 11025.95 examples/s]Map:  52%|█████▏    | 204000/395000 [00:20<00:17, 11064.31 examples/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:50<00:00, 18.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:50<00:00, 12.65s/it]
Map:  52%|█████▏    | 206000/395000 [00:20<00:17, 11065.45 examples/s]Map:  53%|█████▎    | 208000/395000 [00:21<00:21, 8719.76 examples/s] Map:  53%|█████▎    | 210000/395000 [00:21<00:19, 9294.27 examples/s]Map:  54%|█████▎    | 212000/395000 [00:21<00:18, 9836.12 examples/s]Map:  54%|█████▍    | 214000/395000 [00:21<00:17, 10265.40 examples/s]Map:  55%|█████▍    | 216000/395000 [00:21<00:16, 10543.94 examples/s]Map:  55%|█████▌    | 218000/395000 [00:21<00:16, 10742.63 examples/s]Map:  56%|█████▌    | 220000/395000 [00:22<00:16, 10901.39 examples/s]Map:  56%|█████▌    | 222000/395000 [00:22<00:15, 11036.30 examples/s]Map:  57%|█████▋    | 224000/395000 [00:22<00:15, 11167.16 examples/s]Map:  57%|█████▋    | 226000/395000 [00:22<00:14, 11286.77 examples/s]Map:  58%|█████▊    | 228000/395000 [00:22<00:17, 9287.50 examples/s] Map:  58%|█████▊    | 230000/395000 [00:23<00:16, 9797.37 examples/s]11/12/2024 03:36:09 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
11/12/2024 03:36:09 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
Map:  59%|█████▊    | 232000/395000 [00:23<00:16, 10128.31 examples/s]Map:  59%|█████▉    | 234000/395000 [00:23<00:15, 10387.85 examples/s]Map:  60%|█████▉    | 236000/395000 [00:23<00:15, 10542.40 examples/s]Map:  60%|██████    | 238000/395000 [00:23<00:14, 10693.87 examples/s]Map:  61%|██████    | 240000/395000 [00:24<00:14, 10711.86 examples/s]Map:  61%|██████▏   | 242000/395000 [00:24<00:14, 10815.63 examples/s]Map:  62%|██████▏   | 244000/395000 [00:24<00:13, 10884.36 examples/s]Map:  62%|██████▏   | 246000/395000 [00:24<00:16, 8998.81 examples/s] Map:  63%|██████▎   | 248000/395000 [00:24<00:15, 9502.07 examples/s]Map:  63%|██████▎   | 250000/395000 [00:25<00:15, 9589.61 examples/s]Map:  64%|██████▎   | 251000/395000 [00:25<00:14, 9655.98 examples/s]Map:  64%|██████▍   | 253000/395000 [00:25<00:14, 9576.90 examples/s]Map:  64%|██████▍   | 254000/395000 [00:25<00:14, 9617.21 examples/s]11/12/2024 03:36:11 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
11/12/2024 03:36:11 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
Map:  65%|██████▍   | 256000/395000 [00:25<00:13, 10087.89 examples/s]Map:  65%|██████▌   | 258000/395000 [00:25<00:13, 10502.64 examples/s]Map:  66%|██████▌   | 260000/395000 [00:26<00:12, 10741.43 examples/s]Map:  66%|██████▋   | 262000/395000 [00:26<00:12, 10927.89 examples/s]Map:  67%|██████▋   | 264000/395000 [00:26<00:11, 11082.07 examples/s]Map:  67%|██████▋   | 266000/395000 [00:26<00:14, 9168.58 examples/s] Map:  68%|██████▊   | 268000/395000 [00:26<00:12, 9774.39 examples/s]Map:  68%|██████▊   | 270000/395000 [00:27<00:12, 10104.98 examples/s]Map:  69%|██████▉   | 272000/395000 [00:27<00:12, 10068.42 examples/s]Map:  69%|██████▉   | 274000/395000 [00:27<00:11, 10333.74 examples/s]Map:  70%|██████▉   | 276000/395000 [00:27<00:11, 10594.50 examples/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:51<00:59, 29.76s/it]Map:  70%|███████   | 278000/395000 [00:27<00:11, 10582.05 examples/s]Map:  71%|███████   | 280000/395000 [00:28<00:11, 10299.18 examples/s]Map:  71%|███████▏  | 282000/395000 [00:28<00:10, 10532.53 examples/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:52<00:16, 16.45s/it]Map:  72%|███████▏  | 284000/395000 [00:28<00:10, 10589.06 examples/s]Map:  72%|███████▏  | 286000/395000 [00:28<00:12, 8633.32 examples/s] Loading checkpoint shards: 100%|██████████| 4/4 [00:52<00:00, 10.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:52<00:00, 13.18s/it]
Map:  73%|███████▎  | 288000/395000 [00:28<00:11, 9262.62 examples/s]Map:  73%|███████▎  | 290000/395000 [00:29<00:10, 9550.39 examples/s]Map:  74%|███████▍  | 292000/395000 [00:29<00:10, 9935.10 examples/s]Map:  74%|███████▍  | 294000/395000 [00:29<00:09, 10386.19 examples/s]Map:  75%|███████▍  | 296000/395000 [00:29<00:09, 10262.74 examples/s]Map:  75%|███████▌  | 298000/395000 [00:29<00:09, 10384.67 examples/s]Map:  76%|███████▌  | 300000/395000 [00:29<00:08, 10717.56 examples/s]Map:  76%|███████▋  | 302000/395000 [00:30<00:08, 10938.23 examples/s]Map:  77%|███████▋  | 304000/395000 [00:30<00:08, 11115.55 examples/s]Map:  77%|███████▋  | 306000/395000 [00:30<00:09, 9301.73 examples/s] Map:  78%|███████▊  | 308000/395000 [00:30<00:08, 9829.74 examples/s]Map:  78%|███████▊  | 310000/395000 [00:30<00:08, 10246.50 examples/s]Map:  79%|███████▉  | 312000/395000 [00:31<00:07, 10544.64 examples/s]11/12/2024 03:36:17 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
11/12/2024 03:36:17 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
Map:  79%|███████▉  | 314000/395000 [00:31<00:07, 10803.22 examples/s]Map:  80%|████████  | 316000/395000 [00:31<00:07, 10980.80 examples/s]Map:  81%|████████  | 318000/395000 [00:31<00:06, 11083.06 examples/s]Map:  81%|████████  | 320000/395000 [00:31<00:06, 11147.25 examples/s]Map:  82%|████████▏ | 322000/395000 [00:32<00:06, 11289.23 examples/s]Map:  82%|████████▏ | 324000/395000 [00:32<00:07, 9382.37 examples/s] Map:  83%|████████▎ | 326000/395000 [00:32<00:06, 9887.12 examples/s]Map:  83%|████████▎ | 328000/395000 [00:32<00:06, 10277.81 examples/s]Map:  84%|████████▎ | 330000/395000 [00:32<00:06, 10618.31 examples/s]Map:  84%|████████▍ | 332000/395000 [00:33<00:05, 10746.24 examples/s]Map:  85%|████████▍ | 334000/395000 [00:33<00:05, 10944.09 examples/s]11/12/2024 03:36:19 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
11/12/2024 03:36:19 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
Map:  85%|████████▌ | 336000/395000 [00:33<00:05, 11063.09 examples/s]Map:  86%|████████▌ | 338000/395000 [00:33<00:05, 11191.06 examples/s]Map:  86%|████████▌ | 340000/395000 [00:33<00:04, 11242.86 examples/s]Map:  87%|████████▋ | 342000/395000 [00:33<00:04, 11326.87 examples/s]Map:  87%|████████▋ | 344000/395000 [00:34<00:05, 9380.63 examples/s] Map:  88%|████████▊ | 346000/395000 [00:34<00:04, 9908.97 examples/s]Map:  88%|████████▊ | 348000/395000 [00:34<00:04, 10358.83 examples/s]Map:  89%|████████▊ | 350000/395000 [00:34<00:04, 10481.53 examples/s]Map:  89%|████████▉ | 352000/395000 [00:34<00:04, 10731.37 examples/s]Map:  90%|████████▉ | 354000/395000 [00:35<00:03, 10991.62 examples/s]Map:  90%|█████████ | 356000/395000 [00:35<00:03, 11117.00 examples/s]Map:  91%|█████████ | 358000/395000 [00:35<00:03, 11181.65 examples/s]Map:  91%|█████████ | 360000/395000 [00:35<00:03, 11312.30 examples/s]Map:  92%|█████████▏| 362000/395000 [00:35<00:02, 11320.17 examples/s]Map:  92%|█████████▏| 364000/395000 [00:36<00:03, 9384.79 examples/s] Map:  93%|█████████▎| 366000/395000 [00:36<00:02, 9865.61 examples/s]Map:  93%|█████████▎| 368000/395000 [00:36<00:02, 10271.71 examples/s]Map:  94%|█████████▎| 370000/395000 [00:36<00:02, 10570.92 examples/s]Map:  94%|█████████▍| 372000/395000 [00:36<00:02, 10761.63 examples/s]Map:  95%|█████████▍| 374000/395000 [00:36<00:01, 10869.24 examples/s]Map:  95%|█████████▌| 376000/395000 [00:37<00:01, 11056.36 examples/s]Map:  96%|█████████▌| 378000/395000 [00:37<00:01, 11172.28 examples/s]Map:  96%|█████████▌| 380000/395000 [00:37<00:01, 11219.46 examples/s]Map:  97%|█████████▋| 382000/395000 [00:37<00:01, 11266.74 examples/s]Map:  97%|█████████▋| 384000/395000 [00:37<00:01, 9393.53 examples/s] Map:  98%|█████████▊| 386000/395000 [00:38<00:00, 9878.88 examples/s]Map:  98%|█████████▊| 388000/395000 [00:38<00:00, 10295.64 examples/s]Map:  99%|█████████▊| 390000/395000 [00:38<00:00, 10614.16 examples/s]Map:  99%|█████████▉| 392000/395000 [00:38<00:00, 10843.62 examples/s]Map: 100%|█████████▉| 394000/395000 [00:38<00:00, 11003.41 examples/s]Map: 100%|██████████| 395000/395000 [00:38<00:00, 10140.17 examples/s]
Loading checkpoint shards: 100%|██████████| 4/4 [01:29<00:00, 33.96s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:29<00:00, 22.47s/it]
11/12/2024 03:36:53 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
11/12/2024 03:36:53 - INFO - src.model_utils.modeling_itb - Small LM A model size: 973.466624 M
11/12/2024 03:36:55 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
11/12/2024 03:36:55 - INFO - src.model_utils.modeling_itb - mapping a layer size: 9.4464 M
11/12/2024 03:36:58 - INFO - __main__ - Dataset({
    features: ['query', 'response', 'type', 'original_question', 'prompt', 'prefix'],
    num_rows: 395000
})
11/12/2024 03:36:58 - INFO - __main__ - ImplicitTransBridge(
  (llm): Qwen2ForCausalLM(
    (model): Qwen2Model(
      (embed_tokens): Embedding(152064, 3584)
      (layers): ModuleList(
        (0-27): 28 x Qwen2DecoderLayer(
          (self_attn): Qwen2SdpaAttention(
            (q_proj): Linear(in_features=3584, out_features=3584, bias=True)
            (k_proj): Linear(in_features=3584, out_features=512, bias=True)
            (v_proj): Linear(in_features=3584, out_features=512, bias=True)
            (o_proj): Linear(in_features=3584, out_features=3584, bias=False)
            (rotary_emb): Qwen2RotaryEmbedding()
          )
          (mlp): Qwen2MLP(
            (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)
            (up_proj): Linear(in_features=3584, out_features=18944, bias=False)
            (down_proj): Linear(in_features=18944, out_features=3584, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
          (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        )
      )
      (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    )
    (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
  )
  (llm_embedding_layer): Embedding(152064, 3584)
  (mt_model): MT5Model(
    (shared): Embedding(250112, 1024)
    (encoder): MT5Stack(
      (embed_tokens): Embedding(250112, 1024)
      (block): ModuleList(
        (0): MT5Block(
          (layer): ModuleList(
            (0): MT5LayerSelfAttention(
              (SelfAttention): MT5Attention(
                (q): Linear(in_features=1024, out_features=1024, bias=False)
                (k): Linear(in_features=1024, out_features=1024, bias=False)
                (v): Linear(in_features=1024, out_features=1024, bias=False)
                (o): Linear(in_features=1024, out_features=1024, bias=False)
                (relative_attention_bias): Embedding(32, 16)
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): MT5LayerFF(
              (DenseReluDense): MT5DenseGatedActDense(
                (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
                (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
                (wo): Linear(in_features=2816, out_features=1024, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (act): NewGELUActivation()
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1-23): 23 x MT5Block(
          (layer): ModuleList(
            (0): MT5LayerSelfAttention(
              (SelfAttention): MT5Attention(
                (q): Linear(in_features=1024, out_features=1024, bias=False)
                (k): Linear(in_features=1024, out_features=1024, bias=False)
                (v): Linear(in_features=1024, out_features=1024, bias=False)
                (o): Linear(in_features=1024, out_features=1024, bias=False)
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): MT5LayerFF(
              (DenseReluDense): MT5DenseGatedActDense(
                (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
                (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
                (wo): Linear(in_features=2816, out_features=1024, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (act): NewGELUActivation()
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (final_layer_norm): MT5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (decoder): MT5Stack(
      (embed_tokens): Embedding(250112, 1024)
      (block): ModuleList(
        (0): MT5Block(
          (layer): ModuleList(
            (0): MT5LayerSelfAttention(
              (SelfAttention): MT5Attention(
                (q): Linear(in_features=1024, out_features=1024, bias=False)
                (k): Linear(in_features=1024, out_features=1024, bias=False)
                (v): Linear(in_features=1024, out_features=1024, bias=False)
                (o): Linear(in_features=1024, out_features=1024, bias=False)
                (relative_attention_bias): Embedding(32, 16)
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): MT5LayerCrossAttention(
              (EncDecAttention): MT5Attention(
                (q): Linear(in_features=1024, out_features=1024, bias=False)
                (k): Linear(in_features=1024, out_features=1024, bias=False)
                (v): Linear(in_features=1024, out_features=1024, bias=False)
                (o): Linear(in_features=1024, out_features=1024, bias=False)
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (2): MT5LayerFF(
              (DenseReluDense): MT5DenseGatedActDense(
                (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
                (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
                (wo): Linear(in_features=2816, out_features=1024, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (act): NewGELUActivation()
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1-23): 23 x MT5Block(
          (layer): ModuleList(
            (0): MT5LayerSelfAttention(
              (SelfAttention): MT5Attention(
                (q): Linear(in_features=1024, out_features=1024, bias=False)
                (k): Linear(in_features=1024, out_features=1024, bias=False)
                (v): Linear(in_features=1024, out_features=1024, bias=False)
                (o): Linear(in_features=1024, out_features=1024, bias=False)
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): MT5LayerCrossAttention(
              (EncDecAttention): MT5Attention(
                (q): Linear(in_features=1024, out_features=1024, bias=False)
                (k): Linear(in_features=1024, out_features=1024, bias=False)
                (v): Linear(in_features=1024, out_features=1024, bias=False)
                (o): Linear(in_features=1024, out_features=1024, bias=False)
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (2): MT5LayerFF(
              (DenseReluDense): MT5DenseGatedActDense(
                (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
                (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
                (wo): Linear(in_features=2816, out_features=1024, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (act): NewGELUActivation()
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (final_layer_norm): MT5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (slm_a): MT5Stack(
    (embed_tokens): Embedding(250112, 1024)
    (block): ModuleList(
      (0): MT5Block(
        (layer): ModuleList(
          (0): MT5LayerSelfAttention(
            (SelfAttention): MT5Attention(
              (q): Linear(in_features=1024, out_features=1024, bias=False)
              (k): Linear(in_features=1024, out_features=1024, bias=False)
              (v): Linear(in_features=1024, out_features=1024, bias=False)
              (o): Linear(in_features=1024, out_features=1024, bias=False)
              (relative_attention_bias): Embedding(32, 16)
            )
            (layer_norm): MT5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): MT5LayerFF(
            (DenseReluDense): MT5DenseGatedActDense(
              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
              (wo): Linear(in_features=2816, out_features=1024, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): NewGELUActivation()
            )
            (layer_norm): MT5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1-23): 23 x MT5Block(
        (layer): ModuleList(
          (0): MT5LayerSelfAttention(
            (SelfAttention): MT5Attention(
              (q): Linear(in_features=1024, out_features=1024, bias=False)
              (k): Linear(in_features=1024, out_features=1024, bias=False)
              (v): Linear(in_features=1024, out_features=1024, bias=False)
              (o): Linear(in_features=1024, out_features=1024, bias=False)
            )
            (layer_norm): MT5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): MT5LayerFF(
            (DenseReluDense): MT5DenseGatedActDense(
              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
              (wo): Linear(in_features=2816, out_features=1024, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): NewGELUActivation()
            )
            (layer_norm): MT5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (final_layer_norm): MT5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (mapping_a): Mapping(
    (mlp): MLP(
      (linear1): Linear(in_features=1024, out_features=2048, bias=True)
      (linear2): Linear(in_features=2048, out_features=3584, bias=True)
      (relu): ReLU()
    )
  )
)
11/12/2024 03:36:58 - INFO - __main__ - trainable params: 9446400 || all params: 8598529536 || trainable%: 0.1099
11/12/2024 03:36:58 - WARNING - accelerate.utils.other - Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:648] 2024-11-12 03:36:58,449 >> Using auto half precision backend
[2024-11-12 03:36:58,903] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.4, git-hash=unknown, git-branch=unknown
[2024-11-12 03:37:43,633] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-12 03:37:43,636] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-12 03:37:43,636] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-12 03:37:43,640] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-11-12 03:37:43,640] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-11-12 03:37:43,640] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-12 03:37:43,640] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 500000000
[2024-11-12 03:37:43,640] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 500000000
[2024-11-12 03:37:43,640] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: False
[2024-11-12 03:37:43,640] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False
[WARNING|logging.py:313] 2024-11-12 03:37:43,863 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-12 03:37:43,863 >> You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-12 03:37:43,864 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-12 03:37:43,864 >> You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-12 03:37:43,867 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-12 03:37:43,867 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-12 03:37:43,867 >> You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-12 03:37:43,867 >> You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-12 03:37:43,871 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-12 03:37:43,871 >> You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-12 03:37:43,875 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-12 03:37:43,876 >> You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-12 03:37:43,882 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-12 03:37:43,883 >> You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[2024-11-12 03:37:44,048] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2024-11-12 03:37:44,049] [INFO] [utils.py:782:see_memory_usage] MA 16.16 GB         Max_MA 16.16 GB         CA 16.29 GB         Max_CA 16 GB 
[2024-11-12 03:37:44,049] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 71.28 GB, percent = 3.5%
[2024-11-12 03:37:44,179] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2024-11-12 03:37:44,180] [INFO] [utils.py:782:see_memory_usage] MA 16.16 GB         Max_MA 16.17 GB         CA 16.29 GB         Max_CA 16 GB 
[2024-11-12 03:37:44,180] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 71.28 GB, percent = 3.5%
[2024-11-12 03:37:44,180] [INFO] [stage_1_and_2.py:543:__init__] optimizer state initialized
[2024-11-12 03:37:44,319] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2024-11-12 03:37:44,320] [INFO] [utils.py:782:see_memory_usage] MA 16.16 GB         Max_MA 16.16 GB         CA 16.29 GB         Max_CA 16 GB 
[2024-11-12 03:37:44,320] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 71.29 GB, percent = 3.5%
[2024-11-12 03:37:44,320] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2024-11-12 03:37:44,320] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-12 03:37:44,320] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-12 03:37:44,320] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002, 0.0002], mom=[(0.9, 0.999), (0.9, 0.999)]
[2024-11-12 03:37:44,322] [INFO] [config.py:997:print] DeepSpeedEngine configuration:
[2024-11-12 03:37:44,323] [INFO] [config.py:1001:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-12 03:37:44,323] [INFO] [config.py:1001:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-12 03:37:44,323] [INFO] [config.py:1001:print]   amp_enabled .................. False
[2024-11-12 03:37:44,323] [INFO] [config.py:1001:print]   amp_params ................... False
[2024-11-12 03:37:44,323] [INFO] [config.py:1001:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-12 03:37:44,323] [INFO] [config.py:1001:print]   bfloat16_enabled ............. True
[2024-11-12 03:37:44,323] [INFO] [config.py:1001:print]   bfloat16_immediate_grad_update  False
[2024-11-12 03:37:44,323] [INFO] [config.py:1001:print]   checkpoint_parallel_write_pipeline  False
[2024-11-12 03:37:44,323] [INFO] [config.py:1001:print]   checkpoint_tag_validation_enabled  True
[2024-11-12 03:37:44,323] [INFO] [config.py:1001:print]   checkpoint_tag_validation_fail  False
[2024-11-12 03:37:44,323] [INFO] [config.py:1001:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5f89fdde20>
[2024-11-12 03:37:44,323] [INFO] [config.py:1001:print]   communication_data_type ...... None
[2024-11-12 03:37:44,323] [INFO] [config.py:1001:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-12 03:37:44,323] [INFO] [config.py:1001:print]   curriculum_enabled_legacy .... False
[2024-11-12 03:37:44,323] [INFO] [config.py:1001:print]   curriculum_params_legacy ..... False
[2024-11-12 03:37:44,323] [INFO] [config.py:1001:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-12 03:37:44,323] [INFO] [config.py:1001:print]   data_efficiency_enabled ...... False
[2024-11-12 03:37:44,323] [INFO] [config.py:1001:print]   dataloader_drop_last ......... False
[2024-11-12 03:37:44,323] [INFO] [config.py:1001:print]   disable_allgather ............ False
[2024-11-12 03:37:44,323] [INFO] [config.py:1001:print]   dump_state ................... False
[2024-11-12 03:37:44,323] [INFO] [config.py:1001:print]   dynamic_loss_scale_args ...... None
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   eigenvalue_enabled ........... False
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   eigenvalue_layer_num ......... 0
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   eigenvalue_max_iter .......... 100
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   eigenvalue_stability ......... 1e-06
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   eigenvalue_tol ............... 0.01
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   eigenvalue_verbose ........... False
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   elasticity_enabled ........... False
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   fp16_auto_cast ............... None
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   fp16_enabled ................. False
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   fp16_master_weights_and_gradients  False
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   global_rank .................. 0
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   grad_accum_dtype ............. None
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   gradient_accumulation_steps .. 4
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   gradient_clipping ............ 1.0
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   gradient_predivide_factor .... 1.0
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   graph_harvesting ............. False
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   initial_dynamic_scale ........ 1
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   load_universal_checkpoint .... False
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   loss_scale ................... 1.0
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   memory_breakdown ............. False
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   mics_hierarchial_params_gather  False
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   mics_shard_size .............. -1
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   optimizer_legacy_fusion ...... False
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   optimizer_name ............... None
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   optimizer_params ............. None
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   pld_enabled .................. False
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   pld_params ................... False
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   prescale_gradients ........... False
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   scheduler_name ............... None
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   scheduler_params ............. None
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   sparse_attention ............. None
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   sparse_gradients_enabled ..... False
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   steps_per_print .............. inf
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   timers_config ................ enabled=True synchronized=True
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   train_batch_size ............. 128
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   train_micro_batch_size_per_gpu  4
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   use_data_before_expert_parallel_  False
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   use_node_local_storage ....... False
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   wall_clock_breakdown ......... False
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   weight_quantization_config ... None
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   world_size ................... 8
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   zero_allow_untested_optimizer  True
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   zero_enabled ................. True
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-12 03:37:44,324] [INFO] [config.py:1001:print]   zero_optimization_stage ...... 2
[2024-11-12 03:37:44,325] [INFO] [config.py:987:print_user_config]   json = {
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 4, 
    "gradient_accumulation_steps": 4, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }
}
[INFO|trainer.py:2134] 2024-11-12 03:37:44,325 >> ***** Running training *****
[INFO|trainer.py:2135] 2024-11-12 03:37:44,325 >>   Num examples = 395,000
[INFO|trainer.py:2136] 2024-11-12 03:37:44,325 >>   Num Epochs = 1
[INFO|trainer.py:2137] 2024-11-12 03:37:44,325 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2140] 2024-11-12 03:37:44,325 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2141] 2024-11-12 03:37:44,325 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2142] 2024-11-12 03:37:44,325 >>   Total optimization steps = 3,086
[INFO|trainer.py:2143] 2024-11-12 03:37:44,328 >>   Number of trainable parameters = 9,446,400
  0%|          | 0/3086 [00:00<?, ?it/s][WARNING|logging.py:313] 2024-11-12 03:37:44,346 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-12 03:37:44,350 >> You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|modeling_utils.py:1264] 2024-11-12 03:37:45,043 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed
[WARNING|modeling_utils.py:1264] 2024-11-12 03:37:45,043 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed
[WARNING|modeling_utils.py:1264] 2024-11-12 03:37:45,043 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed
[WARNING|modeling_utils.py:1264] 2024-11-12 03:37:45,043 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed
[WARNING|modeling_utils.py:1264] 2024-11-12 03:37:45,043 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed
[WARNING|modeling_utils.py:1264] 2024-11-12 03:37:45,043 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed
[WARNING|modeling_utils.py:1264] 2024-11-12 03:37:45,043 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed
[WARNING|modeling_utils.py:1264] 2024-11-12 03:37:45,044 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed
  0%|          | 1/3086 [00:02<2:01:01,  2.35s/it]  0%|          | 2/3086 [00:04<1:46:49,  2.08s/it]  0%|          | 3/3086 [00:06<1:44:51,  2.04s/it]  0%|          | 4/3086 [00:08<1:51:17,  2.17s/it]  0%|          | 5/3086 [00:10<1:52:07,  2.18s/it]  0%|          | 6/3086 [00:12<1:41:09,  1.97s/it]  0%|          | 7/3086 [00:14<1:47:49,  2.10s/it]  0%|          | 8/3086 [00:17<1:50:52,  2.16s/it]  0%|          | 9/3086 [00:18<1:43:09,  2.01s/it]  0%|          | 10/3086 [00:20<1:42:34,  2.00s/it]                                                   {'loss': 1.197, 'grad_norm': 0.3856011927127838, 'learning_rate': 0.00019935191186001296, 'epoch': 0.0}
  0%|          | 10/3086 [00:20<1:42:34,  2.00s/it]  0%|          | 11/3086 [00:22<1:45:23,  2.06s/it]  0%|          | 12/3086 [00:24<1:44:12,  2.03s/it]  0%|          | 13/3086 [00:27<1:51:43,  2.18s/it]  0%|          | 14/3086 [00:30<2:01:00,  2.36s/it]  0%|          | 15/3086 [00:31<1:50:47,  2.16s/it]  1%|          | 16/3086 [00:33<1:49:31,  2.14s/it]  1%|          | 17/3086 [00:35<1:40:54,  1.97s/it]  1%|          | 18/3086 [00:37<1:48:07,  2.11s/it]  1%|          | 19/3086 [00:39<1:40:52,  1.97s/it]  1%|          | 20/3086 [00:41<1:42:53,  2.01s/it]                                                   {'loss': 1.1082, 'grad_norm': 0.5603898167610168, 'learning_rate': 0.00019870382372002594, 'epoch': 0.01}
  1%|          | 20/3086 [00:41<1:42:53,  2.01s/it]  1%|          | 21/3086 [00:43<1:38:51,  1.94s/it]  1%|          | 22/3086 [00:45<1:37:05,  1.90s/it]  1%|          | 23/3086 [00:47<1:38:47,  1.94s/it]  1%|          | 24/3086 [00:48<1:34:16,  1.85s/it]  1%|          | 25/3086 [00:50<1:31:31,  1.79s/it]  1%|          | 26/3086 [00:52<1:25:12,  1.67s/it]  1%|          | 27/3086 [00:53<1:28:03,  1.73s/it]  1%|          | 28/3086 [00:55<1:25:26,  1.68s/it]  1%|          | 29/3086 [00:57<1:29:38,  1.76s/it]  1%|          | 30/3086 [00:59<1:35:50,  1.88s/it]                                                   {'loss': 1.0158, 'grad_norm': 0.5148584246635437, 'learning_rate': 0.0001980557355800389, 'epoch': 0.01}
  1%|          | 30/3086 [00:59<1:35:50,  1.88s/it]  1%|          | 31/3086 [01:01<1:38:14,  1.93s/it]  1%|          | 32/3086 [01:03<1:34:36,  1.86s/it]  1%|          | 33/3086 [01:05<1:32:52,  1.83s/it]  1%|          | 34/3086 [01:06<1:31:40,  1.80s/it]  1%|          | 35/3086 [01:08<1:29:57,  1.77s/it]  1%|          | 36/3086 [01:10<1:39:24,  1.96s/it]  1%|          | 37/3086 [01:12<1:34:27,  1.86s/it]  1%|          | 38/3086 [01:14<1:39:39,  1.96s/it]  1%|▏         | 39/3086 [01:16<1:37:07,  1.91s/it]  1%|▏         | 40/3086 [01:18<1:34:55,  1.87s/it]                                                   {'loss': 0.9638, 'grad_norm': 0.49530360102653503, 'learning_rate': 0.00019740764744005185, 'epoch': 0.01}
  1%|▏         | 40/3086 [01:18<1:34:55,  1.87s/it]  1%|▏         | 41/3086 [01:21<1:48:51,  2.14s/it]  1%|▏         | 42/3086 [01:22<1:45:45,  2.08s/it]  1%|▏         | 43/3086 [01:24<1:44:04,  2.05s/it]  1%|▏         | 44/3086 [01:26<1:39:59,  1.97s/it]  1%|▏         | 45/3086 [01:29<1:51:15,  2.20s/it]  1%|▏         | 46/3086 [01:31<1:44:17,  2.06s/it]  2%|▏         | 47/3086 [01:32<1:37:17,  1.92s/it]  2%|▏         | 48/3086 [01:34<1:33:49,  1.85s/it]  2%|▏         | 49/3086 [01:36<1:32:46,  1.83s/it]  2%|▏         | 50/3086 [01:38<1:37:15,  1.92s/it]                                                   {'loss': 0.8882, 'grad_norm': 0.4695718586444855, 'learning_rate': 0.00019675955930006483, 'epoch': 0.02}
  2%|▏         | 50/3086 [01:38<1:37:15,  1.92s/it]  2%|▏         | 51/3086 [01:40<1:41:30,  2.01s/it]  2%|▏         | 52/3086 [01:42<1:46:41,  2.11s/it]  2%|▏         | 53/3086 [01:45<1:54:25,  2.26s/it]  2%|▏         | 54/3086 [01:47<1:52:01,  2.22s/it]  2%|▏         | 55/3086 [01:49<1:42:27,  2.03s/it]  2%|▏         | 56/3086 [01:52<1:56:56,  2.32s/it]  2%|▏         | 57/3086 [01:54<1:52:58,  2.24s/it]  2%|▏         | 58/3086 [01:56<1:47:44,  2.13s/it]  2%|▏         | 59/3086 [01:58<1:55:32,  2.29s/it]  2%|▏         | 60/3086 [02:01<1:54:59,  2.28s/it]                                                   {'loss': 0.79, 'grad_norm': 0.40005025267601013, 'learning_rate': 0.00019611147116007778, 'epoch': 0.02}
  2%|▏         | 60/3086 [02:01<1:54:59,  2.28s/it]  2%|▏         | 61/3086 [02:03<1:49:23,  2.17s/it]  2%|▏         | 62/3086 [02:04<1:40:31,  1.99s/it]  2%|▏         | 63/3086 [02:07<1:47:28,  2.13s/it]  2%|▏         | 64/3086 [02:08<1:43:48,  2.06s/it]  2%|▏         | 65/3086 [02:10<1:39:43,  1.98s/it]  2%|▏         | 66/3086 [02:12<1:43:18,  2.05s/it]  2%|▏         | 67/3086 [02:14<1:37:18,  1.93s/it]  2%|▏         | 68/3086 [02:16<1:39:36,  1.98s/it]  2%|▏         | 69/3086 [02:18<1:40:15,  1.99s/it]  2%|▏         | 70/3086 [02:20<1:42:17,  2.04s/it]                                                   {'loss': 0.7835, 'grad_norm': 0.4333130419254303, 'learning_rate': 0.00019546338302009076, 'epoch': 0.02}
  2%|▏         | 70/3086 [02:20<1:42:17,  2.04s/it]  2%|▏         | 71/3086 [02:22<1:40:00,  1.99s/it]  2%|▏         | 72/3086 [02:24<1:37:19,  1.94s/it]  2%|▏         | 73/3086 [02:26<1:34:10,  1.88s/it]  2%|▏         | 74/3086 [02:28<1:38:12,  1.96s/it]  2%|▏         | 75/3086 [02:30<1:43:33,  2.06s/it]  2%|▏         | 76/3086 [02:32<1:39:55,  1.99s/it]  2%|▏         | 77/3086 [02:34<1:38:03,  1.96s/it]  3%|▎         | 78/3086 [02:36<1:35:04,  1.90s/it]  3%|▎         | 79/3086 [02:38<1:42:38,  2.05s/it]  3%|▎         | 80/3086 [02:40<1:43:49,  2.07s/it]                                                   {'loss': 0.7312, 'grad_norm': 0.4418777823448181, 'learning_rate': 0.00019481529488010372, 'epoch': 0.03}
  3%|▎         | 80/3086 [02:40<1:43:49,  2.07s/it]  3%|▎         | 81/3086 [02:42<1:45:40,  2.11s/it]  3%|▎         | 82/3086 [02:44<1:39:39,  1.99s/it]  3%|▎         | 83/3086 [02:47<1:44:59,  2.10s/it]  3%|▎         | 84/3086 [02:48<1:40:08,  2.00s/it]  3%|▎         | 85/3086 [02:50<1:42:55,  2.06s/it]  3%|▎         | 86/3086 [02:52<1:36:57,  1.94s/it]  3%|▎         | 87/3086 [02:54<1:34:35,  1.89s/it]  3%|▎         | 88/3086 [02:56<1:32:30,  1.85s/it]  3%|▎         | 89/3086 [02:57<1:30:04,  1.80s/it]  3%|▎         | 90/3086 [03:00<1:37:19,  1.95s/it]                                                   {'loss': 0.7293, 'grad_norm': 0.9737394452095032, 'learning_rate': 0.00019416720674011667, 'epoch': 0.03}
  3%|▎         | 90/3086 [03:00<1:37:19,  1.95s/it]  3%|▎         | 91/3086 [03:02<1:36:59,  1.94s/it]  3%|▎         | 92/3086 [03:04<1:36:30,  1.93s/it]  3%|▎         | 93/3086 [03:07<1:52:06,  2.25s/it]  3%|▎         | 94/3086 [03:09<1:49:35,  2.20s/it]  3%|▎         | 95/3086 [03:11<1:47:26,  2.16s/it]  3%|▎         | 96/3086 [03:12<1:42:35,  2.06s/it]  3%|▎         | 97/3086 [03:15<1:42:50,  2.06s/it]  3%|▎         | 98/3086 [03:17<1:43:11,  2.07s/it]  3%|▎         | 99/3086 [03:20<1:55:39,  2.32s/it]  3%|▎         | 100/3086 [03:21<1:50:02,  2.21s/it]                                                    {'loss': 0.7088, 'grad_norm': 0.6295893788337708, 'learning_rate': 0.00019351911860012962, 'epoch': 0.03}
  3%|▎         | 100/3086 [03:21<1:50:02,  2.21s/it]  3%|▎         | 101/3086 [03:23<1:44:03,  2.09s/it]  3%|▎         | 102/3086 [03:25<1:42:29,  2.06s/it]  3%|▎         | 103/3086 [03:27<1:37:42,  1.97s/it]  3%|▎         | 104/3086 [03:29<1:42:18,  2.06s/it]  3%|▎         | 105/3086 [03:31<1:43:55,  2.09s/it]  3%|▎         | 106/3086 [03:34<1:49:22,  2.20s/it]  3%|▎         | 107/3086 [03:36<1:44:22,  2.10s/it]  3%|▎         | 108/3086 [03:38<1:41:22,  2.04s/it]  4%|▎         | 109/3086 [03:40<1:39:50,  2.01s/it]  4%|▎         | 110/3086 [03:42<1:40:49,  2.03s/it]                                                    {'loss': 0.7061, 'grad_norm': 0.43917056918144226, 'learning_rate': 0.00019287103046014258, 'epoch': 0.04}
  4%|▎         | 110/3086 [03:42<1:40:49,  2.03s/it]  4%|▎         | 111/3086 [03:43<1:36:38,  1.95s/it]  4%|▎         | 112/3086 [03:45<1:36:29,  1.95s/it]  4%|▎         | 113/3086 [03:47<1:33:20,  1.88s/it]  4%|▎         | 114/3086 [03:49<1:26:23,  1.74s/it]  4%|▎         | 115/3086 [03:50<1:27:07,  1.76s/it]  4%|▍         | 116/3086 [03:52<1:28:18,  1.78s/it]  4%|▍         | 117/3086 [03:54<1:30:34,  1.83s/it]  4%|▍         | 118/3086 [03:56<1:32:24,  1.87s/it]  4%|▍         | 119/3086 [03:58<1:35:55,  1.94s/it]  4%|▍         | 120/3086 [04:00<1:32:15,  1.87s/it]                                                    {'loss': 0.699, 'grad_norm': 0.5335282683372498, 'learning_rate': 0.00019222294232015553, 'epoch': 0.04}
  4%|▍         | 120/3086 [04:00<1:32:15,  1.87s/it]  4%|▍         | 121/3086 [04:02<1:35:00,  1.92s/it]  4%|▍         | 122/3086 [04:04<1:40:52,  2.04s/it]  4%|▍         | 123/3086 [04:07<1:43:17,  2.09s/it]  4%|▍         | 124/3086 [04:09<1:49:39,  2.22s/it]  4%|▍         | 125/3086 [04:12<1:55:49,  2.35s/it]  4%|▍         | 126/3086 [04:14<1:56:23,  2.36s/it]  4%|▍         | 127/3086 [04:16<1:52:21,  2.28s/it]  4%|▍         | 128/3086 [04:18<1:48:27,  2.20s/it]  4%|▍         | 129/3086 [04:20<1:49:13,  2.22s/it]  4%|▍         | 130/3086 [04:22<1:44:35,  2.12s/it]                                                    {'loss': 0.703, 'grad_norm': 0.7313830256462097, 'learning_rate': 0.0001915748541801685, 'epoch': 0.04}
  4%|▍         | 130/3086 [04:22<1:44:35,  2.12s/it]  4%|▍         | 131/3086 [04:25<1:46:16,  2.16s/it]  4%|▍         | 132/3086 [04:26<1:42:02,  2.07s/it]  4%|▍         | 133/3086 [04:28<1:38:35,  2.00s/it]  4%|▍         | 134/3086 [04:30<1:39:20,  2.02s/it]  4%|▍         | 135/3086 [04:32<1:33:32,  1.90s/it]  4%|▍         | 136/3086 [04:34<1:28:38,  1.80s/it]  4%|▍         | 137/3086 [04:36<1:33:03,  1.89s/it]  4%|▍         | 138/3086 [04:37<1:31:31,  1.86s/it]  5%|▍         | 139/3086 [04:39<1:34:07,  1.92s/it]  5%|▍         | 140/3086 [04:41<1:31:07,  1.86s/it]                                                    {'loss': 0.6971, 'grad_norm': 0.6854750514030457, 'learning_rate': 0.00019092676604018146, 'epoch': 0.05}
  5%|▍         | 140/3086 [04:41<1:31:07,  1.86s/it]  5%|▍         | 141/3086 [04:43<1:35:09,  1.94s/it]  5%|▍         | 142/3086 [04:45<1:37:28,  1.99s/it]  5%|▍         | 143/3086 [04:47<1:37:00,  1.98s/it]  5%|▍         | 144/3086 [04:49<1:32:27,  1.89s/it]  5%|▍         | 145/3086 [04:51<1:33:54,  1.92s/it]  5%|▍         | 146/3086 [04:53<1:34:51,  1.94s/it]  5%|▍         | 147/3086 [04:55<1:31:41,  1.87s/it]  5%|▍         | 148/3086 [04:57<1:39:02,  2.02s/it]  5%|▍         | 149/3086 [04:59<1:40:23,  2.05s/it]  5%|▍         | 150/3086 [05:01<1:39:33,  2.03s/it]                                                    {'loss': 0.6815, 'grad_norm': 0.5986210107803345, 'learning_rate': 0.00019027867790019444, 'epoch': 0.05}
  5%|▍         | 150/3086 [05:01<1:39:33,  2.03s/it]  5%|▍         | 151/3086 [05:03<1:35:08,  1.94s/it]  5%|▍         | 152/3086 [05:05<1:38:18,  2.01s/it]  5%|▍         | 153/3086 [05:07<1:37:19,  1.99s/it]  5%|▍         | 154/3086 [05:09<1:35:53,  1.96s/it]  5%|▌         | 155/3086 [05:11<1:33:44,  1.92s/it]  5%|▌         | 156/3086 [05:13<1:37:27,  2.00s/it]  5%|▌         | 157/3086 [05:15<1:36:36,  1.98s/it]  5%|▌         | 158/3086 [05:17<1:38:58,  2.03s/it]  5%|▌         | 159/3086 [05:19<1:36:34,  1.98s/it]  5%|▌         | 160/3086 [05:21<1:41:55,  2.09s/it]                                                    {'loss': 0.6678, 'grad_norm': 0.7416941523551941, 'learning_rate': 0.0001896305897602074, 'epoch': 0.05}
  5%|▌         | 160/3086 [05:21<1:41:55,  2.09s/it]  5%|▌         | 161/3086 [05:24<1:54:32,  2.35s/it]  5%|▌         | 162/3086 [05:26<1:48:56,  2.24s/it]  5%|▌         | 163/3086 [05:28<1:44:21,  2.14s/it]  5%|▌         | 164/3086 [05:30<1:44:00,  2.14s/it]  5%|▌         | 165/3086 [05:33<1:49:49,  2.26s/it]  5%|▌         | 166/3086 [05:35<1:53:07,  2.32s/it]  5%|▌         | 167/3086 [05:37<1:48:36,  2.23s/it]  5%|▌         | 168/3086 [05:39<1:44:44,  2.15s/it]  5%|▌         | 169/3086 [05:41<1:37:16,  2.00s/it]  6%|▌         | 170/3086 [05:43<1:33:22,  1.92s/it]                                                    {'loss': 0.672, 'grad_norm': 0.5649078488349915, 'learning_rate': 0.00018898250162022035, 'epoch': 0.06}
  6%|▌         | 170/3086 [05:43<1:33:22,  1.92s/it]  6%|▌         | 171/3086 [05:45<1:44:27,  2.15s/it]  6%|▌         | 172/3086 [05:47<1:36:48,  1.99s/it]  6%|▌         | 173/3086 [05:49<1:34:28,  1.95s/it]  6%|▌         | 174/3086 [05:51<1:41:44,  2.10s/it]  6%|▌         | 175/3086 [05:53<1:42:05,  2.10s/it]  6%|▌         | 176/3086 [05:55<1:33:48,  1.93s/it]  6%|▌         | 177/3086 [05:57<1:37:40,  2.01s/it]  6%|▌         | 178/3086 [05:59<1:38:08,  2.02s/it]  6%|▌         | 179/3086 [06:01<1:32:19,  1.91s/it]  6%|▌         | 180/3086 [06:02<1:26:32,  1.79s/it]                                                    {'loss': 0.6688, 'grad_norm': 0.6171154975891113, 'learning_rate': 0.00018833441348023333, 'epoch': 0.06}
  6%|▌         | 180/3086 [06:02<1:26:32,  1.79s/it]  6%|▌         | 181/3086 [06:04<1:26:13,  1.78s/it]  6%|▌         | 182/3086 [06:06<1:30:56,  1.88s/it]  6%|▌         | 183/3086 [06:08<1:30:33,  1.87s/it]  6%|▌         | 184/3086 [06:10<1:30:14,  1.87s/it]  6%|▌         | 185/3086 [06:12<1:29:07,  1.84s/it]  6%|▌         | 186/3086 [06:14<1:30:08,  1.86s/it]  6%|▌         | 187/3086 [06:15<1:27:47,  1.82s/it]  6%|▌         | 188/3086 [06:18<1:35:44,  1.98s/it]  6%|▌         | 189/3086 [06:20<1:41:49,  2.11s/it]  6%|▌         | 190/3086 [06:22<1:33:11,  1.93s/it]                                                    {'loss': 0.6607, 'grad_norm': 0.605580747127533, 'learning_rate': 0.00018768632534024628, 'epoch': 0.06}
  6%|▌         | 190/3086 [06:22<1:33:11,  1.93s/it]  6%|▌         | 191/3086 [06:24<1:38:09,  2.03s/it]  6%|▌         | 192/3086 [06:26<1:33:06,  1.93s/it]  6%|▋         | 193/3086 [06:28<1:34:37,  1.96s/it]  6%|▋         | 194/3086 [06:30<1:35:58,  1.99s/it]  6%|▋         | 195/3086 [06:31<1:33:36,  1.94s/it]  6%|▋         | 196/3086 [06:34<1:41:41,  2.11s/it]  6%|▋         | 197/3086 [06:36<1:37:28,  2.02s/it]  6%|▋         | 198/3086 [06:38<1:47:04,  2.22s/it]  6%|▋         | 199/3086 [06:40<1:42:14,  2.12s/it]  6%|▋         | 200/3086 [06:42<1:41:55,  2.12s/it]                                                    {'loss': 0.649, 'grad_norm': 0.426144003868103, 'learning_rate': 0.00018703823720025924, 'epoch': 0.06}
  6%|▋         | 200/3086 [06:42<1:41:55,  2.12s/it]  7%|▋         | 201/3086 [06:44<1:39:34,  2.07s/it]  7%|▋         | 202/3086 [06:47<1:46:13,  2.21s/it]  7%|▋         | 203/3086 [06:49<1:40:44,  2.10s/it]  7%|▋         | 204/3086 [06:50<1:33:19,  1.94s/it]  7%|▋         | 205/3086 [06:53<1:43:44,  2.16s/it]  7%|▋         | 206/3086 [06:55<1:38:55,  2.06s/it]  7%|▋         | 207/3086 [06:57<1:36:54,  2.02s/it]  7%|▋         | 208/3086 [06:59<1:40:58,  2.11s/it]  7%|▋         | 209/3086 [07:01<1:35:50,  2.00s/it]  7%|▋         | 210/3086 [07:03<1:34:12,  1.97s/it]                                                    {'loss': 0.6502, 'grad_norm': 0.7448009252548218, 'learning_rate': 0.00018639014906027222, 'epoch': 0.07}
  7%|▋         | 210/3086 [07:03<1:34:12,  1.97s/it]  7%|▋         | 211/3086 [07:05<1:39:48,  2.08s/it]  7%|▋         | 212/3086 [07:07<1:43:39,  2.16s/it]  7%|▋         | 213/3086 [07:10<1:42:45,  2.15s/it]  7%|▋         | 214/3086 [07:11<1:36:45,  2.02s/it]  7%|▋         | 215/3086 [07:14<1:45:25,  2.20s/it]  7%|▋         | 216/3086 [07:16<1:42:04,  2.13s/it]  7%|▋         | 217/3086 [07:18<1:40:58,  2.11s/it]  7%|▋         | 218/3086 [07:20<1:37:43,  2.04s/it]  7%|▋         | 219/3086 [07:22<1:39:17,  2.08s/it]  7%|▋         | 220/3086 [07:24<1:33:22,  1.95s/it]                                                    {'loss': 0.6505, 'grad_norm': 0.895988404750824, 'learning_rate': 0.00018574206092028517, 'epoch': 0.07}
  7%|▋         | 220/3086 [07:24<1:33:22,  1.95s/it]  7%|▋         | 221/3086 [07:26<1:41:41,  2.13s/it]  7%|▋         | 222/3086 [07:28<1:36:57,  2.03s/it]  7%|▋         | 223/3086 [07:30<1:39:40,  2.09s/it]  7%|▋         | 224/3086 [07:32<1:38:25,  2.06s/it]  7%|▋         | 225/3086 [07:35<1:44:05,  2.18s/it]  7%|▋         | 226/3086 [07:37<1:47:45,  2.26s/it]  7%|▋         | 227/3086 [07:40<1:56:38,  2.45s/it]  7%|▋         | 228/3086 [07:43<1:57:38,  2.47s/it]  7%|▋         | 229/3086 [07:44<1:49:34,  2.30s/it]  7%|▋         | 230/3086 [07:46<1:38:00,  2.06s/it]                                                    {'loss': 0.6367, 'grad_norm': 0.7538196444511414, 'learning_rate': 0.00018509397278029813, 'epoch': 0.07}
  7%|▋         | 230/3086 [07:46<1:38:00,  2.06s/it]  7%|▋         | 231/3086 [07:48<1:34:24,  1.98s/it]  8%|▊         | 232/3086 [07:50<1:41:22,  2.13s/it]  8%|▊         | 233/3086 [07:52<1:36:12,  2.02s/it]  8%|▊         | 234/3086 [07:54<1:39:36,  2.10s/it]  8%|▊         | 235/3086 [07:56<1:35:13,  2.00s/it]  8%|▊         | 236/3086 [07:58<1:34:35,  1.99s/it]  8%|▊         | 237/3086 [08:00<1:31:38,  1.93s/it]  8%|▊         | 238/3086 [08:02<1:32:19,  1.94s/it]  8%|▊         | 239/3086 [08:04<1:31:27,  1.93s/it]  8%|▊         | 240/3086 [08:05<1:29:49,  1.89s/it]                                                    {'loss': 0.6375, 'grad_norm': 0.5869702100753784, 'learning_rate': 0.0001844458846403111, 'epoch': 0.08}
  8%|▊         | 240/3086 [08:05<1:29:49,  1.89s/it]  8%|▊         | 241/3086 [08:07<1:28:37,  1.87s/it]  8%|▊         | 242/3086 [08:09<1:28:35,  1.87s/it]  8%|▊         | 243/3086 [08:11<1:30:57,  1.92s/it]  8%|▊         | 244/3086 [08:14<1:37:15,  2.05s/it]  8%|▊         | 245/3086 [08:15<1:35:11,  2.01s/it]  8%|▊         | 246/3086 [08:17<1:34:14,  1.99s/it]  8%|▊         | 247/3086 [08:20<1:36:24,  2.04s/it]  8%|▊         | 248/3086 [08:21<1:33:44,  1.98s/it]  8%|▊         | 249/3086 [08:23<1:28:40,  1.88s/it]  8%|▊         | 250/3086 [08:25<1:30:58,  1.92s/it]                                                    {'loss': 0.6425, 'grad_norm': 0.706311821937561, 'learning_rate': 0.00018379779650032406, 'epoch': 0.08}
  8%|▊         | 250/3086 [08:25<1:30:58,  1.92s/it]  8%|▊         | 251/3086 [08:27<1:28:13,  1.87s/it]  8%|▊         | 252/3086 [08:29<1:29:05,  1.89s/it]  8%|▊         | 253/3086 [08:31<1:34:32,  2.00s/it]  8%|▊         | 254/3086 [08:33<1:36:18,  2.04s/it]  8%|▊         | 255/3086 [08:35<1:28:01,  1.87s/it]  8%|▊         | 256/3086 [08:38<1:43:10,  2.19s/it]  8%|▊         | 257/3086 [08:40<1:44:13,  2.21s/it]  8%|▊         | 258/3086 [08:41<1:37:08,  2.06s/it]  8%|▊         | 259/3086 [08:44<1:36:36,  2.05s/it]  8%|▊         | 260/3086 [08:45<1:29:17,  1.90s/it]                                                    {'loss': 0.6414, 'grad_norm': 0.9223597645759583, 'learning_rate': 0.000183149708360337, 'epoch': 0.08}
  8%|▊         | 260/3086 [08:45<1:29:17,  1.90s/it]  8%|▊         | 261/3086 [08:47<1:26:22,  1.83s/it]  8%|▊         | 262/3086 [08:49<1:31:19,  1.94s/it]  9%|▊         | 263/3086 [08:51<1:29:30,  1.90s/it]  9%|▊         | 264/3086 [08:53<1:35:06,  2.02s/it]  9%|▊         | 265/3086 [08:55<1:34:49,  2.02s/it]  9%|▊         | 266/3086 [08:57<1:33:26,  1.99s/it]  9%|▊         | 267/3086 [08:59<1:30:10,  1.92s/it]  9%|▊         | 268/3086 [09:01<1:32:56,  1.98s/it]  9%|▊         | 269/3086 [09:03<1:32:40,  1.97s/it]  9%|▊         | 270/3086 [09:04<1:28:12,  1.88s/it]                                                    {'loss': 0.6415, 'grad_norm': 0.676120936870575, 'learning_rate': 0.00018250162022034997, 'epoch': 0.09}
  9%|▊         | 270/3086 [09:04<1:28:12,  1.88s/it]  9%|▉         | 271/3086 [09:06<1:27:26,  1.86s/it]  9%|▉         | 272/3086 [09:09<1:45:24,  2.25s/it]  9%|▉         | 273/3086 [09:11<1:40:50,  2.15s/it]  9%|▉         | 274/3086 [09:13<1:39:49,  2.13s/it]  9%|▉         | 275/3086 [09:16<1:39:30,  2.12s/it]  9%|▉         | 276/3086 [09:18<1:42:35,  2.19s/it]  9%|▉         | 277/3086 [09:20<1:43:48,  2.22s/it]  9%|▉         | 278/3086 [09:22<1:42:33,  2.19s/it]  9%|▉         | 279/3086 [09:24<1:35:25,  2.04s/it]  9%|▉         | 280/3086 [09:26<1:39:21,  2.12s/it]                                                    {'loss': 0.6399, 'grad_norm': 0.4421338438987732, 'learning_rate': 0.00018185353208036292, 'epoch': 0.09}
  9%|▉         | 280/3086 [09:26<1:39:21,  2.12s/it]  9%|▉         | 281/3086 [09:28<1:31:50,  1.96s/it]  9%|▉         | 282/3086 [09:30<1:35:16,  2.04s/it]  9%|▉         | 283/3086 [09:32<1:32:55,  1.99s/it]  9%|▉         | 284/3086 [09:34<1:30:15,  1.93s/it]  9%|▉         | 285/3086 [09:36<1:29:54,  1.93s/it]  9%|▉         | 286/3086 [09:38<1:39:24,  2.13s/it]  9%|▉         | 287/3086 [09:40<1:33:03,  1.99s/it]  9%|▉         | 288/3086 [09:43<1:42:21,  2.20s/it]  9%|▉         | 289/3086 [09:44<1:37:13,  2.09s/it]  9%|▉         | 290/3086 [09:46<1:35:20,  2.05s/it]                                                    {'loss': 0.6133, 'grad_norm': 0.9016205668449402, 'learning_rate': 0.0001812054439403759, 'epoch': 0.09}
  9%|▉         | 290/3086 [09:46<1:35:20,  2.05s/it]  9%|▉         | 291/3086 [09:48<1:30:07,  1.93s/it]  9%|▉         | 292/3086 [09:50<1:32:22,  1.98s/it]  9%|▉         | 293/3086 [09:52<1:34:57,  2.04s/it] 10%|▉         | 294/3086 [09:54<1:32:19,  1.98s/it] 10%|▉         | 295/3086 [09:57<1:36:46,  2.08s/it] 10%|▉         | 296/3086 [09:58<1:30:10,  1.94s/it] 10%|▉         | 297/3086 [10:00<1:31:28,  1.97s/it] 10%|▉         | 298/3086 [10:02<1:32:25,  1.99s/it] 10%|▉         | 299/3086 [10:04<1:31:51,  1.98s/it] 10%|▉         | 300/3086 [10:06<1:26:31,  1.86s/it]                                                    {'loss': 0.6301, 'grad_norm': 0.5547265410423279, 'learning_rate': 0.00018055735580038885, 'epoch': 0.1}
 10%|▉         | 300/3086 [10:06<1:26:31,  1.86s/it] 10%|▉         | 301/3086 [10:08<1:30:16,  1.95s/it] 10%|▉         | 302/3086 [10:10<1:26:35,  1.87s/it] 10%|▉         | 303/3086 [10:12<1:31:22,  1.97s/it] 10%|▉         | 304/3086 [10:14<1:37:20,  2.10s/it] 10%|▉         | 305/3086 [10:16<1:37:54,  2.11s/it] 10%|▉         | 306/3086 [10:18<1:38:05,  2.12s/it] 10%|▉         | 307/3086 [10:21<1:39:16,  2.14s/it] 10%|▉         | 308/3086 [10:22<1:33:44,  2.02s/it] 10%|█         | 309/3086 [10:24<1:27:37,  1.89s/it] 10%|█         | 310/3086 [10:26<1:24:12,  1.82s/it]                                                    {'loss': 0.6361, 'grad_norm': 0.5407803654670715, 'learning_rate': 0.0001799092676604018, 'epoch': 0.1}
 10%|█         | 310/3086 [10:26<1:24:12,  1.82s/it] 10%|█         | 311/3086 [10:27<1:23:55,  1.81s/it] 10%|█         | 312/3086 [10:29<1:25:52,  1.86s/it] 10%|█         | 313/3086 [10:31<1:23:20,  1.80s/it] 10%|█         | 314/3086 [10:33<1:24:04,  1.82s/it] 10%|█         | 315/3086 [10:35<1:29:32,  1.94s/it] 10%|█         | 316/3086 [10:37<1:26:13,  1.87s/it] 10%|█         | 317/3086 [10:39<1:27:09,  1.89s/it] 10%|█         | 318/3086 [10:41<1:28:51,  1.93s/it] 10%|█         | 319/3086 [10:43<1:29:44,  1.95s/it] 10%|█         | 320/3086 [10:45<1:29:52,  1.95s/it]                                                    {'loss': 0.6106, 'grad_norm': 0.5803713798522949, 'learning_rate': 0.00017926117952041479, 'epoch': 0.1}
 10%|█         | 320/3086 [10:45<1:29:52,  1.95s/it] 10%|█         | 321/3086 [10:47<1:31:08,  1.98s/it] 10%|█         | 322/3086 [10:49<1:27:31,  1.90s/it] 10%|█         | 323/3086 [10:50<1:25:06,  1.85s/it] 10%|█         | 324/3086 [10:52<1:26:33,  1.88s/it] 11%|█         | 325/3086 [10:54<1:23:42,  1.82s/it] 11%|█         | 326/3086 [10:56<1:30:40,  1.97s/it] 11%|█         | 327/3086 [10:58<1:27:57,  1.91s/it] 11%|█         | 328/3086 [11:00<1:29:08,  1.94s/it] 11%|█         | 329/3086 [11:02<1:33:46,  2.04s/it] 11%|█         | 330/3086 [11:04<1:34:05,  2.05s/it]                                                    {'loss': 0.6272, 'grad_norm': 0.5017828345298767, 'learning_rate': 0.00017861309138042774, 'epoch': 0.11}
 11%|█         | 330/3086 [11:04<1:34:05,  2.05s/it] 11%|█         | 331/3086 [11:07<1:35:57,  2.09s/it] 11%|█         | 332/3086 [11:08<1:32:30,  2.02s/it] 11%|█         | 333/3086 [11:10<1:28:36,  1.93s/it] 11%|█         | 334/3086 [11:12<1:34:42,  2.06s/it] 11%|█         | 335/3086 [11:14<1:33:16,  2.03s/it] 11%|█         | 336/3086 [11:17<1:34:05,  2.05s/it] 11%|█         | 337/3086 [11:18<1:31:25,  2.00s/it] 11%|█         | 338/3086 [11:20<1:27:13,  1.90s/it] 11%|█         | 339/3086 [11:22<1:28:44,  1.94s/it] 11%|█         | 340/3086 [11:24<1:33:03,  2.03s/it]                                                    {'loss': 0.6152, 'grad_norm': 0.8968108296394348, 'learning_rate': 0.0001779650032404407, 'epoch': 0.11}
 11%|█         | 340/3086 [11:24<1:33:03,  2.03s/it] 11%|█         | 341/3086 [11:26<1:30:07,  1.97s/it] 11%|█         | 342/3086 [11:28<1:29:56,  1.97s/it] 11%|█         | 343/3086 [11:30<1:23:45,  1.83s/it] 11%|█         | 344/3086 [11:31<1:20:37,  1.76s/it] 11%|█         | 345/3086 [11:34<1:28:42,  1.94s/it] 11%|█         | 346/3086 [11:36<1:34:36,  2.07s/it] 11%|█         | 347/3086 [11:38<1:29:42,  1.97s/it] 11%|█▏        | 348/3086 [11:40<1:30:36,  1.99s/it] 11%|█▏        | 349/3086 [11:42<1:30:42,  1.99s/it] 11%|█▏        | 350/3086 [11:44<1:32:50,  2.04s/it]                                                    {'loss': 0.6261, 'grad_norm': 0.5729802846908569, 'learning_rate': 0.00017731691510045367, 'epoch': 0.11}
 11%|█▏        | 350/3086 [11:44<1:32:50,  2.04s/it] 11%|█▏        | 351/3086 [11:46<1:28:30,  1.94s/it] 11%|█▏        | 352/3086 [11:48<1:28:07,  1.93s/it] 11%|█▏        | 353/3086 [11:49<1:27:59,  1.93s/it] 11%|█▏        | 354/3086 [11:52<1:31:02,  2.00s/it] 12%|█▏        | 355/3086 [11:53<1:28:22,  1.94s/it] 12%|█▏        | 356/3086 [11:55<1:28:15,  1.94s/it] 12%|█▏        | 357/3086 [11:57<1:23:00,  1.83s/it] 12%|█▏        | 358/3086 [11:59<1:30:13,  1.98s/it] 12%|█▏        | 359/3086 [12:01<1:33:09,  2.05s/it] 12%|█▏        | 360/3086 [12:03<1:26:31,  1.90s/it]                                                    {'loss': 0.6187, 'grad_norm': 0.5691643953323364, 'learning_rate': 0.00017666882696046663, 'epoch': 0.12}
 12%|█▏        | 360/3086 [12:03<1:26:31,  1.90s/it] 12%|█▏        | 361/3086 [12:05<1:30:22,  1.99s/it] 12%|█▏        | 362/3086 [12:07<1:31:40,  2.02s/it] 12%|█▏        | 363/3086 [12:09<1:28:14,  1.94s/it] 12%|█▏        | 364/3086 [12:11<1:28:59,  1.96s/it] 12%|█▏        | 365/3086 [12:13<1:28:47,  1.96s/it] 12%|█▏        | 366/3086 [12:15<1:28:27,  1.95s/it] 12%|█▏        | 367/3086 [12:17<1:31:35,  2.02s/it] 12%|█▏        | 368/3086 [12:19<1:31:13,  2.01s/it] 12%|█▏        | 369/3086 [12:21<1:28:38,  1.96s/it] 12%|█▏        | 370/3086 [12:23<1:27:32,  1.93s/it]                                                    {'loss': 0.6259, 'grad_norm': 0.7673326134681702, 'learning_rate': 0.0001760207388204796, 'epoch': 0.12}
 12%|█▏        | 370/3086 [12:23<1:27:32,  1.93s/it] 12%|█▏        | 371/3086 [12:25<1:29:13,  1.97s/it] 12%|█▏        | 372/3086 [12:27<1:26:24,  1.91s/it] 12%|█▏        | 373/3086 [12:29<1:27:22,  1.93s/it] 12%|█▏        | 374/3086 [12:30<1:24:22,  1.87s/it] 12%|█▏        | 375/3086 [12:33<1:30:25,  2.00s/it] 12%|█▏        | 376/3086 [12:35<1:30:25,  2.00s/it] 12%|█▏        | 377/3086 [12:37<1:29:40,  1.99s/it] 12%|█▏        | 378/3086 [12:39<1:29:43,  1.99s/it] 12%|█▏        | 379/3086 [12:41<1:39:04,  2.20s/it] 12%|█▏        | 380/3086 [12:43<1:37:46,  2.17s/it]                                                    {'loss': 0.6195, 'grad_norm': 0.5300140976905823, 'learning_rate': 0.00017537265068049256, 'epoch': 0.12}
 12%|█▏        | 380/3086 [12:43<1:37:46,  2.17s/it] 12%|█▏        | 381/3086 [12:45<1:34:32,  2.10s/it] 12%|█▏        | 382/3086 [12:48<1:35:25,  2.12s/it] 12%|█▏        | 383/3086 [12:50<1:36:00,  2.13s/it] 12%|█▏        | 384/3086 [12:52<1:36:50,  2.15s/it] 12%|█▏        | 385/3086 [12:54<1:35:44,  2.13s/it] 13%|█▎        | 386/3086 [12:56<1:34:52,  2.11s/it] 13%|█▎        | 387/3086 [12:58<1:36:57,  2.16s/it] 13%|█▎        | 388/3086 [13:00<1:31:57,  2.05s/it] 13%|█▎        | 389/3086 [13:02<1:33:05,  2.07s/it] 13%|█▎        | 390/3086 [13:04<1:26:53,  1.93s/it]                                                    {'loss': 0.6278, 'grad_norm': 0.6536523699760437, 'learning_rate': 0.0001747245625405055, 'epoch': 0.13}
 13%|█▎        | 390/3086 [13:04<1:26:53,  1.93s/it] 13%|█▎        | 391/3086 [13:07<1:39:44,  2.22s/it] 13%|█▎        | 392/3086 [13:09<1:39:03,  2.21s/it] 13%|█▎        | 393/3086 [13:11<1:33:40,  2.09s/it] 13%|█▎        | 394/3086 [13:13<1:37:00,  2.16s/it] 13%|█▎        | 395/3086 [13:15<1:35:55,  2.14s/it] 13%|█▎        | 396/3086 [13:17<1:33:38,  2.09s/it] 13%|█▎        | 397/3086 [13:19<1:25:28,  1.91s/it] 13%|█▎        | 398/3086 [13:20<1:21:45,  1.83s/it] 13%|█▎        | 399/3086 [13:22<1:25:19,  1.91s/it] 13%|█▎        | 400/3086 [13:25<1:30:31,  2.02s/it]                                                    {'loss': 0.6223, 'grad_norm': 0.6199556589126587, 'learning_rate': 0.0001740764744005185, 'epoch': 0.13}
 13%|█▎        | 400/3086 [13:25<1:30:31,  2.02s/it] 13%|█▎        | 401/3086 [13:27<1:32:39,  2.07s/it] 13%|█▎        | 402/3086 [13:29<1:33:26,  2.09s/it] 13%|█▎        | 403/3086 [13:31<1:28:30,  1.98s/it] 13%|█▎        | 404/3086 [13:33<1:33:52,  2.10s/it] 13%|█▎        | 405/3086 [13:35<1:33:18,  2.09s/it] 13%|█▎        | 406/3086 [13:37<1:35:44,  2.14s/it] 13%|█▎        | 407/3086 [13:40<1:38:17,  2.20s/it] 13%|█▎        | 408/3086 [13:42<1:36:40,  2.17s/it] 13%|█▎        | 409/3086 [13:44<1:33:52,  2.10s/it] 13%|█▎        | 410/3086 [13:45<1:26:25,  1.94s/it]                                                    {'loss': 0.6027, 'grad_norm': 0.5260212421417236, 'learning_rate': 0.00017342838626053145, 'epoch': 0.13}
 13%|█▎        | 410/3086 [13:45<1:26:25,  1.94s/it] 13%|█▎        | 411/3086 [13:48<1:31:11,  2.05s/it] 13%|█▎        | 412/3086 [13:49<1:27:08,  1.96s/it] 13%|█▎        | 413/3086 [13:51<1:24:20,  1.89s/it] 13%|█▎        | 414/3086 [13:53<1:22:42,  1.86s/it] 13%|█▎        | 415/3086 [13:55<1:28:26,  1.99s/it] 13%|█▎        | 416/3086 [13:57<1:33:05,  2.09s/it] 14%|█▎        | 417/3086 [13:59<1:28:23,  1.99s/it] 14%|█▎        | 418/3086 [14:01<1:27:57,  1.98s/it] 14%|█▎        | 419/3086 [14:03<1:29:46,  2.02s/it] 14%|█▎        | 420/3086 [14:06<1:37:50,  2.20s/it]                                                    {'loss': 0.6139, 'grad_norm': 0.578906238079071, 'learning_rate': 0.0001727802981205444, 'epoch': 0.14}
 14%|█▎        | 420/3086 [14:06<1:37:50,  2.20s/it] 14%|█▎        | 421/3086 [14:08<1:33:07,  2.10s/it] 14%|█▎        | 422/3086 [14:10<1:30:17,  2.03s/it] 14%|█▎        | 423/3086 [14:11<1:24:34,  1.91s/it] 14%|█▎        | 424/3086 [14:14<1:31:29,  2.06s/it] 14%|█▍        | 425/3086 [14:16<1:38:06,  2.21s/it] 14%|█▍        | 426/3086 [14:18<1:32:13,  2.08s/it] 14%|█▍        | 427/3086 [14:20<1:37:12,  2.19s/it] 14%|█▍        | 428/3086 [14:22<1:32:11,  2.08s/it] 14%|█▍        | 429/3086 [14:24<1:32:21,  2.09s/it] 14%|█▍        | 430/3086 [14:27<1:33:44,  2.12s/it]                                                    {'loss': 0.6183, 'grad_norm': 0.5011010766029358, 'learning_rate': 0.00017213220998055738, 'epoch': 0.14}
 14%|█▍        | 430/3086 [14:27<1:33:44,  2.12s/it] 14%|█▍        | 431/3086 [14:28<1:30:10,  2.04s/it] 14%|█▍        | 432/3086 [14:31<1:37:34,  2.21s/it] 14%|█▍        | 433/3086 [14:33<1:33:10,  2.11s/it] 14%|█▍        | 434/3086 [14:35<1:29:43,  2.03s/it] 14%|█▍        | 435/3086 [14:37<1:35:45,  2.17s/it] 14%|█▍        | 436/3086 [14:39<1:31:58,  2.08s/it] 14%|█▍        | 437/3086 [14:42<1:36:46,  2.19s/it] 14%|█▍        | 438/3086 [14:43<1:30:04,  2.04s/it] 14%|█▍        | 439/3086 [14:45<1:29:53,  2.04s/it] 14%|█▍        | 440/3086 [14:47<1:25:25,  1.94s/it]                                                    {'loss': 0.5985, 'grad_norm': 0.7994365692138672, 'learning_rate': 0.00017148412184057033, 'epoch': 0.14}
 14%|█▍        | 440/3086 [14:47<1:25:25,  1.94s/it] 14%|█▍        | 441/3086 [14:49<1:31:47,  2.08s/it] 14%|█▍        | 442/3086 [14:51<1:27:52,  1.99s/it] 14%|█▍        | 443/3086 [14:54<1:32:53,  2.11s/it] 14%|█▍        | 444/3086 [14:56<1:35:31,  2.17s/it] 14%|█▍        | 445/3086 [14:58<1:29:41,  2.04s/it] 14%|█▍        | 446/3086 [15:00<1:27:38,  1.99s/it] 14%|█▍        | 447/3086 [15:01<1:26:59,  1.98s/it] 15%|█▍        | 448/3086 [15:03<1:22:45,  1.88s/it] 15%|█▍        | 449/3086 [15:05<1:24:34,  1.92s/it] 15%|█▍        | 450/3086 [15:07<1:19:45,  1.82s/it]                                                    {'loss': 0.6162, 'grad_norm': 0.7697229385375977, 'learning_rate': 0.0001708360337005833, 'epoch': 0.15}
 15%|█▍        | 450/3086 [15:07<1:19:45,  1.82s/it] 15%|█▍        | 451/3086 [15:09<1:23:52,  1.91s/it] 15%|█▍        | 452/3086 [15:10<1:17:59,  1.78s/it] 15%|█▍        | 453/3086 [15:12<1:19:31,  1.81s/it] 15%|█▍        | 454/3086 [15:14<1:23:54,  1.91s/it] 15%|█▍        | 455/3086 [15:17<1:27:35,  2.00s/it] 15%|█▍        | 456/3086 [15:19<1:28:44,  2.02s/it] 15%|█▍        | 457/3086 [15:21<1:33:47,  2.14s/it] 15%|█▍        | 458/3086 [15:23<1:28:15,  2.02s/it] 15%|█▍        | 459/3086 [15:24<1:23:43,  1.91s/it] 15%|█▍        | 460/3086 [15:26<1:21:28,  1.86s/it]                                                    {'loss': 0.612, 'grad_norm': 0.47463691234588623, 'learning_rate': 0.00017018794556059624, 'epoch': 0.15}
 15%|█▍        | 460/3086 [15:26<1:21:28,  1.86s/it] 15%|█▍        | 461/3086 [15:28<1:23:46,  1.92s/it] 15%|█▍        | 462/3086 [15:30<1:20:19,  1.84s/it] 15%|█▌        | 463/3086 [15:32<1:19:57,  1.83s/it] 15%|█▌        | 464/3086 [15:34<1:27:41,  2.01s/it] 15%|█▌        | 465/3086 [15:36<1:30:32,  2.07s/it] 15%|█▌        | 466/3086 [15:39<1:32:46,  2.12s/it] 15%|█▌        | 467/3086 [15:40<1:27:01,  1.99s/it] 15%|█▌        | 468/3086 [15:43<1:31:55,  2.11s/it] 15%|█▌        | 469/3086 [15:44<1:26:00,  1.97s/it] 15%|█▌        | 470/3086 [15:47<1:29:26,  2.05s/it]                                                    {'loss': 0.6099, 'grad_norm': 0.4867801070213318, 'learning_rate': 0.0001695398574206092, 'epoch': 0.15}
 15%|█▌        | 470/3086 [15:47<1:29:26,  2.05s/it] 15%|█▌        | 471/3086 [15:48<1:23:08,  1.91s/it] 15%|█▌        | 472/3086 [15:50<1:21:27,  1.87s/it] 15%|█▌        | 473/3086 [15:52<1:19:10,  1.82s/it] 15%|█▌        | 474/3086 [15:53<1:20:11,  1.84s/it] 15%|█▌        | 475/3086 [15:56<1:22:49,  1.90s/it] 15%|█▌        | 476/3086 [15:57<1:21:00,  1.86s/it] 15%|█▌        | 477/3086 [15:59<1:24:07,  1.93s/it] 15%|█▌        | 478/3086 [16:02<1:26:42,  1.99s/it] 16%|█▌        | 479/3086 [16:04<1:26:52,  2.00s/it] 16%|█▌        | 480/3086 [16:06<1:28:07,  2.03s/it]                                                    {'loss': 0.6159, 'grad_norm': 0.5213855504989624, 'learning_rate': 0.00016889176928062217, 'epoch': 0.16}
 16%|█▌        | 480/3086 [16:06<1:28:07,  2.03s/it] 16%|█▌        | 481/3086 [16:07<1:25:51,  1.98s/it] 16%|█▌        | 482/3086 [16:09<1:24:26,  1.95s/it] 16%|█▌        | 483/3086 [16:11<1:24:34,  1.95s/it] 16%|█▌        | 484/3086 [16:13<1:25:01,  1.96s/it] 16%|█▌        | 485/3086 [16:15<1:24:57,  1.96s/it] 16%|█▌        | 486/3086 [16:17<1:23:43,  1.93s/it] 16%|█▌        | 487/3086 [16:19<1:20:47,  1.86s/it] 16%|█▌        | 488/3086 [16:21<1:19:45,  1.84s/it] 16%|█▌        | 489/3086 [16:23<1:21:58,  1.89s/it] 16%|█▌        | 490/3086 [16:25<1:25:23,  1.97s/it]                                                    {'loss': 0.6028, 'grad_norm': 0.6055184602737427, 'learning_rate': 0.00016824368114063513, 'epoch': 0.16}
 16%|█▌        | 490/3086 [16:25<1:25:23,  1.97s/it] 16%|█▌        | 491/3086 [16:27<1:24:46,  1.96s/it] 16%|█▌        | 492/3086 [16:29<1:22:48,  1.92s/it] 16%|█▌        | 493/3086 [16:30<1:19:30,  1.84s/it] 16%|█▌        | 494/3086 [16:32<1:22:07,  1.90s/it] 16%|█▌        | 495/3086 [16:34<1:18:02,  1.81s/it] 16%|█▌        | 496/3086 [16:36<1:21:31,  1.89s/it] 16%|█▌        | 497/3086 [16:37<1:16:20,  1.77s/it] 16%|█▌        | 498/3086 [16:39<1:17:35,  1.80s/it] 16%|█▌        | 499/3086 [16:41<1:18:16,  1.82s/it] 16%|█▌        | 500/3086 [16:44<1:32:36,  2.15s/it]                                                    {'loss': 0.6149, 'grad_norm': 0.4618006646633148, 'learning_rate': 0.00016759559300064808, 'epoch': 0.16}
 16%|█▌        | 500/3086 [16:44<1:32:36,  2.15s/it] 16%|█▌        | 501/3086 [16:46<1:29:01,  2.07s/it] 16%|█▋        | 502/3086 [16:48<1:23:59,  1.95s/it] 16%|█▋        | 503/3086 [16:49<1:19:59,  1.86s/it] 16%|█▋        | 504/3086 [16:51<1:18:44,  1.83s/it] 16%|█▋        | 505/3086 [16:53<1:24:16,  1.96s/it] 16%|█▋        | 506/3086 [16:55<1:22:06,  1.91s/it] 16%|█▋        | 507/3086 [16:57<1:24:27,  1.96s/it] 16%|█▋        | 508/3086 [16:59<1:23:39,  1.95s/it] 16%|█▋        | 509/3086 [17:01<1:20:59,  1.89s/it] 17%|█▋        | 510/3086 [17:03<1:19:25,  1.85s/it]                                                    {'loss': 0.6111, 'grad_norm': 0.5570126175880432, 'learning_rate': 0.00016694750486066106, 'epoch': 0.17}
 17%|█▋        | 510/3086 [17:03<1:19:25,  1.85s/it] 17%|█▋        | 511/3086 [17:04<1:17:21,  1.80s/it] 17%|█▋        | 512/3086 [17:06<1:17:31,  1.81s/it] 17%|█▋        | 513/3086 [17:08<1:18:18,  1.83s/it] 17%|█▋        | 514/3086 [17:10<1:22:25,  1.92s/it] 17%|█▋        | 515/3086 [17:12<1:20:19,  1.87s/it] 17%|█▋        | 516/3086 [17:15<1:35:59,  2.24s/it] 17%|█▋        | 517/3086 [17:17<1:27:13,  2.04s/it] 17%|█▋        | 518/3086 [17:19<1:28:22,  2.06s/it] 17%|█▋        | 519/3086 [17:21<1:35:05,  2.22s/it] 17%|█▋        | 520/3086 [17:23<1:31:41,  2.14s/it]                                                    {'loss': 0.6017, 'grad_norm': 0.641956090927124, 'learning_rate': 0.00016629941672067401, 'epoch': 0.17}
 17%|█▋        | 520/3086 [17:23<1:31:41,  2.14s/it] 17%|█▋        | 521/3086 [17:25<1:31:43,  2.15s/it] 17%|█▋        | 522/3086 [17:28<1:35:48,  2.24s/it] 17%|█▋        | 523/3086 [17:30<1:38:04,  2.30s/it] 17%|█▋        | 524/3086 [17:32<1:37:30,  2.28s/it] 17%|█▋        | 525/3086 [17:34<1:33:49,  2.20s/it] 17%|█▋        | 526/3086 [17:36<1:30:37,  2.12s/it] 17%|█▋        | 527/3086 [17:39<1:34:29,  2.22s/it] 17%|█▋        | 528/3086 [17:41<1:35:36,  2.24s/it] 17%|█▋        | 529/3086 [17:43<1:34:33,  2.22s/it] 17%|█▋        | 530/3086 [17:45<1:32:56,  2.18s/it]                                                    {'loss': 0.6036, 'grad_norm': 0.6085080504417419, 'learning_rate': 0.00016565132858068697, 'epoch': 0.17}
 17%|█▋        | 530/3086 [17:45<1:32:56,  2.18s/it] 17%|█▋        | 531/3086 [17:48<1:33:16,  2.19s/it] 17%|█▋        | 532/3086 [17:50<1:32:28,  2.17s/it] 17%|█▋        | 533/3086 [17:52<1:32:09,  2.17s/it] 17%|█▋        | 534/3086 [17:54<1:31:12,  2.14s/it] 17%|█▋        | 535/3086 [17:56<1:34:56,  2.23s/it] 17%|█▋        | 536/3086 [17:58<1:30:00,  2.12s/it] 17%|█▋        | 537/3086 [18:00<1:23:35,  1.97s/it] 17%|█▋        | 538/3086 [18:02<1:23:04,  1.96s/it] 17%|█▋        | 539/3086 [18:04<1:25:47,  2.02s/it] 17%|█▋        | 540/3086 [18:06<1:21:22,  1.92s/it]                                                    {'loss': 0.6175, 'grad_norm': 0.5306702852249146, 'learning_rate': 0.00016500324044069995, 'epoch': 0.17}
 17%|█▋        | 540/3086 [18:06<1:21:22,  1.92s/it] 18%|█▊        | 541/3086 [18:08<1:22:51,  1.95s/it] 18%|█▊        | 542/3086 [18:10<1:26:29,  2.04s/it] 18%|█▊        | 543/3086 [18:12<1:25:10,  2.01s/it] 18%|█▊        | 544/3086 [18:14<1:21:16,  1.92s/it] 18%|█▊        | 545/3086 [18:15<1:19:11,  1.87s/it] 18%|█▊        | 546/3086 [18:17<1:16:57,  1.82s/it] 18%|█▊        | 547/3086 [18:19<1:13:20,  1.73s/it] 18%|█▊        | 548/3086 [18:21<1:21:11,  1.92s/it] 18%|█▊        | 549/3086 [18:23<1:17:14,  1.83s/it] 18%|█▊        | 550/3086 [18:24<1:17:17,  1.83s/it]                                                    {'loss': 0.6053, 'grad_norm': 0.3849874436855316, 'learning_rate': 0.0001643551523007129, 'epoch': 0.18}
 18%|█▊        | 550/3086 [18:24<1:17:17,  1.83s/it] 18%|█▊        | 551/3086 [18:26<1:20:17,  1.90s/it] 18%|█▊        | 552/3086 [18:28<1:16:49,  1.82s/it] 18%|█▊        | 553/3086 [18:30<1:16:53,  1.82s/it] 18%|█▊        | 554/3086 [18:32<1:13:45,  1.75s/it] 18%|█▊        | 555/3086 [18:33<1:13:35,  1.74s/it] 18%|█▊        | 556/3086 [18:35<1:12:01,  1.71s/it] 18%|█▊        | 557/3086 [18:37<1:15:28,  1.79s/it] 18%|█▊        | 558/3086 [18:39<1:15:32,  1.79s/it] 18%|█▊        | 559/3086 [18:41<1:17:06,  1.83s/it] 18%|█▊        | 560/3086 [18:43<1:21:57,  1.95s/it]                                                    {'loss': 0.5975, 'grad_norm': 0.39800775051116943, 'learning_rate': 0.00016370706416072588, 'epoch': 0.18}
 18%|█▊        | 560/3086 [18:43<1:21:57,  1.95s/it] 18%|█▊        | 561/3086 [18:45<1:20:44,  1.92s/it] 18%|█▊        | 562/3086 [18:47<1:21:03,  1.93s/it] 18%|█▊        | 563/3086 [18:48<1:20:32,  1.92s/it] 18%|█▊        | 564/3086 [18:50<1:19:44,  1.90s/it] 18%|█▊        | 565/3086 [18:52<1:17:27,  1.84s/it] 18%|█▊        | 566/3086 [18:54<1:17:03,  1.83s/it] 18%|█▊        | 567/3086 [18:56<1:17:04,  1.84s/it] 18%|█▊        | 568/3086 [18:58<1:17:25,  1.85s/it] 18%|█▊        | 569/3086 [19:00<1:18:22,  1.87s/it] 18%|█▊        | 570/3086 [19:01<1:16:29,  1.82s/it]                                                    {'loss': 0.6052, 'grad_norm': 0.7687283158302307, 'learning_rate': 0.00016305897602073884, 'epoch': 0.18}
 18%|█▊        | 570/3086 [19:01<1:16:29,  1.82s/it] 19%|█▊        | 571/3086 [19:03<1:20:37,  1.92s/it] 19%|█▊        | 572/3086 [19:05<1:19:04,  1.89s/it] 19%|█▊        | 573/3086 [19:07<1:18:03,  1.86s/it] 19%|█▊        | 574/3086 [19:09<1:25:12,  2.04s/it] 19%|█▊        | 575/3086 [19:12<1:32:47,  2.22s/it] 19%|█▊        | 576/3086 [19:14<1:24:47,  2.03s/it] 19%|█▊        | 577/3086 [19:16<1:26:52,  2.08s/it] 19%|█▊        | 578/3086 [19:18<1:25:36,  2.05s/it] 19%|█▉        | 579/3086 [19:20<1:27:11,  2.09s/it] 19%|█▉        | 580/3086 [19:22<1:20:50,  1.94s/it]                                                    {'loss': 0.6093, 'grad_norm': 0.6331831216812134, 'learning_rate': 0.0001624108878807518, 'epoch': 0.19}
 19%|█▉        | 580/3086 [19:22<1:20:50,  1.94s/it] 19%|█▉        | 581/3086 [19:24<1:22:53,  1.99s/it] 19%|█▉        | 582/3086 [19:25<1:19:19,  1.90s/it] 19%|█▉        | 583/3086 [19:28<1:22:34,  1.98s/it] 19%|█▉        | 584/3086 [19:30<1:25:44,  2.06s/it] 19%|█▉        | 585/3086 [19:31<1:21:05,  1.95s/it] 19%|█▉        | 586/3086 [19:34<1:24:59,  2.04s/it] 19%|█▉        | 587/3086 [19:36<1:23:23,  2.00s/it] 19%|█▉        | 588/3086 [19:38<1:23:11,  2.00s/it] 19%|█▉        | 589/3086 [19:40<1:23:21,  2.00s/it] 19%|█▉        | 590/3086 [19:42<1:24:48,  2.04s/it]                                                    {'loss': 0.5969, 'grad_norm': 0.6400271058082581, 'learning_rate': 0.00016176279974076477, 'epoch': 0.19}
 19%|█▉        | 590/3086 [19:42<1:24:48,  2.04s/it] 19%|█▉        | 591/3086 [19:43<1:19:43,  1.92s/it] 19%|█▉        | 592/3086 [19:45<1:18:16,  1.88s/it] 19%|█▉        | 593/3086 [19:47<1:22:09,  1.98s/it] 19%|█▉        | 594/3086 [19:49<1:20:55,  1.95s/it] 19%|█▉        | 595/3086 [19:51<1:22:25,  1.99s/it] 19%|█▉        | 596/3086 [19:53<1:19:54,  1.93s/it] 19%|█▉        | 597/3086 [19:55<1:22:39,  1.99s/it] 19%|█▉        | 598/3086 [19:57<1:22:46,  2.00s/it] 19%|█▉        | 599/3086 [20:00<1:31:38,  2.21s/it] 19%|█▉        | 600/3086 [20:02<1:33:42,  2.26s/it]                                                    {'loss': 0.6023, 'grad_norm': 0.40558695793151855, 'learning_rate': 0.00016111471160077772, 'epoch': 0.19}
 19%|█▉        | 600/3086 [20:02<1:33:42,  2.26s/it] 19%|█▉        | 601/3086 [20:05<1:32:47,  2.24s/it] 20%|█▉        | 602/3086 [20:06<1:27:38,  2.12s/it] 20%|█▉        | 603/3086 [20:09<1:31:05,  2.20s/it] 20%|█▉        | 604/3086 [20:11<1:34:43,  2.29s/it] 20%|█▉        | 605/3086 [20:13<1:30:07,  2.18s/it] 20%|█▉        | 606/3086 [20:15<1:26:24,  2.09s/it] 20%|█▉        | 607/3086 [20:17<1:30:00,  2.18s/it] 20%|█▉        | 608/3086 [20:20<1:39:51,  2.42s/it] 20%|█▉        | 609/3086 [20:22<1:31:03,  2.21s/it] 20%|█▉        | 610/3086 [20:24<1:29:43,  2.17s/it]                                                    {'loss': 0.5975, 'grad_norm': 0.4487569332122803, 'learning_rate': 0.00016046662346079068, 'epoch': 0.2}
 20%|█▉        | 610/3086 [20:24<1:29:43,  2.17s/it] 20%|█▉        | 611/3086 [20:27<1:31:23,  2.22s/it] 20%|█▉        | 612/3086 [20:29<1:31:22,  2.22s/it] 20%|█▉        | 613/3086 [20:31<1:27:18,  2.12s/it] 20%|█▉        | 614/3086 [20:34<1:36:12,  2.34s/it] 20%|█▉        | 615/3086 [20:36<1:35:59,  2.33s/it] 20%|█▉        | 616/3086 [20:38<1:29:45,  2.18s/it] 20%|█▉        | 617/3086 [20:39<1:22:55,  2.01s/it] 20%|██        | 618/3086 [20:42<1:25:45,  2.09s/it] 20%|██        | 619/3086 [20:43<1:22:39,  2.01s/it] 20%|██        | 620/3086 [20:45<1:19:15,  1.93s/it]                                                    {'loss': 0.6123, 'grad_norm': 0.5041390061378479, 'learning_rate': 0.00015981853532080363, 'epoch': 0.2}
 20%|██        | 620/3086 [20:45<1:19:15,  1.93s/it] 20%|██        | 621/3086 [20:47<1:17:47,  1.89s/it] 20%|██        | 622/3086 [20:49<1:24:49,  2.07s/it] 20%|██        | 623/3086 [20:51<1:24:10,  2.05s/it] 20%|██        | 624/3086 [20:53<1:20:52,  1.97s/it] 20%|██        | 625/3086 [20:55<1:20:35,  1.96s/it] 20%|██        | 626/3086 [20:57<1:15:51,  1.85s/it] 20%|██        | 627/3086 [20:59<1:20:30,  1.96s/it] 20%|██        | 628/3086 [21:01<1:20:02,  1.95s/it] 20%|██        | 629/3086 [21:03<1:23:25,  2.04s/it] 20%|██        | 630/3086 [21:05<1:24:33,  2.07s/it]                                                    {'loss': 0.6029, 'grad_norm': 0.7707961797714233, 'learning_rate': 0.00015917044718081658, 'epoch': 0.2}
 20%|██        | 630/3086 [21:05<1:24:33,  2.07s/it] 20%|██        | 631/3086 [21:07<1:22:29,  2.02s/it] 20%|██        | 632/3086 [21:09<1:25:18,  2.09s/it] 21%|██        | 633/3086 [21:11<1:22:21,  2.01s/it] 21%|██        | 634/3086 [21:14<1:29:49,  2.20s/it] 21%|██        | 635/3086 [21:16<1:27:57,  2.15s/it] 21%|██        | 636/3086 [21:18<1:24:52,  2.08s/it] 21%|██        | 637/3086 [21:20<1:28:29,  2.17s/it] 21%|██        | 638/3086 [21:23<1:33:40,  2.30s/it] 21%|██        | 639/3086 [21:25<1:34:21,  2.31s/it] 21%|██        | 640/3086 [21:27<1:26:14,  2.12s/it]                                                    {'loss': 0.5999, 'grad_norm': 0.5627627372741699, 'learning_rate': 0.00015852235904082956, 'epoch': 0.21}
 21%|██        | 640/3086 [21:27<1:26:14,  2.12s/it] 21%|██        | 641/3086 [21:29<1:22:09,  2.02s/it] 21%|██        | 642/3086 [21:31<1:22:57,  2.04s/it] 21%|██        | 643/3086 [21:33<1:27:30,  2.15s/it] 21%|██        | 644/3086 [21:35<1:26:53,  2.13s/it] 21%|██        | 645/3086 [21:37<1:27:45,  2.16s/it] 21%|██        | 646/3086 [21:40<1:29:17,  2.20s/it] 21%|██        | 647/3086 [21:42<1:32:24,  2.27s/it] 21%|██        | 648/3086 [21:44<1:28:51,  2.19s/it] 21%|██        | 649/3086 [21:46<1:22:32,  2.03s/it] 21%|██        | 650/3086 [21:48<1:20:43,  1.99s/it]                                                    {'loss': 0.6097, 'grad_norm': 0.4235589802265167, 'learning_rate': 0.00015787427090084252, 'epoch': 0.21}
 21%|██        | 650/3086 [21:48<1:20:43,  1.99s/it] 21%|██        | 651/3086 [21:49<1:15:49,  1.87s/it] 21%|██        | 652/3086 [21:51<1:13:46,  1.82s/it] 21%|██        | 653/3086 [21:53<1:14:56,  1.85s/it] 21%|██        | 654/3086 [21:55<1:15:01,  1.85s/it] 21%|██        | 655/3086 [21:57<1:20:08,  1.98s/it] 21%|██▏       | 656/3086 [21:59<1:18:40,  1.94s/it] 21%|██▏       | 657/3086 [22:01<1:20:43,  1.99s/it] 21%|██▏       | 658/3086 [22:03<1:23:44,  2.07s/it] 21%|██▏       | 659/3086 [22:06<1:36:11,  2.38s/it] 21%|██▏       | 660/3086 [22:08<1:29:55,  2.22s/it]                                                    {'loss': 0.5955, 'grad_norm': 0.40963366627693176, 'learning_rate': 0.00015722618276085547, 'epoch': 0.21}
 21%|██▏       | 660/3086 [22:08<1:29:55,  2.22s/it] 21%|██▏       | 661/3086 [22:10<1:25:18,  2.11s/it] 21%|██▏       | 662/3086 [22:12<1:20:40,  2.00s/it] 21%|██▏       | 663/3086 [22:14<1:17:04,  1.91s/it] 22%|██▏       | 664/3086 [22:16<1:19:03,  1.96s/it] 22%|██▏       | 665/3086 [22:18<1:20:18,  1.99s/it] 22%|██▏       | 666/3086 [22:19<1:18:08,  1.94s/it] 22%|██▏       | 667/3086 [22:22<1:25:19,  2.12s/it] 22%|██▏       | 668/3086 [22:25<1:33:04,  2.31s/it] 22%|██▏       | 669/3086 [22:27<1:32:35,  2.30s/it] 22%|██▏       | 670/3086 [22:29<1:28:00,  2.19s/it]                                                    {'loss': 0.5993, 'grad_norm': 0.4703250229358673, 'learning_rate': 0.00015657809462086845, 'epoch': 0.22}
 22%|██▏       | 670/3086 [22:29<1:28:00,  2.19s/it] 22%|██▏       | 671/3086 [22:31<1:26:06,  2.14s/it] 22%|██▏       | 672/3086 [22:33<1:22:52,  2.06s/it] 22%|██▏       | 673/3086 [22:35<1:23:08,  2.07s/it] 22%|██▏       | 674/3086 [22:37<1:20:11,  1.99s/it] 22%|██▏       | 675/3086 [22:39<1:22:44,  2.06s/it] 22%|██▏       | 676/3086 [22:41<1:19:28,  1.98s/it] 22%|██▏       | 677/3086 [22:42<1:15:31,  1.88s/it] 22%|██▏       | 678/3086 [22:44<1:15:06,  1.87s/it] 22%|██▏       | 679/3086 [22:46<1:15:57,  1.89s/it] 22%|██▏       | 680/3086 [22:48<1:14:52,  1.87s/it]                                                    {'loss': 0.5979, 'grad_norm': 0.40009403228759766, 'learning_rate': 0.0001559300064808814, 'epoch': 0.22}
 22%|██▏       | 680/3086 [22:48<1:14:52,  1.87s/it] 22%|██▏       | 681/3086 [22:50<1:13:13,  1.83s/it] 22%|██▏       | 682/3086 [22:51<1:11:37,  1.79s/it] 22%|██▏       | 683/3086 [22:54<1:16:58,  1.92s/it] 22%|██▏       | 684/3086 [22:55<1:13:16,  1.83s/it] 22%|██▏       | 685/3086 [22:57<1:11:05,  1.78s/it] 22%|██▏       | 686/3086 [22:59<1:13:01,  1.83s/it] 22%|██▏       | 687/3086 [23:01<1:13:00,  1.83s/it] 22%|██▏       | 688/3086 [23:03<1:15:27,  1.89s/it] 22%|██▏       | 689/3086 [23:04<1:11:30,  1.79s/it] 22%|██▏       | 690/3086 [23:07<1:19:59,  2.00s/it]                                                    {'loss': 0.6061, 'grad_norm': 0.5209850668907166, 'learning_rate': 0.00015528191834089436, 'epoch': 0.22}
 22%|██▏       | 690/3086 [23:07<1:19:59,  2.00s/it] 22%|██▏       | 691/3086 [23:09<1:18:57,  1.98s/it] 22%|██▏       | 692/3086 [23:11<1:22:23,  2.07s/it] 22%|██▏       | 693/3086 [23:13<1:17:51,  1.95s/it] 22%|██▏       | 694/3086 [23:15<1:19:34,  2.00s/it] 23%|██▎       | 695/3086 [23:17<1:18:34,  1.97s/it] 23%|██▎       | 696/3086 [23:19<1:17:16,  1.94s/it] 23%|██▎       | 697/3086 [23:20<1:15:54,  1.91s/it] 23%|██▎       | 698/3086 [23:22<1:17:20,  1.94s/it] 23%|██▎       | 699/3086 [23:24<1:17:20,  1.94s/it] 23%|██▎       | 700/3086 [23:26<1:18:18,  1.97s/it]                                                    {'loss': 0.6033, 'grad_norm': 0.6221922039985657, 'learning_rate': 0.00015463383020090734, 'epoch': 0.23}
 23%|██▎       | 700/3086 [23:26<1:18:18,  1.97s/it] 23%|██▎       | 701/3086 [23:28<1:18:06,  1.96s/it] 23%|██▎       | 702/3086 [23:30<1:18:36,  1.98s/it] 23%|██▎       | 703/3086 [23:32<1:17:59,  1.96s/it] 23%|██▎       | 704/3086 [23:34<1:17:15,  1.95s/it] 23%|██▎       | 705/3086 [23:36<1:16:14,  1.92s/it] 23%|██▎       | 706/3086 [23:38<1:20:10,  2.02s/it] 23%|██▎       | 707/3086 [23:40<1:21:50,  2.06s/it] 23%|██▎       | 708/3086 [23:42<1:18:46,  1.99s/it] 23%|██▎       | 709/3086 [23:44<1:19:21,  2.00s/it] 23%|██▎       | 710/3086 [23:46<1:18:53,  1.99s/it]                                                    {'loss': 0.6006, 'grad_norm': 0.4025300443172455, 'learning_rate': 0.0001539857420609203, 'epoch': 0.23}
 23%|██▎       | 710/3086 [23:46<1:18:53,  1.99s/it] 23%|██▎       | 711/3086 [23:48<1:19:28,  2.01s/it] 23%|██▎       | 712/3086 [23:50<1:18:30,  1.98s/it] 23%|██▎       | 713/3086 [23:53<1:21:39,  2.06s/it] 23%|██▎       | 714/3086 [23:54<1:18:13,  1.98s/it] 23%|██▎       | 715/3086 [23:56<1:15:43,  1.92s/it] 23%|██▎       | 716/3086 [23:58<1:12:10,  1.83s/it] 23%|██▎       | 717/3086 [23:59<1:11:48,  1.82s/it] 23%|██▎       | 718/3086 [24:01<1:12:55,  1.85s/it] 23%|██▎       | 719/3086 [24:03<1:12:34,  1.84s/it] 23%|██▎       | 720/3086 [24:05<1:12:30,  1.84s/it]                                                    {'loss': 0.5992, 'grad_norm': 0.4479903280735016, 'learning_rate': 0.00015333765392093324, 'epoch': 0.23}
 23%|██▎       | 720/3086 [24:05<1:12:30,  1.84s/it] 23%|██▎       | 721/3086 [24:07<1:14:04,  1.88s/it] 23%|██▎       | 722/3086 [24:10<1:21:19,  2.06s/it] 23%|██▎       | 723/3086 [24:11<1:16:24,  1.94s/it] 23%|██▎       | 724/3086 [24:13<1:19:17,  2.01s/it] 23%|██▎       | 725/3086 [24:15<1:15:32,  1.92s/it] 24%|██▎       | 726/3086 [24:17<1:13:04,  1.86s/it] 24%|██▎       | 727/3086 [24:19<1:15:19,  1.92s/it] 24%|██▎       | 728/3086 [24:21<1:13:39,  1.87s/it] 24%|██▎       | 729/3086 [24:23<1:15:41,  1.93s/it] 24%|██▎       | 730/3086 [24:25<1:18:18,  1.99s/it]                                                    {'loss': 0.6043, 'grad_norm': 0.5445745587348938, 'learning_rate': 0.00015268956578094622, 'epoch': 0.24}
 24%|██▎       | 730/3086 [24:25<1:18:18,  1.99s/it] 24%|██▎       | 731/3086 [24:27<1:20:32,  2.05s/it] 24%|██▎       | 732/3086 [24:29<1:19:24,  2.02s/it] 24%|██▍       | 733/3086 [24:31<1:23:42,  2.13s/it] 24%|██▍       | 734/3086 [24:34<1:25:04,  2.17s/it] 24%|██▍       | 735/3086 [24:36<1:27:08,  2.22s/it] 24%|██▍       | 736/3086 [24:38<1:21:40,  2.09s/it] 24%|██▍       | 737/3086 [24:40<1:19:12,  2.02s/it] 24%|██▍       | 738/3086 [24:42<1:21:11,  2.07s/it] 24%|██▍       | 739/3086 [24:44<1:19:40,  2.04s/it] 24%|██▍       | 740/3086 [24:45<1:16:21,  1.95s/it]                                                    {'loss': 0.5917, 'grad_norm': 0.49537190794944763, 'learning_rate': 0.00015204147764095918, 'epoch': 0.24}
 24%|██▍       | 740/3086 [24:45<1:16:21,  1.95s/it] 24%|██▍       | 741/3086 [24:47<1:15:13,  1.92s/it] 24%|██▍       | 742/3086 [24:49<1:16:12,  1.95s/it] 24%|██▍       | 743/3086 [24:51<1:14:14,  1.90s/it] 24%|██▍       | 744/3086 [24:53<1:14:24,  1.91s/it] 24%|██▍       | 745/3086 [24:55<1:14:55,  1.92s/it] 24%|██▍       | 746/3086 [24:57<1:15:12,  1.93s/it] 24%|██▍       | 747/3086 [24:59<1:16:26,  1.96s/it] 24%|██▍       | 748/3086 [25:01<1:21:31,  2.09s/it] 24%|██▍       | 749/3086 [25:03<1:16:03,  1.95s/it] 24%|██▍       | 750/3086 [25:05<1:16:59,  1.98s/it]                                                    {'loss': 0.5821, 'grad_norm': 0.50299072265625, 'learning_rate': 0.00015139338950097216, 'epoch': 0.24}
 24%|██▍       | 750/3086 [25:05<1:16:59,  1.98s/it] 24%|██▍       | 751/3086 [25:07<1:15:19,  1.94s/it] 24%|██▍       | 752/3086 [25:09<1:20:14,  2.06s/it] 24%|██▍       | 753/3086 [25:11<1:16:54,  1.98s/it] 24%|██▍       | 754/3086 [25:13<1:20:55,  2.08s/it] 24%|██▍       | 755/3086 [25:15<1:18:23,  2.02s/it] 24%|██▍       | 756/3086 [25:17<1:15:44,  1.95s/it] 25%|██▍       | 757/3086 [25:19<1:14:18,  1.91s/it] 25%|██▍       | 758/3086 [25:20<1:09:48,  1.80s/it] 25%|██▍       | 759/3086 [25:23<1:17:09,  1.99s/it] 25%|██▍       | 760/3086 [25:25<1:20:40,  2.08s/it]                                                    {'loss': 0.6069, 'grad_norm': 0.5544588565826416, 'learning_rate': 0.0001507453013609851, 'epoch': 0.25}
 25%|██▍       | 760/3086 [25:25<1:20:40,  2.08s/it] 25%|██▍       | 761/3086 [25:27<1:22:46,  2.14s/it] 25%|██▍       | 762/3086 [25:30<1:26:55,  2.24s/it] 25%|██▍       | 763/3086 [25:32<1:23:08,  2.15s/it] 25%|██▍       | 764/3086 [25:34<1:18:17,  2.02s/it] 25%|██▍       | 765/3086 [25:35<1:13:52,  1.91s/it] 25%|██▍       | 766/3086 [25:37<1:15:43,  1.96s/it] 25%|██▍       | 767/3086 [25:39<1:13:49,  1.91s/it] 25%|██▍       | 768/3086 [25:41<1:15:17,  1.95s/it] 25%|██▍       | 769/3086 [25:43<1:18:29,  2.03s/it] 25%|██▍       | 770/3086 [25:45<1:14:57,  1.94s/it]                                                    {'loss': 0.6028, 'grad_norm': 0.4650318920612335, 'learning_rate': 0.00015009721322099806, 'epoch': 0.25}
 25%|██▍       | 770/3086 [25:45<1:14:57,  1.94s/it] 25%|██▍       | 771/3086 [25:47<1:14:15,  1.92s/it] 25%|██▌       | 772/3086 [25:50<1:25:34,  2.22s/it] 25%|██▌       | 773/3086 [25:52<1:26:02,  2.23s/it] 25%|██▌       | 774/3086 [25:53<1:15:46,  1.97s/it] 25%|██▌       | 775/3086 [25:56<1:18:27,  2.04s/it] 25%|██▌       | 776/3086 [25:58<1:19:09,  2.06s/it] 25%|██▌       | 777/3086 [25:59<1:14:23,  1.93s/it] 25%|██▌       | 778/3086 [26:01<1:12:30,  1.88s/it] 25%|██▌       | 779/3086 [26:03<1:12:33,  1.89s/it] 25%|██▌       | 780/3086 [26:05<1:10:16,  1.83s/it]                                                    {'loss': 0.5894, 'grad_norm': 0.4905356466770172, 'learning_rate': 0.00014944912508101104, 'epoch': 0.25}
 25%|██▌       | 780/3086 [26:05<1:10:16,  1.83s/it] 25%|██▌       | 781/3086 [26:06<1:08:18,  1.78s/it] 25%|██▌       | 782/3086 [26:08<1:10:29,  1.84s/it] 25%|██▌       | 783/3086 [26:10<1:08:43,  1.79s/it] 25%|██▌       | 784/3086 [26:12<1:12:08,  1.88s/it] 25%|██▌       | 785/3086 [26:14<1:11:32,  1.87s/it] 25%|██▌       | 786/3086 [26:15<1:07:16,  1.76s/it] 26%|██▌       | 787/3086 [26:18<1:12:09,  1.88s/it] 26%|██▌       | 788/3086 [26:19<1:10:39,  1.84s/it] 26%|██▌       | 789/3086 [26:21<1:08:41,  1.79s/it] 26%|██▌       | 790/3086 [26:23<1:09:05,  1.81s/it]                                                    {'loss': 0.5938, 'grad_norm': 0.42389586567878723, 'learning_rate': 0.000148801036941024, 'epoch': 0.26}
 26%|██▌       | 790/3086 [26:23<1:09:05,  1.81s/it] 26%|██▌       | 791/3086 [26:25<1:08:18,  1.79s/it] 26%|██▌       | 792/3086 [26:26<1:07:51,  1.77s/it] 26%|██▌       | 793/3086 [26:29<1:13:14,  1.92s/it] 26%|██▌       | 794/3086 [26:31<1:14:03,  1.94s/it] 26%|██▌       | 795/3086 [26:33<1:17:08,  2.02s/it] 26%|██▌       | 796/3086 [26:35<1:22:18,  2.16s/it] 26%|██▌       | 797/3086 [26:37<1:17:23,  2.03s/it] 26%|██▌       | 798/3086 [26:40<1:22:21,  2.16s/it] 26%|██▌       | 799/3086 [26:41<1:19:16,  2.08s/it] 26%|██▌       | 800/3086 [26:44<1:22:43,  2.17s/it]                                                    {'loss': 0.5852, 'grad_norm': 0.4011171758174896, 'learning_rate': 0.00014815294880103695, 'epoch': 0.26}
 26%|██▌       | 800/3086 [26:44<1:22:43,  2.17s/it] 26%|██▌       | 801/3086 [26:46<1:19:50,  2.10s/it] 26%|██▌       | 802/3086 [26:47<1:15:17,  1.98s/it] 26%|██▌       | 803/3086 [26:50<1:16:38,  2.01s/it] 26%|██▌       | 804/3086 [26:51<1:11:14,  1.87s/it] 26%|██▌       | 805/3086 [26:53<1:14:00,  1.95s/it] 26%|██▌       | 806/3086 [26:56<1:20:22,  2.12s/it] 26%|██▌       | 807/3086 [26:57<1:14:51,  1.97s/it] 26%|██▌       | 808/3086 [26:59<1:12:36,  1.91s/it] 26%|██▌       | 809/3086 [27:01<1:10:22,  1.85s/it] 26%|██▌       | 810/3086 [27:03<1:11:24,  1.88s/it]                                                    {'loss': 0.5875, 'grad_norm': 0.4814859926700592, 'learning_rate': 0.0001475048606610499, 'epoch': 0.26}
 26%|██▌       | 810/3086 [27:03<1:11:24,  1.88s/it] 26%|██▋       | 811/3086 [27:05<1:19:50,  2.11s/it] 26%|██▋       | 812/3086 [27:07<1:17:31,  2.05s/it] 26%|██▋       | 813/3086 [27:09<1:17:50,  2.05s/it] 26%|██▋       | 814/3086 [27:11<1:18:07,  2.06s/it] 26%|██▋       | 815/3086 [27:13<1:16:51,  2.03s/it] 26%|██▋       | 816/3086 [27:16<1:17:41,  2.05s/it] 26%|██▋       | 817/3086 [27:17<1:12:53,  1.93s/it] 27%|██▋       | 818/3086 [27:19<1:12:36,  1.92s/it] 27%|██▋       | 819/3086 [27:21<1:12:59,  1.93s/it] 27%|██▋       | 820/3086 [27:23<1:15:59,  2.01s/it]                                                    {'loss': 0.5892, 'grad_norm': 0.7289398908615112, 'learning_rate': 0.00014685677252106286, 'epoch': 0.27}
 27%|██▋       | 820/3086 [27:23<1:15:59,  2.01s/it] 27%|██▋       | 821/3086 [27:25<1:16:59,  2.04s/it] 27%|██▋       | 822/3086 [27:27<1:12:20,  1.92s/it] 27%|██▋       | 823/3086 [27:29<1:16:30,  2.03s/it] 27%|██▋       | 824/3086 [27:31<1:14:44,  1.98s/it] 27%|██▋       | 825/3086 [27:33<1:18:23,  2.08s/it] 27%|██▋       | 826/3086 [27:35<1:14:43,  1.98s/it] 27%|██▋       | 827/3086 [27:38<1:20:21,  2.13s/it] 27%|██▋       | 828/3086 [27:40<1:24:26,  2.24s/it] 27%|██▋       | 829/3086 [27:42<1:20:31,  2.14s/it] 27%|██▋       | 830/3086 [27:44<1:23:19,  2.22s/it]                                                    {'loss': 0.5779, 'grad_norm': 0.45781928300857544, 'learning_rate': 0.0001462086843810758, 'epoch': 0.27}
 27%|██▋       | 830/3086 [27:44<1:23:19,  2.22s/it] 27%|██▋       | 831/3086 [27:47<1:21:45,  2.18s/it] 27%|██▋       | 832/3086 [27:50<1:33:21,  2.49s/it] 27%|██▋       | 833/3086 [27:51<1:24:02,  2.24s/it] 27%|██▋       | 834/3086 [27:53<1:21:39,  2.18s/it] 27%|██▋       | 835/3086 [27:55<1:19:20,  2.11s/it] 27%|██▋       | 836/3086 [27:57<1:17:46,  2.07s/it] 27%|██▋       | 837/3086 [27:59<1:14:29,  1.99s/it] 27%|██▋       | 838/3086 [28:01<1:16:46,  2.05s/it] 27%|██▋       | 839/3086 [28:04<1:21:47,  2.18s/it] 27%|██▋       | 840/3086 [28:06<1:18:33,  2.10s/it]                                                    {'loss': 0.5926, 'grad_norm': 0.6097092032432556, 'learning_rate': 0.0001455605962410888, 'epoch': 0.27}
 27%|██▋       | 840/3086 [28:06<1:18:33,  2.10s/it] 27%|██▋       | 841/3086 [28:08<1:19:42,  2.13s/it] 27%|██▋       | 842/3086 [28:10<1:15:44,  2.03s/it] 27%|██▋       | 843/3086 [28:12<1:19:37,  2.13s/it] 27%|██▋       | 844/3086 [28:14<1:17:32,  2.08s/it] 27%|██▋       | 845/3086 [28:17<1:23:04,  2.22s/it] 27%|██▋       | 846/3086 [28:19<1:25:54,  2.30s/it] 27%|██▋       | 847/3086 [28:21<1:22:13,  2.20s/it] 27%|██▋       | 848/3086 [28:23<1:17:12,  2.07s/it] 28%|██▊       | 849/3086 [28:25<1:14:17,  1.99s/it] 28%|██▊       | 850/3086 [28:26<1:09:45,  1.87s/it]                                                    {'loss': 0.5818, 'grad_norm': 0.35287871956825256, 'learning_rate': 0.00014491250810110174, 'epoch': 0.28}
 28%|██▊       | 850/3086 [28:26<1:09:45,  1.87s/it] 28%|██▊       | 851/3086 [28:28<1:09:04,  1.85s/it] 28%|██▊       | 852/3086 [28:30<1:09:06,  1.86s/it] 28%|██▊       | 853/3086 [28:32<1:11:09,  1.91s/it] 28%|██▊       | 854/3086 [28:34<1:11:35,  1.92s/it] 28%|██▊       | 855/3086 [28:37<1:20:04,  2.15s/it] 28%|██▊       | 856/3086 [28:39<1:23:12,  2.24s/it] 28%|██▊       | 857/3086 [28:41<1:20:40,  2.17s/it] 28%|██▊       | 858/3086 [28:44<1:24:34,  2.28s/it] 28%|██▊       | 859/3086 [28:45<1:19:52,  2.15s/it] 28%|██▊       | 860/3086 [28:47<1:15:40,  2.04s/it]                                                    {'loss': 0.5891, 'grad_norm': 0.5248051285743713, 'learning_rate': 0.00014426441996111473, 'epoch': 0.28}
 28%|██▊       | 860/3086 [28:47<1:15:40,  2.04s/it] 28%|██▊       | 861/3086 [28:49<1:10:39,  1.91s/it] 28%|██▊       | 862/3086 [28:51<1:12:19,  1.95s/it] 28%|██▊       | 863/3086 [28:53<1:13:33,  1.99s/it] 28%|██▊       | 864/3086 [28:55<1:11:00,  1.92s/it] 28%|██▊       | 865/3086 [28:57<1:09:35,  1.88s/it] 28%|██▊       | 866/3086 [28:59<1:13:56,  2.00s/it] 28%|██▊       | 867/3086 [29:01<1:11:08,  1.92s/it] 28%|██▊       | 868/3086 [29:02<1:11:05,  1.92s/it] 28%|██▊       | 869/3086 [29:04<1:11:38,  1.94s/it] 28%|██▊       | 870/3086 [29:06<1:11:42,  1.94s/it]                                                    {'loss': 0.6002, 'grad_norm': 0.48892349004745483, 'learning_rate': 0.00014361633182112768, 'epoch': 0.28}
 28%|██▊       | 870/3086 [29:06<1:11:42,  1.94s/it] 28%|██▊       | 871/3086 [29:08<1:09:57,  1.89s/it] 28%|██▊       | 872/3086 [29:10<1:10:47,  1.92s/it] 28%|██▊       | 873/3086 [29:12<1:10:57,  1.92s/it] 28%|██▊       | 874/3086 [29:14<1:14:01,  2.01s/it] 28%|██▊       | 875/3086 [29:16<1:13:28,  1.99s/it] 28%|██▊       | 876/3086 [29:19<1:17:41,  2.11s/it] 28%|██▊       | 877/3086 [29:21<1:18:12,  2.12s/it] 28%|██▊       | 878/3086 [29:23<1:19:05,  2.15s/it] 28%|██▊       | 879/3086 [29:25<1:13:49,  2.01s/it] 29%|██▊       | 880/3086 [29:27<1:13:45,  2.01s/it]                                                    {'loss': 0.5932, 'grad_norm': 0.6153582334518433, 'learning_rate': 0.00014296824368114063, 'epoch': 0.29}
 29%|██▊       | 880/3086 [29:27<1:13:45,  2.01s/it] 29%|██▊       | 881/3086 [29:29<1:12:26,  1.97s/it] 29%|██▊       | 882/3086 [29:30<1:08:02,  1.85s/it] 29%|██▊       | 883/3086 [29:32<1:10:51,  1.93s/it] 29%|██▊       | 884/3086 [29:34<1:12:17,  1.97s/it] 29%|██▊       | 885/3086 [29:37<1:19:39,  2.17s/it] 29%|██▊       | 886/3086 [29:38<1:12:15,  1.97s/it] 29%|██▊       | 887/3086 [29:41<1:20:22,  2.19s/it] 29%|██▉       | 888/3086 [29:44<1:25:01,  2.32s/it] 29%|██▉       | 889/3086 [29:46<1:18:33,  2.15s/it] 29%|██▉       | 890/3086 [29:48<1:22:08,  2.24s/it]                                                    {'loss': 0.5848, 'grad_norm': 0.5180894136428833, 'learning_rate': 0.0001423201555411536, 'epoch': 0.29}
 29%|██▉       | 890/3086 [29:48<1:22:08,  2.24s/it] 29%|██▉       | 891/3086 [29:50<1:18:41,  2.15s/it] 29%|██▉       | 892/3086 [29:52<1:16:40,  2.10s/it] 29%|██▉       | 893/3086 [29:54<1:14:17,  2.03s/it] 29%|██▉       | 894/3086 [29:56<1:12:06,  1.97s/it] 29%|██▉       | 895/3086 [29:57<1:09:34,  1.91s/it] 29%|██▉       | 896/3086 [30:00<1:14:57,  2.05s/it] 29%|██▉       | 897/3086 [30:02<1:19:26,  2.18s/it] 29%|██▉       | 898/3086 [30:04<1:18:00,  2.14s/it] 29%|██▉       | 899/3086 [30:06<1:17:27,  2.13s/it] 29%|██▉       | 900/3086 [30:09<1:18:01,  2.14s/it]                                                    {'loss': 0.5859, 'grad_norm': 0.5542094111442566, 'learning_rate': 0.00014167206740116657, 'epoch': 0.29}
 29%|██▉       | 900/3086 [30:09<1:18:01,  2.14s/it] 29%|██▉       | 901/3086 [30:10<1:13:23,  2.02s/it] 29%|██▉       | 902/3086 [30:12<1:10:27,  1.94s/it] 29%|██▉       | 903/3086 [30:14<1:10:22,  1.93s/it] 29%|██▉       | 904/3086 [30:16<1:07:29,  1.86s/it] 29%|██▉       | 905/3086 [30:18<1:09:24,  1.91s/it] 29%|██▉       | 906/3086 [30:19<1:07:04,  1.85s/it] 29%|██▉       | 907/3086 [30:22<1:12:53,  2.01s/it] 29%|██▉       | 908/3086 [30:24<1:20:36,  2.22s/it] 29%|██▉       | 909/3086 [30:26<1:15:03,  2.07s/it] 29%|██▉       | 910/3086 [30:28<1:15:08,  2.07s/it]                                                    {'loss': 0.5996, 'grad_norm': 0.469361275434494, 'learning_rate': 0.00014102397926117952, 'epoch': 0.29}
 29%|██▉       | 910/3086 [30:28<1:15:08,  2.07s/it] 30%|██▉       | 911/3086 [30:30<1:15:59,  2.10s/it] 30%|██▉       | 912/3086 [30:32<1:12:31,  2.00s/it] 30%|██▉       | 913/3086 [30:34<1:11:57,  1.99s/it] 30%|██▉       | 914/3086 [30:36<1:11:33,  1.98s/it] 30%|██▉       | 915/3086 [30:38<1:08:25,  1.89s/it] 30%|██▉       | 916/3086 [30:40<1:13:19,  2.03s/it] 30%|██▉       | 917/3086 [30:42<1:09:38,  1.93s/it] 30%|██▉       | 918/3086 [30:44<1:08:07,  1.89s/it] 30%|██▉       | 919/3086 [30:46<1:08:36,  1.90s/it] 30%|██▉       | 920/3086 [30:47<1:05:36,  1.82s/it]                                                    {'loss': 0.5876, 'grad_norm': 0.5006332993507385, 'learning_rate': 0.0001403758911211925, 'epoch': 0.3}
 30%|██▉       | 920/3086 [30:47<1:05:36,  1.82s/it] 30%|██▉       | 921/3086 [30:49<1:04:20,  1.78s/it] 30%|██▉       | 922/3086 [30:51<1:04:39,  1.79s/it] 30%|██▉       | 923/3086 [30:53<1:07:47,  1.88s/it] 30%|██▉       | 924/3086 [30:55<1:11:31,  1.98s/it] 30%|██▉       | 925/3086 [30:57<1:11:03,  1.97s/it] 30%|███       | 926/3086 [30:59<1:10:56,  1.97s/it] 30%|███       | 927/3086 [31:01<1:12:40,  2.02s/it] 30%|███       | 928/3086 [31:03<1:14:03,  2.06s/it] 30%|███       | 929/3086 [31:05<1:13:19,  2.04s/it] 30%|███       | 930/3086 [31:07<1:13:10,  2.04s/it]                                                    {'loss': 0.5802, 'grad_norm': 0.4623856842517853, 'learning_rate': 0.00013972780298120545, 'epoch': 0.3}
 30%|███       | 930/3086 [31:07<1:13:10,  2.04s/it] 30%|███       | 931/3086 [31:09<1:08:53,  1.92s/it] 30%|███       | 932/3086 [31:11<1:08:01,  1.89s/it] 30%|███       | 933/3086 [31:13<1:10:54,  1.98s/it] 30%|███       | 934/3086 [31:16<1:17:49,  2.17s/it] 30%|███       | 935/3086 [31:18<1:16:57,  2.15s/it] 30%|███       | 936/3086 [31:20<1:15:59,  2.12s/it] 30%|███       | 937/3086 [31:21<1:10:32,  1.97s/it] 30%|███       | 938/3086 [31:23<1:07:59,  1.90s/it] 30%|███       | 939/3086 [31:25<1:09:44,  1.95s/it] 30%|███       | 940/3086 [31:27<1:12:04,  2.02s/it]                                                    {'loss': 0.5924, 'grad_norm': 0.40033823251724243, 'learning_rate': 0.00013907971484121843, 'epoch': 0.3}
 30%|███       | 940/3086 [31:27<1:12:04,  2.02s/it] 30%|███       | 941/3086 [31:30<1:16:03,  2.13s/it] 31%|███       | 942/3086 [31:32<1:21:29,  2.28s/it] 31%|███       | 943/3086 [31:34<1:19:09,  2.22s/it] 31%|███       | 944/3086 [31:37<1:18:51,  2.21s/it] 31%|███       | 945/3086 [31:38<1:14:23,  2.08s/it] 31%|███       | 946/3086 [31:41<1:15:56,  2.13s/it] 31%|███       | 947/3086 [31:42<1:12:37,  2.04s/it] 31%|███       | 948/3086 [31:44<1:12:53,  2.05s/it] 31%|███       | 949/3086 [31:46<1:10:17,  1.97s/it] 31%|███       | 950/3086 [31:49<1:13:42,  2.07s/it]                                                    {'loss': 0.5797, 'grad_norm': 0.4981905519962311, 'learning_rate': 0.00013843162670123139, 'epoch': 0.31}
 31%|███       | 950/3086 [31:49<1:13:42,  2.07s/it] 31%|███       | 951/3086 [31:51<1:14:15,  2.09s/it] 31%|███       | 952/3086 [31:53<1:19:17,  2.23s/it] 31%|███       | 953/3086 [31:55<1:15:05,  2.11s/it] 31%|███       | 954/3086 [31:57<1:13:37,  2.07s/it] 31%|███       | 955/3086 [31:59<1:11:47,  2.02s/it] 31%|███       | 956/3086 [32:01<1:15:07,  2.12s/it] 31%|███       | 957/3086 [32:03<1:15:43,  2.13s/it] 31%|███       | 958/3086 [32:06<1:16:10,  2.15s/it] 31%|███       | 959/3086 [32:08<1:14:52,  2.11s/it] 31%|███       | 960/3086 [32:10<1:15:38,  2.13s/it]                                                    {'loss': 0.578, 'grad_norm': 0.4354812502861023, 'learning_rate': 0.00013778353856124434, 'epoch': 0.31}
 31%|███       | 960/3086 [32:10<1:15:38,  2.13s/it] 31%|███       | 961/3086 [32:12<1:17:21,  2.18s/it] 31%|███       | 962/3086 [32:14<1:13:33,  2.08s/it] 31%|███       | 963/3086 [32:16<1:13:57,  2.09s/it] 31%|███       | 964/3086 [32:18<1:09:39,  1.97s/it] 31%|███▏      | 965/3086 [32:20<1:13:04,  2.07s/it] 31%|███▏      | 966/3086 [32:22<1:16:02,  2.15s/it] 31%|███▏      | 967/3086 [32:25<1:17:21,  2.19s/it] 31%|███▏      | 968/3086 [32:26<1:12:13,  2.05s/it] 31%|███▏      | 969/3086 [32:28<1:07:28,  1.91s/it] 31%|███▏      | 970/3086 [32:30<1:09:21,  1.97s/it]                                                    {'loss': 0.5855, 'grad_norm': 0.48241373896598816, 'learning_rate': 0.0001371354504212573, 'epoch': 0.31}
 31%|███▏      | 970/3086 [32:30<1:09:21,  1.97s/it] 31%|███▏      | 971/3086 [32:32<1:06:50,  1.90s/it] 31%|███▏      | 972/3086 [32:34<1:05:41,  1.86s/it] 32%|███▏      | 973/3086 [32:35<1:03:51,  1.81s/it] 32%|███▏      | 974/3086 [32:38<1:08:12,  1.94s/it] 32%|███▏      | 975/3086 [32:39<1:03:41,  1.81s/it] 32%|███▏      | 976/3086 [32:41<1:05:54,  1.87s/it] 32%|███▏      | 977/3086 [32:43<1:07:11,  1.91s/it] 32%|███▏      | 978/3086 [32:45<1:08:26,  1.95s/it] 32%|███▏      | 979/3086 [32:47<1:07:22,  1.92s/it] 32%|███▏      | 980/3086 [32:49<1:05:17,  1.86s/it]                                                    {'loss': 0.5857, 'grad_norm': 0.4360247254371643, 'learning_rate': 0.00013648736228127025, 'epoch': 0.32}
 32%|███▏      | 980/3086 [32:49<1:05:17,  1.86s/it] 32%|███▏      | 981/3086 [32:51<1:08:12,  1.94s/it] 32%|███▏      | 982/3086 [32:53<1:10:41,  2.02s/it] 32%|███▏      | 983/3086 [32:55<1:11:29,  2.04s/it] 32%|███▏      | 984/3086 [32:57<1:08:07,  1.94s/it] 32%|███▏      | 985/3086 [32:59<1:08:43,  1.96s/it] 32%|███▏      | 986/3086 [33:01<1:08:49,  1.97s/it] 32%|███▏      | 987/3086 [33:03<1:09:55,  2.00s/it] 32%|███▏      | 988/3086 [33:05<1:06:19,  1.90s/it] 32%|███▏      | 989/3086 [33:06<1:06:34,  1.90s/it] 32%|███▏      | 990/3086 [33:08<1:02:38,  1.79s/it]                                                    {'loss': 0.5967, 'grad_norm': 0.46723589301109314, 'learning_rate': 0.0001358392741412832, 'epoch': 0.32}
 32%|███▏      | 990/3086 [33:08<1:02:38,  1.79s/it] 32%|███▏      | 991/3086 [33:10<1:06:01,  1.89s/it] 32%|███▏      | 992/3086 [33:12<1:04:34,  1.85s/it] 32%|███▏      | 993/3086 [33:14<1:06:46,  1.91s/it] 32%|███▏      | 994/3086 [33:16<1:07:15,  1.93s/it] 32%|███▏      | 995/3086 [33:18<1:04:20,  1.85s/it] 32%|███▏      | 996/3086 [33:20<1:06:34,  1.91s/it] 32%|███▏      | 997/3086 [33:22<1:06:03,  1.90s/it] 32%|███▏      | 998/3086 [33:24<1:10:33,  2.03s/it] 32%|███▏      | 999/3086 [33:26<1:07:14,  1.93s/it] 32%|███▏      | 1000/3086 [33:27<1:03:36,  1.83s/it]                                                     {'loss': 0.5863, 'grad_norm': 0.590335488319397, 'learning_rate': 0.00013519118600129618, 'epoch': 0.32}
 32%|███▏      | 1000/3086 [33:27<1:03:36,  1.83s/it] 32%|███▏      | 1001/3086 [33:29<1:08:11,  1.96s/it] 32%|███▏      | 1002/3086 [33:32<1:12:39,  2.09s/it] 33%|███▎      | 1003/3086 [33:33<1:07:35,  1.95s/it] 33%|███▎      | 1004/3086 [33:35<1:03:49,  1.84s/it] 33%|███▎      | 1005/3086 [33:37<1:05:32,  1.89s/it] 33%|███▎      | 1006/3086 [33:39<1:08:24,  1.97s/it] 33%|███▎      | 1007/3086 [33:42<1:13:16,  2.11s/it] 33%|███▎      | 1008/3086 [33:44<1:12:53,  2.10s/it] 33%|███▎      | 1009/3086 [33:46<1:12:46,  2.10s/it] 33%|███▎      | 1010/3086 [33:48<1:11:09,  2.06s/it]                                                     {'loss': 0.5915, 'grad_norm': 0.5669798254966736, 'learning_rate': 0.00013454309786130913, 'epoch': 0.33}
 33%|███▎      | 1010/3086 [33:48<1:11:09,  2.06s/it] 33%|███▎      | 1011/3086 [33:49<1:06:51,  1.93s/it] 33%|███▎      | 1012/3086 [33:52<1:11:11,  2.06s/it] 33%|███▎      | 1013/3086 [33:53<1:07:13,  1.95s/it] 33%|███▎      | 1014/3086 [33:55<1:03:52,  1.85s/it] 33%|███▎      | 1015/3086 [33:58<1:11:01,  2.06s/it] 33%|███▎      | 1016/3086 [33:59<1:06:20,  1.92s/it] 33%|███▎      | 1017/3086 [34:01<1:08:27,  1.99s/it] 33%|███▎      | 1018/3086 [34:03<1:06:11,  1.92s/it] 33%|███▎      | 1019/3086 [34:05<1:06:55,  1.94s/it] 33%|███▎      | 1020/3086 [34:07<1:08:05,  1.98s/it]                                                     {'loss': 0.5904, 'grad_norm': 0.449942022562027, 'learning_rate': 0.00013389500972132209, 'epoch': 0.33}
 33%|███▎      | 1020/3086 [34:07<1:08:05,  1.98s/it] 33%|███▎      | 1021/3086 [34:09<1:04:40,  1.88s/it] 33%|███▎      | 1022/3086 [34:11<1:08:11,  1.98s/it] 33%|███▎      | 1023/3086 [34:13<1:05:34,  1.91s/it] 33%|███▎      | 1024/3086 [34:15<1:08:33,  1.99s/it] 33%|███▎      | 1025/3086 [34:17<1:05:56,  1.92s/it] 33%|███▎      | 1026/3086 [34:18<1:03:32,  1.85s/it] 33%|███▎      | 1027/3086 [34:20<1:01:39,  1.80s/it] 33%|███▎      | 1028/3086 [34:22<1:05:28,  1.91s/it] 33%|███▎      | 1029/3086 [34:25<1:09:49,  2.04s/it] 33%|███▎      | 1030/3086 [34:26<1:05:34,  1.91s/it]                                                     {'loss': 0.5902, 'grad_norm': 0.44293081760406494, 'learning_rate': 0.00013324692158133507, 'epoch': 0.33}
 33%|███▎      | 1030/3086 [34:26<1:05:34,  1.91s/it] 33%|███▎      | 1031/3086 [34:28<1:07:27,  1.97s/it] 33%|███▎      | 1032/3086 [34:30<1:06:25,  1.94s/it] 33%|███▎      | 1033/3086 [34:32<1:03:46,  1.86s/it] 34%|███▎      | 1034/3086 [34:34<1:09:43,  2.04s/it] 34%|███▎      | 1035/3086 [34:36<1:10:47,  2.07s/it] 34%|███▎      | 1036/3086 [34:38<1:08:39,  2.01s/it] 34%|███▎      | 1037/3086 [34:40<1:10:12,  2.06s/it] 34%|███▎      | 1038/3086 [34:42<1:05:19,  1.91s/it] 34%|███▎      | 1039/3086 [34:44<1:01:53,  1.81s/it] 34%|███▎      | 1040/3086 [34:46<1:06:12,  1.94s/it]                                                     {'loss': 0.5887, 'grad_norm': 0.461931049823761, 'learning_rate': 0.00013259883344134802, 'epoch': 0.34}
 34%|███▎      | 1040/3086 [34:46<1:06:12,  1.94s/it] 34%|███▎      | 1041/3086 [34:48<1:04:56,  1.91s/it] 34%|███▍      | 1042/3086 [34:50<1:09:14,  2.03s/it] 34%|███▍      | 1043/3086 [34:52<1:06:11,  1.94s/it] 34%|███▍      | 1044/3086 [34:54<1:06:06,  1.94s/it] 34%|███▍      | 1045/3086 [34:56<1:07:36,  1.99s/it] 34%|███▍      | 1046/3086 [34:58<1:08:38,  2.02s/it] 34%|███▍      | 1047/3086 [35:00<1:07:31,  1.99s/it] 34%|███▍      | 1048/3086 [35:02<1:08:20,  2.01s/it] 34%|███▍      | 1049/3086 [35:04<1:06:22,  1.96s/it] 34%|███▍      | 1050/3086 [35:06<1:08:20,  2.01s/it]                                                     {'loss': 0.5879, 'grad_norm': 0.44325265288352966, 'learning_rate': 0.000131950745301361, 'epoch': 0.34}
 34%|███▍      | 1050/3086 [35:06<1:08:20,  2.01s/it] 34%|███▍      | 1051/3086 [35:07<1:04:13,  1.89s/it] 34%|███▍      | 1052/3086 [35:09<1:02:40,  1.85s/it] 34%|███▍      | 1053/3086 [35:11<1:02:28,  1.84s/it] 34%|███▍      | 1054/3086 [35:13<59:12,  1.75s/it]   34%|███▍      | 1055/3086 [35:15<1:08:43,  2.03s/it] 34%|███▍      | 1056/3086 [35:17<1:03:35,  1.88s/it] 34%|███▍      | 1057/3086 [35:19<1:07:05,  1.98s/it] 34%|███▍      | 1058/3086 [35:21<1:06:54,  1.98s/it] 34%|███▍      | 1059/3086 [35:24<1:12:55,  2.16s/it] 34%|███▍      | 1060/3086 [35:26<1:11:05,  2.11s/it]                                                     {'loss': 0.5957, 'grad_norm': 0.3510960340499878, 'learning_rate': 0.00013130265716137395, 'epoch': 0.34}
 34%|███▍      | 1060/3086 [35:26<1:11:05,  2.11s/it] 34%|███▍      | 1061/3086 [35:27<1:06:49,  1.98s/it] 34%|███▍      | 1062/3086 [35:29<1:07:35,  2.00s/it] 34%|███▍      | 1063/3086 [35:31<1:04:11,  1.90s/it] 34%|███▍      | 1064/3086 [35:34<1:11:56,  2.13s/it] 35%|███▍      | 1065/3086 [35:35<1:09:04,  2.05s/it] 35%|███▍      | 1066/3086 [35:37<1:05:02,  1.93s/it] 35%|███▍      | 1067/3086 [35:39<1:05:33,  1.95s/it] 35%|███▍      | 1068/3086 [35:42<1:10:39,  2.10s/it] 35%|███▍      | 1069/3086 [35:43<1:08:08,  2.03s/it] 35%|███▍      | 1070/3086 [35:45<1:08:07,  2.03s/it]                                                     {'loss': 0.5941, 'grad_norm': 0.5194435119628906, 'learning_rate': 0.0001306545690213869, 'epoch': 0.35}
 35%|███▍      | 1070/3086 [35:45<1:08:07,  2.03s/it] 35%|███▍      | 1071/3086 [35:48<1:09:30,  2.07s/it] 35%|███▍      | 1072/3086 [35:49<1:05:24,  1.95s/it] 35%|███▍      | 1073/3086 [35:51<1:07:09,  2.00s/it] 35%|███▍      | 1074/3086 [35:54<1:08:46,  2.05s/it] 35%|███▍      | 1075/3086 [35:55<1:03:30,  1.89s/it] 35%|███▍      | 1076/3086 [35:57<1:02:50,  1.88s/it] 35%|███▍      | 1077/3086 [35:59<1:05:03,  1.94s/it] 35%|███▍      | 1078/3086 [36:01<1:04:24,  1.92s/it] 35%|███▍      | 1079/3086 [36:03<1:02:19,  1.86s/it] 35%|███▍      | 1080/3086 [36:05<1:03:44,  1.91s/it]                                                     {'loss': 0.5902, 'grad_norm': 0.48441001772880554, 'learning_rate': 0.0001300064808813999, 'epoch': 0.35}
 35%|███▍      | 1080/3086 [36:05<1:03:44,  1.91s/it] 35%|███▌      | 1081/3086 [36:07<1:06:24,  1.99s/it] 35%|███▌      | 1082/3086 [36:09<1:09:39,  2.09s/it] 35%|███▌      | 1083/3086 [36:11<1:11:11,  2.13s/it] 35%|███▌      | 1084/3086 [36:13<1:07:33,  2.02s/it] 35%|███▌      | 1085/3086 [36:16<1:13:40,  2.21s/it] 35%|███▌      | 1086/3086 [36:18<1:10:21,  2.11s/it] 35%|███▌      | 1087/3086 [36:20<1:11:05,  2.13s/it] 35%|███▌      | 1088/3086 [36:22<1:11:34,  2.15s/it] 35%|███▌      | 1089/3086 [36:24<1:10:29,  2.12s/it] 35%|███▌      | 1090/3086 [36:26<1:08:35,  2.06s/it]                                                     {'loss': 0.5836, 'grad_norm': 0.40921348333358765, 'learning_rate': 0.00012935839274141284, 'epoch': 0.35}
 35%|███▌      | 1090/3086 [36:26<1:08:35,  2.06s/it] 35%|███▌      | 1091/3086 [36:28<1:06:23,  2.00s/it] 35%|███▌      | 1092/3086 [36:30<1:08:37,  2.06s/it] 35%|███▌      | 1093/3086 [36:33<1:16:17,  2.30s/it] 35%|███▌      | 1094/3086 [36:35<1:13:50,  2.22s/it] 35%|███▌      | 1095/3086 [36:37<1:10:02,  2.11s/it] 36%|███▌      | 1096/3086 [36:39<1:11:18,  2.15s/it] 36%|███▌      | 1097/3086 [36:41<1:09:33,  2.10s/it] 36%|███▌      | 1098/3086 [36:43<1:09:22,  2.09s/it] 36%|███▌      | 1099/3086 [36:46<1:14:09,  2.24s/it] 36%|███▌      | 1100/3086 [36:47<1:07:59,  2.05s/it]                                                     {'loss': 0.5827, 'grad_norm': 0.5409151911735535, 'learning_rate': 0.0001287103046014258, 'epoch': 0.36}
 36%|███▌      | 1100/3086 [36:47<1:07:59,  2.05s/it] 36%|███▌      | 1101/3086 [36:49<1:05:21,  1.98s/it] 36%|███▌      | 1102/3086 [36:51<1:07:08,  2.03s/it] 36%|███▌      | 1103/3086 [36:53<1:05:54,  1.99s/it] 36%|███▌      | 1104/3086 [36:55<1:08:03,  2.06s/it] 36%|███▌      | 1105/3086 [36:57<1:07:56,  2.06s/it] 36%|███▌      | 1106/3086 [37:00<1:08:15,  2.07s/it] 36%|███▌      | 1107/3086 [37:02<1:07:58,  2.06s/it] 36%|███▌      | 1108/3086 [37:03<1:04:08,  1.95s/it] 36%|███▌      | 1109/3086 [37:05<1:05:19,  1.98s/it] 36%|███▌      | 1110/3086 [37:07<1:05:32,  1.99s/it]                                                     {'loss': 0.584, 'grad_norm': 0.4529513716697693, 'learning_rate': 0.00012806221646143877, 'epoch': 0.36}
 36%|███▌      | 1110/3086 [37:07<1:05:32,  1.99s/it] 36%|███▌      | 1111/3086 [37:09<1:04:40,  1.96s/it] 36%|███▌      | 1112/3086 [37:11<1:05:12,  1.98s/it] 36%|███▌      | 1113/3086 [37:13<1:03:09,  1.92s/it] 36%|███▌      | 1114/3086 [37:16<1:09:31,  2.12s/it] 36%|███▌      | 1115/3086 [37:18<1:12:15,  2.20s/it] 36%|███▌      | 1116/3086 [37:20<1:06:53,  2.04s/it] 36%|███▌      | 1117/3086 [37:22<1:14:17,  2.26s/it] 36%|███▌      | 1118/3086 [37:24<1:10:13,  2.14s/it] 36%|███▋      | 1119/3086 [37:26<1:07:29,  2.06s/it] 36%|███▋      | 1120/3086 [37:28<1:07:08,  2.05s/it]                                                     {'loss': 0.5836, 'grad_norm': 0.594215452671051, 'learning_rate': 0.00012741412832145173, 'epoch': 0.36}
 36%|███▋      | 1120/3086 [37:28<1:07:08,  2.05s/it] 36%|███▋      | 1121/3086 [37:30<1:06:38,  2.03s/it] 36%|███▋      | 1122/3086 [37:32<1:03:41,  1.95s/it] 36%|███▋      | 1123/3086 [37:34<1:03:57,  1.95s/it] 36%|███▋      | 1124/3086 [37:36<1:08:01,  2.08s/it] 36%|███▋      | 1125/3086 [37:38<1:05:45,  2.01s/it] 36%|███▋      | 1126/3086 [37:40<1:04:49,  1.98s/it] 37%|███▋      | 1127/3086 [37:42<1:03:32,  1.95s/it] 37%|███▋      | 1128/3086 [37:44<1:04:34,  1.98s/it] 37%|███▋      | 1129/3086 [37:46<1:01:55,  1.90s/it] 37%|███▋      | 1130/3086 [37:48<1:01:39,  1.89s/it]                                                     {'loss': 0.5856, 'grad_norm': 0.3442552089691162, 'learning_rate': 0.0001267660401814647, 'epoch': 0.37}
 37%|███▋      | 1130/3086 [37:48<1:01:39,  1.89s/it] 37%|███▋      | 1131/3086 [37:50<1:05:02,  2.00s/it] 37%|███▋      | 1132/3086 [37:51<1:01:42,  1.89s/it] 37%|███▋      | 1133/3086 [37:53<1:00:13,  1.85s/it] 37%|███▋      | 1134/3086 [37:55<1:00:18,  1.85s/it] 37%|███▋      | 1135/3086 [37:57<1:04:54,  2.00s/it] 37%|███▋      | 1136/3086 [37:59<1:01:10,  1.88s/it] 37%|███▋      | 1137/3086 [38:01<1:01:32,  1.89s/it] 37%|███▋      | 1138/3086 [38:03<1:02:41,  1.93s/it] 37%|███▋      | 1139/3086 [38:05<1:05:15,  2.01s/it] 37%|███▋      | 1140/3086 [38:08<1:08:19,  2.11s/it]                                                     {'loss': 0.5798, 'grad_norm': 0.539898693561554, 'learning_rate': 0.00012611795204147766, 'epoch': 0.37}
 37%|███▋      | 1140/3086 [38:08<1:08:19,  2.11s/it] 37%|███▋      | 1141/3086 [38:10<1:09:34,  2.15s/it] 37%|███▋      | 1142/3086 [38:12<1:07:40,  2.09s/it] 37%|███▋      | 1143/3086 [38:13<1:04:48,  2.00s/it] 37%|███▋      | 1144/3086 [38:16<1:05:55,  2.04s/it] 37%|███▋      | 1145/3086 [38:18<1:05:03,  2.01s/it] 37%|███▋      | 1146/3086 [38:19<1:03:04,  1.95s/it] 37%|███▋      | 1147/3086 [38:21<1:03:19,  1.96s/it] 37%|███▋      | 1148/3086 [38:23<1:01:18,  1.90s/it] 37%|███▋      | 1149/3086 [38:25<57:57,  1.80s/it]   37%|███▋      | 1150/3086 [38:27<1:04:34,  2.00s/it]                                                     {'loss': 0.5897, 'grad_norm': 0.42407742142677307, 'learning_rate': 0.00012546986390149061, 'epoch': 0.37}
 37%|███▋      | 1150/3086 [38:27<1:04:34,  2.00s/it] 37%|███▋      | 1151/3086 [38:29<1:03:53,  1.98s/it] 37%|███▋      | 1152/3086 [38:32<1:10:45,  2.20s/it] 37%|███▋      | 1153/3086 [38:33<1:06:02,  2.05s/it] 37%|███▋      | 1154/3086 [38:35<1:05:11,  2.02s/it] 37%|███▋      | 1155/3086 [38:38<1:09:15,  2.15s/it] 37%|███▋      | 1156/3086 [38:40<1:07:23,  2.10s/it] 37%|███▋      | 1157/3086 [38:42<1:05:57,  2.05s/it] 38%|███▊      | 1158/3086 [38:44<1:05:27,  2.04s/it] 38%|███▊      | 1159/3086 [38:46<1:07:36,  2.11s/it] 38%|███▊      | 1160/3086 [38:49<1:11:17,  2.22s/it]                                                     {'loss': 0.5705, 'grad_norm': 0.520633339881897, 'learning_rate': 0.00012482177576150357, 'epoch': 0.38}
 38%|███▊      | 1160/3086 [38:49<1:11:17,  2.22s/it] 38%|███▊      | 1161/3086 [38:50<1:04:25,  2.01s/it] 38%|███▊      | 1162/3086 [38:52<1:06:38,  2.08s/it] 38%|███▊      | 1163/3086 [38:54<1:02:27,  1.95s/it] 38%|███▊      | 1164/3086 [38:56<1:02:28,  1.95s/it] 38%|███▊      | 1165/3086 [38:58<1:03:32,  1.98s/it] 38%|███▊      | 1166/3086 [39:01<1:10:39,  2.21s/it] 38%|███▊      | 1167/3086 [39:03<1:11:17,  2.23s/it] 38%|███▊      | 1168/3086 [39:05<1:06:50,  2.09s/it] 38%|███▊      | 1169/3086 [39:08<1:13:25,  2.30s/it] 38%|███▊      | 1170/3086 [39:09<1:06:44,  2.09s/it]                                                     {'loss': 0.5881, 'grad_norm': 0.5476008653640747, 'learning_rate': 0.00012417368762151652, 'epoch': 0.38}
 38%|███▊      | 1170/3086 [39:09<1:06:44,  2.09s/it] 38%|███▊      | 1171/3086 [39:11<1:04:43,  2.03s/it] 38%|███▊      | 1172/3086 [39:14<1:13:18,  2.30s/it] 38%|███▊      | 1173/3086 [39:16<1:07:27,  2.12s/it] 38%|███▊      | 1174/3086 [39:17<1:04:37,  2.03s/it] 38%|███▊      | 1175/3086 [39:20<1:06:45,  2.10s/it] 38%|███▊      | 1176/3086 [39:22<1:05:18,  2.05s/it] 38%|███▊      | 1177/3086 [39:23<1:02:27,  1.96s/it] 38%|███▊      | 1178/3086 [39:26<1:03:46,  2.01s/it] 38%|███▊      | 1179/3086 [39:27<1:00:59,  1.92s/it] 38%|███▊      | 1180/3086 [39:29<57:54,  1.82s/it]                                                     {'loss': 0.569, 'grad_norm': 0.5321537256240845, 'learning_rate': 0.00012352559948152947, 'epoch': 0.38}
 38%|███▊      | 1180/3086 [39:29<57:54,  1.82s/it] 38%|███▊      | 1181/3086 [39:31<56:44,  1.79s/it] 38%|███▊      | 1182/3086 [39:32<55:42,  1.76s/it] 38%|███▊      | 1183/3086 [39:34<55:51,  1.76s/it] 38%|███▊      | 1184/3086 [39:36<56:38,  1.79s/it] 38%|███▊      | 1185/3086 [39:38<56:09,  1.77s/it] 38%|███▊      | 1186/3086 [39:40<1:01:19,  1.94s/it] 38%|███▊      | 1187/3086 [39:42<59:55,  1.89s/it]   38%|███▊      | 1188/3086 [39:44<1:00:53,  1.92s/it] 39%|███▊      | 1189/3086 [39:46<59:49,  1.89s/it]   39%|███▊      | 1190/3086 [39:48<1:06:11,  2.09s/it]                                                     {'loss': 0.5763, 'grad_norm': 0.45270004868507385, 'learning_rate': 0.00012287751134154246, 'epoch': 0.39}
 39%|███▊      | 1190/3086 [39:48<1:06:11,  2.09s/it] 39%|███▊      | 1191/3086 [39:50<1:06:35,  2.11s/it] 39%|███▊      | 1192/3086 [39:52<1:07:38,  2.14s/it] 39%|███▊      | 1193/3086 [39:55<1:06:43,  2.12s/it] 39%|███▊      | 1194/3086 [39:56<1:03:01,  2.00s/it] 39%|███▊      | 1195/3086 [39:58<1:03:57,  2.03s/it] 39%|███▉      | 1196/3086 [40:00<1:02:14,  1.98s/it] 39%|███▉      | 1197/3086 [40:02<1:04:16,  2.04s/it] 39%|███▉      | 1198/3086 [40:04<1:01:50,  1.97s/it] 39%|███▉      | 1199/3086 [40:06<1:01:10,  1.94s/it] 39%|███▉      | 1200/3086 [40:08<1:01:17,  1.95s/it]                                                     {'loss': 0.5782, 'grad_norm': 0.4898817837238312, 'learning_rate': 0.0001222294232015554, 'epoch': 0.39}
 39%|███▉      | 1200/3086 [40:08<1:01:17,  1.95s/it] 39%|███▉      | 1201/3086 [40:10<1:05:15,  2.08s/it] 39%|███▉      | 1202/3086 [40:13<1:11:42,  2.28s/it] 39%|███▉      | 1203/3086 [40:15<1:10:56,  2.26s/it] 39%|███▉      | 1204/3086 [40:18<1:09:56,  2.23s/it] 39%|███▉      | 1205/3086 [40:19<1:06:37,  2.13s/it] 39%|███▉      | 1206/3086 [40:21<1:03:28,  2.03s/it] 39%|███▉      | 1207/3086 [40:23<1:03:12,  2.02s/it] 39%|███▉      | 1208/3086 [40:25<1:00:32,  1.93s/it] 39%|███▉      | 1209/3086 [40:27<1:00:31,  1.93s/it] 39%|███▉      | 1210/3086 [40:29<1:01:57,  1.98s/it]                                                     {'loss': 0.586, 'grad_norm': 0.5948662757873535, 'learning_rate': 0.00012158133506156838, 'epoch': 0.39}
 39%|███▉      | 1210/3086 [40:29<1:01:57,  1.98s/it] 39%|███▉      | 1211/3086 [40:31<1:02:48,  2.01s/it] 39%|███▉      | 1212/3086 [40:33<59:52,  1.92s/it]   39%|███▉      | 1213/3086 [40:35<1:00:14,  1.93s/it] 39%|███▉      | 1214/3086 [40:37<59:32,  1.91s/it]   39%|███▉      | 1215/3086 [40:39<1:02:35,  2.01s/it] 39%|███▉      | 1216/3086 [40:41<1:00:56,  1.96s/it] 39%|███▉      | 1217/3086 [40:43<1:01:34,  1.98s/it] 39%|███▉      | 1218/3086 [40:45<1:04:54,  2.09s/it] 40%|███▉      | 1219/3086 [40:47<1:05:43,  2.11s/it] 40%|███▉      | 1220/3086 [40:50<1:11:12,  2.29s/it]                                                     {'loss': 0.5673, 'grad_norm': 0.38422125577926636, 'learning_rate': 0.00012093324692158134, 'epoch': 0.4}
 40%|███▉      | 1220/3086 [40:50<1:11:12,  2.29s/it] 40%|███▉      | 1221/3086 [40:52<1:08:18,  2.20s/it] 40%|███▉      | 1222/3086 [40:54<1:08:49,  2.22s/it] 40%|███▉      | 1223/3086 [40:56<1:04:17,  2.07s/it] 40%|███▉      | 1224/3086 [40:58<1:07:00,  2.16s/it] 40%|███▉      | 1225/3086 [41:00<1:04:35,  2.08s/it] 40%|███▉      | 1226/3086 [41:02<1:06:00,  2.13s/it] 40%|███▉      | 1227/3086 [41:05<1:07:05,  2.17s/it] 40%|███▉      | 1228/3086 [41:07<1:06:55,  2.16s/it] 40%|███▉      | 1229/3086 [41:08<1:02:17,  2.01s/it] 40%|███▉      | 1230/3086 [41:11<1:04:55,  2.10s/it]                                                     {'loss': 0.5705, 'grad_norm': 0.502738356590271, 'learning_rate': 0.0001202851587815943, 'epoch': 0.4}
 40%|███▉      | 1230/3086 [41:11<1:04:55,  2.10s/it] 40%|███▉      | 1231/3086 [41:13<1:04:59,  2.10s/it] 40%|███▉      | 1232/3086 [41:16<1:13:30,  2.38s/it] 40%|███▉      | 1233/3086 [41:18<1:09:26,  2.25s/it] 40%|███▉      | 1234/3086 [41:20<1:07:03,  2.17s/it] 40%|████      | 1235/3086 [41:21<1:02:00,  2.01s/it] 40%|████      | 1236/3086 [41:24<1:03:38,  2.06s/it] 40%|████      | 1237/3086 [41:26<1:03:06,  2.05s/it] 40%|████      | 1238/3086 [41:27<1:00:45,  1.97s/it] 40%|████      | 1239/3086 [41:29<1:01:28,  2.00s/it] 40%|████      | 1240/3086 [41:32<1:06:08,  2.15s/it]                                                     {'loss': 0.5837, 'grad_norm': 0.5516445636749268, 'learning_rate': 0.00011963707064160728, 'epoch': 0.4}
 40%|████      | 1240/3086 [41:32<1:06:08,  2.15s/it] 40%|████      | 1241/3086 [41:34<1:06:13,  2.15s/it] 40%|████      | 1242/3086 [41:36<1:01:47,  2.01s/it] 40%|████      | 1243/3086 [41:38<1:04:01,  2.08s/it] 40%|████      | 1244/3086 [41:40<1:04:38,  2.11s/it] 40%|████      | 1245/3086 [41:42<1:01:34,  2.01s/it] 40%|████      | 1246/3086 [41:44<1:01:04,  1.99s/it] 40%|████      | 1247/3086 [41:46<57:13,  1.87s/it]   40%|████      | 1248/3086 [41:47<55:21,  1.81s/it] 40%|████      | 1249/3086 [41:49<57:11,  1.87s/it] 41%|████      | 1250/3086 [41:51<59:15,  1.94s/it]                                                   {'loss': 0.5747, 'grad_norm': 0.640584409236908, 'learning_rate': 0.00011898898250162023, 'epoch': 0.41}
 41%|████      | 1250/3086 [41:51<59:15,  1.94s/it] 41%|████      | 1251/3086 [41:53<58:22,  1.91s/it] 41%|████      | 1252/3086 [41:55<57:22,  1.88s/it] 41%|████      | 1253/3086 [41:57<55:48,  1.83s/it] 41%|████      | 1254/3086 [41:59<1:01:48,  2.02s/it] 41%|████      | 1255/3086 [42:01<1:02:02,  2.03s/it] 41%|████      | 1256/3086 [42:04<1:09:16,  2.27s/it] 41%|████      | 1257/3086 [42:06<1:09:35,  2.28s/it] 41%|████      | 1258/3086 [42:08<1:06:56,  2.20s/it] 41%|████      | 1259/3086 [42:11<1:10:00,  2.30s/it] 41%|████      | 1260/3086 [42:13<1:07:03,  2.20s/it]                                                     {'loss': 0.5804, 'grad_norm': 0.47450968623161316, 'learning_rate': 0.00011834089436163318, 'epoch': 0.41}
 41%|████      | 1260/3086 [42:13<1:07:03,  2.20s/it] 41%|████      | 1261/3086 [42:15<1:03:06,  2.07s/it] 41%|████      | 1262/3086 [42:16<1:00:46,  2.00s/it] 41%|████      | 1263/3086 [42:18<58:51,  1.94s/it]   41%|████      | 1264/3086 [42:20<1:00:45,  2.00s/it] 41%|████      | 1265/3086 [42:23<1:03:01,  2.08s/it] 41%|████      | 1266/3086 [42:24<59:22,  1.96s/it]   41%|████      | 1267/3086 [42:26<56:52,  1.88s/it] 41%|████      | 1268/3086 [42:28<57:14,  1.89s/it] 41%|████      | 1269/3086 [42:30<1:00:52,  2.01s/it] 41%|████      | 1270/3086 [42:32<59:22,  1.96s/it]                                                     {'loss': 0.5822, 'grad_norm': 0.3767673373222351, 'learning_rate': 0.00011769280622164616, 'epoch': 0.41}
 41%|████      | 1270/3086 [42:32<59:22,  1.96s/it] 41%|████      | 1271/3086 [42:34<58:16,  1.93s/it] 41%|████      | 1272/3086 [42:36<59:34,  1.97s/it] 41%|████▏     | 1273/3086 [42:38<59:22,  1.97s/it] 41%|████▏     | 1274/3086 [42:40<1:01:20,  2.03s/it] 41%|████▏     | 1275/3086 [42:43<1:05:31,  2.17s/it] 41%|████▏     | 1276/3086 [42:44<1:01:35,  2.04s/it] 41%|████▏     | 1277/3086 [42:46<1:00:49,  2.02s/it] 41%|████▏     | 1278/3086 [42:49<1:02:11,  2.06s/it] 41%|████▏     | 1279/3086 [42:51<1:08:23,  2.27s/it] 41%|████▏     | 1280/3086 [42:53<1:03:45,  2.12s/it]                                                     {'loss': 0.5748, 'grad_norm': 0.3901333808898926, 'learning_rate': 0.00011704471808165912, 'epoch': 0.41}
 41%|████▏     | 1280/3086 [42:53<1:03:45,  2.12s/it] 42%|████▏     | 1281/3086 [42:55<1:04:48,  2.15s/it] 42%|████▏     | 1282/3086 [42:58<1:08:14,  2.27s/it] 42%|████▏     | 1283/3086 [43:00<1:08:30,  2.28s/it] 42%|████▏     | 1284/3086 [43:03<1:10:37,  2.35s/it] 42%|████▏     | 1285/3086 [43:05<1:09:37,  2.32s/it] 42%|████▏     | 1286/3086 [43:07<1:05:23,  2.18s/it] 42%|████▏     | 1287/3086 [43:09<1:09:18,  2.31s/it] 42%|████▏     | 1288/3086 [43:11<1:03:44,  2.13s/it] 42%|████▏     | 1289/3086 [43:13<59:38,  1.99s/it]   42%|████▏     | 1290/3086 [43:14<57:31,  1.92s/it]                                                   {'loss': 0.5776, 'grad_norm': 0.4582273066043854, 'learning_rate': 0.00011639662994167207, 'epoch': 0.42}
 42%|████▏     | 1290/3086 [43:14<57:31,  1.92s/it] 42%|████▏     | 1291/3086 [43:16<55:40,  1.86s/it] 42%|████▏     | 1292/3086 [43:18<55:16,  1.85s/it] 42%|████▏     | 1293/3086 [43:20<54:19,  1.82s/it] 42%|████▏     | 1294/3086 [43:21<51:36,  1.73s/it] 42%|████▏     | 1295/3086 [43:23<50:58,  1.71s/it] 42%|████▏     | 1296/3086 [43:24<48:46,  1.63s/it] 42%|████▏     | 1297/3086 [43:27<57:23,  1.92s/it] 42%|████▏     | 1298/3086 [43:29<56:22,  1.89s/it] 42%|████▏     | 1299/3086 [43:31<57:56,  1.95s/it] 42%|████▏     | 1300/3086 [43:34<1:05:54,  2.21s/it]                                                     {'loss': 0.5845, 'grad_norm': 0.511530339717865, 'learning_rate': 0.00011574854180168504, 'epoch': 0.42}
 42%|████▏     | 1300/3086 [43:34<1:05:54,  2.21s/it] 42%|████▏     | 1301/3086 [43:36<1:02:09,  2.09s/it] 42%|████▏     | 1302/3086 [43:38<1:04:32,  2.17s/it] 42%|████▏     | 1303/3086 [43:40<59:53,  2.02s/it]   42%|████▏     | 1304/3086 [43:41<56:42,  1.91s/it] 42%|████▏     | 1305/3086 [43:43<59:52,  2.02s/it] 42%|████▏     | 1306/3086 [43:46<1:02:06,  2.09s/it] 42%|████▏     | 1307/3086 [43:48<1:02:32,  2.11s/it] 42%|████▏     | 1308/3086 [43:50<1:01:16,  2.07s/it] 42%|████▏     | 1309/3086 [43:52<1:03:58,  2.16s/it] 42%|████▏     | 1310/3086 [43:54<1:00:31,  2.04s/it]                                                     {'loss': 0.5733, 'grad_norm': 0.4503968358039856, 'learning_rate': 0.00011510045366169799, 'epoch': 0.42}
 42%|████▏     | 1310/3086 [43:54<1:00:31,  2.04s/it] 42%|████▏     | 1311/3086 [43:56<57:11,  1.93s/it]   43%|████▎     | 1312/3086 [43:58<58:05,  1.96s/it] 43%|████▎     | 1313/3086 [44:00<1:00:29,  2.05s/it] 43%|████▎     | 1314/3086 [44:02<59:05,  2.00s/it]   43%|████▎     | 1315/3086 [44:04<56:21,  1.91s/it] 43%|████▎     | 1316/3086 [44:06<59:48,  2.03s/it] 43%|████▎     | 1317/3086 [44:08<58:04,  1.97s/it] 43%|████▎     | 1318/3086 [44:09<55:15,  1.88s/it] 43%|████▎     | 1319/3086 [44:11<56:06,  1.91s/it] 43%|████▎     | 1320/3086 [44:14<59:13,  2.01s/it]                                                   {'loss': 0.5824, 'grad_norm': 0.42770224809646606, 'learning_rate': 0.00011445236552171097, 'epoch': 0.43}
 43%|████▎     | 1320/3086 [44:14<59:13,  2.01s/it] 43%|████▎     | 1321/3086 [44:15<57:50,  1.97s/it] 43%|████▎     | 1322/3086 [44:17<53:34,  1.82s/it] 43%|████▎     | 1323/3086 [44:19<57:48,  1.97s/it] 43%|████▎     | 1324/3086 [44:22<1:05:09,  2.22s/it] 43%|████▎     | 1325/3086 [44:24<59:40,  2.03s/it]   43%|████▎     | 1326/3086 [44:25<56:21,  1.92s/it] 43%|████▎     | 1327/3086 [44:28<58:56,  2.01s/it] 43%|████▎     | 1328/3086 [44:29<57:59,  1.98s/it] 43%|████▎     | 1329/3086 [44:31<56:56,  1.94s/it] 43%|████▎     | 1330/3086 [44:33<57:52,  1.98s/it]                                                   {'loss': 0.5856, 'grad_norm': 0.6548895239830017, 'learning_rate': 0.00011380427738172392, 'epoch': 0.43}
 43%|████▎     | 1330/3086 [44:33<57:52,  1.98s/it] 43%|████▎     | 1331/3086 [44:35<57:06,  1.95s/it] 43%|████▎     | 1332/3086 [44:38<1:04:40,  2.21s/it] 43%|████▎     | 1333/3086 [44:40<1:01:36,  2.11s/it] 43%|████▎     | 1334/3086 [44:42<1:02:43,  2.15s/it] 43%|████▎     | 1335/3086 [44:44<1:01:47,  2.12s/it] 43%|████▎     | 1336/3086 [44:46<1:01:45,  2.12s/it] 43%|████▎     | 1337/3086 [44:48<1:00:06,  2.06s/it] 43%|████▎     | 1338/3086 [44:50<58:28,  2.01s/it]   43%|████▎     | 1339/3086 [44:53<1:03:03,  2.17s/it] 43%|████▎     | 1340/3086 [44:55<1:07:39,  2.33s/it]                                                     {'loss': 0.5846, 'grad_norm': 0.42303502559661865, 'learning_rate': 0.00011315618924173688, 'epoch': 0.43}
 43%|████▎     | 1340/3086 [44:55<1:07:39,  2.33s/it] 43%|████▎     | 1341/3086 [44:58<1:11:20,  2.45s/it] 43%|████▎     | 1342/3086 [45:00<1:05:24,  2.25s/it] 44%|████▎     | 1343/3086 [45:02<1:06:24,  2.29s/it] 44%|████▎     | 1344/3086 [45:04<1:05:26,  2.25s/it] 44%|████▎     | 1345/3086 [45:07<1:05:36,  2.26s/it] 44%|████▎     | 1346/3086 [45:09<1:08:14,  2.35s/it] 44%|████▎     | 1347/3086 [45:11<1:02:47,  2.17s/it] 44%|████▎     | 1348/3086 [45:13<1:01:02,  2.11s/it] 44%|████▎     | 1349/3086 [45:15<57:33,  1.99s/it]   44%|████▎     | 1350/3086 [45:17<57:19,  1.98s/it]                                                   {'loss': 0.5709, 'grad_norm': 0.49514201283454895, 'learning_rate': 0.00011250810110174986, 'epoch': 0.44}
 44%|████▎     | 1350/3086 [45:17<57:19,  1.98s/it] 44%|████▍     | 1351/3086 [45:19<1:03:16,  2.19s/it] 44%|████▍     | 1352/3086 [45:22<1:03:59,  2.21s/it] 44%|████▍     | 1353/3086 [45:24<1:01:08,  2.12s/it] 44%|████▍     | 1354/3086 [45:25<57:58,  2.01s/it]   44%|████▍     | 1355/3086 [45:27<56:57,  1.97s/it] 44%|████▍     | 1356/3086 [45:30<1:01:32,  2.13s/it] 44%|████▍     | 1357/3086 [45:32<1:02:54,  2.18s/it] 44%|████▍     | 1358/3086 [45:34<1:00:19,  2.09s/it] 44%|████▍     | 1359/3086 [45:36<58:08,  2.02s/it]   44%|████▍     | 1360/3086 [45:38<1:00:40,  2.11s/it]                                                     {'loss': 0.5716, 'grad_norm': 0.42271187901496887, 'learning_rate': 0.00011186001296176281, 'epoch': 0.44}
 44%|████▍     | 1360/3086 [45:38<1:00:40,  2.11s/it] 44%|████▍     | 1361/3086 [45:40<59:39,  2.07s/it]   44%|████▍     | 1362/3086 [45:42<1:00:29,  2.11s/it] 44%|████▍     | 1363/3086 [45:44<57:21,  2.00s/it]   44%|████▍     | 1364/3086 [45:46<56:04,  1.95s/it] 44%|████▍     | 1365/3086 [45:48<56:35,  1.97s/it] 44%|████▍     | 1366/3086 [45:50<54:35,  1.90s/it] 44%|████▍     | 1367/3086 [45:51<52:09,  1.82s/it] 44%|████▍     | 1368/3086 [45:53<52:01,  1.82s/it] 44%|████▍     | 1369/3086 [45:55<53:38,  1.87s/it] 44%|████▍     | 1370/3086 [45:57<56:31,  1.98s/it]                                                   {'loss': 0.5852, 'grad_norm': 0.5367713570594788, 'learning_rate': 0.00011121192482177576, 'epoch': 0.44}
 44%|████▍     | 1370/3086 [45:57<56:31,  1.98s/it] 44%|████▍     | 1371/3086 [45:59<55:18,  1.93s/it] 44%|████▍     | 1372/3086 [46:02<1:00:39,  2.12s/it] 44%|████▍     | 1373/3086 [46:05<1:07:16,  2.36s/it] 45%|████▍     | 1374/3086 [46:06<1:03:45,  2.23s/it] 45%|████▍     | 1375/3086 [46:09<1:07:13,  2.36s/it] 45%|████▍     | 1376/3086 [46:11<1:02:53,  2.21s/it] 45%|████▍     | 1377/3086 [46:13<1:04:27,  2.26s/it] 45%|████▍     | 1378/3086 [46:15<59:42,  2.10s/it]   45%|████▍     | 1379/3086 [46:17<57:14,  2.01s/it] 45%|████▍     | 1380/3086 [46:19<56:48,  2.00s/it]                                                   {'loss': 0.569, 'grad_norm': 0.4383346140384674, 'learning_rate': 0.00011056383668178873, 'epoch': 0.45}
 45%|████▍     | 1380/3086 [46:19<56:48,  2.00s/it] 45%|████▍     | 1381/3086 [46:21<54:15,  1.91s/it] 45%|████▍     | 1382/3086 [46:23<55:33,  1.96s/it] 45%|████▍     | 1383/3086 [46:25<55:19,  1.95s/it] 45%|████▍     | 1384/3086 [46:26<53:42,  1.89s/it] 45%|████▍     | 1385/3086 [46:28<51:09,  1.80s/it] 45%|████▍     | 1386/3086 [46:30<52:17,  1.85s/it] 45%|████▍     | 1387/3086 [46:33<1:00:50,  2.15s/it] 45%|████▍     | 1388/3086 [46:35<1:03:10,  2.23s/it] 45%|████▌     | 1389/3086 [46:37<58:12,  2.06s/it]   45%|████▌     | 1390/3086 [46:39<56:45,  2.01s/it]                                                   {'loss': 0.5772, 'grad_norm': 0.4158763885498047, 'learning_rate': 0.00010991574854180168, 'epoch': 0.45}
 45%|████▌     | 1390/3086 [46:39<56:45,  2.01s/it] 45%|████▌     | 1391/3086 [46:41<58:11,  2.06s/it] 45%|████▌     | 1392/3086 [46:43<57:16,  2.03s/it] 45%|████▌     | 1393/3086 [46:45<55:47,  1.98s/it] 45%|████▌     | 1394/3086 [46:46<54:22,  1.93s/it] 45%|████▌     | 1395/3086 [46:48<54:32,  1.93s/it] 45%|████▌     | 1396/3086 [46:51<1:00:07,  2.13s/it] 45%|████▌     | 1397/3086 [46:54<1:04:26,  2.29s/it] 45%|████▌     | 1398/3086 [46:56<1:01:30,  2.19s/it] 45%|████▌     | 1399/3086 [46:58<1:00:50,  2.16s/it] 45%|████▌     | 1400/3086 [47:00<1:03:38,  2.26s/it]                                                     {'loss': 0.5673, 'grad_norm': 0.4234270453453064, 'learning_rate': 0.00010926766040181464, 'epoch': 0.45}
 45%|████▌     | 1400/3086 [47:00<1:03:38,  2.26s/it] 45%|████▌     | 1401/3086 [47:03<1:05:15,  2.32s/it] 45%|████▌     | 1402/3086 [47:05<1:01:34,  2.19s/it] 45%|████▌     | 1403/3086 [47:06<58:04,  2.07s/it]   45%|████▌     | 1404/3086 [47:08<56:31,  2.02s/it] 46%|████▌     | 1405/3086 [47:10<55:50,  1.99s/it] 46%|████▌     | 1406/3086 [47:12<56:14,  2.01s/it] 46%|████▌     | 1407/3086 [47:14<56:28,  2.02s/it] 46%|████▌     | 1408/3086 [47:17<58:26,  2.09s/it] 46%|████▌     | 1409/3086 [47:18<53:18,  1.91s/it] 46%|████▌     | 1410/3086 [47:20<56:02,  2.01s/it]                                                   {'loss': 0.5751, 'grad_norm': 0.5475953817367554, 'learning_rate': 0.00010861957226182762, 'epoch': 0.46}
 46%|████▌     | 1410/3086 [47:20<56:02,  2.01s/it] 46%|████▌     | 1411/3086 [47:22<53:09,  1.90s/it] 46%|████▌     | 1412/3086 [47:24<52:36,  1.89s/it] 46%|████▌     | 1413/3086 [47:26<54:55,  1.97s/it] 46%|████▌     | 1414/3086 [47:28<56:26,  2.03s/it] 46%|████▌     | 1415/3086 [47:30<56:29,  2.03s/it] 46%|████▌     | 1416/3086 [47:32<54:22,  1.95s/it] 46%|████▌     | 1417/3086 [47:34<57:51,  2.08s/it] 46%|████▌     | 1418/3086 [47:36<54:53,  1.97s/it] 46%|████▌     | 1419/3086 [47:38<54:46,  1.97s/it] 46%|████▌     | 1420/3086 [47:40<53:25,  1.92s/it]                                                   {'loss': 0.5727, 'grad_norm': 0.4322044551372528, 'learning_rate': 0.00010797148412184057, 'epoch': 0.46}
 46%|████▌     | 1420/3086 [47:40<53:25,  1.92s/it] 46%|████▌     | 1421/3086 [47:42<57:00,  2.05s/it] 46%|████▌     | 1422/3086 [47:44<54:29,  1.97s/it] 46%|████▌     | 1423/3086 [47:46<54:34,  1.97s/it] 46%|████▌     | 1424/3086 [47:48<55:57,  2.02s/it] 46%|████▌     | 1425/3086 [47:50<54:39,  1.97s/it] 46%|████▌     | 1426/3086 [47:52<52:10,  1.89s/it] 46%|████▌     | 1427/3086 [47:54<52:55,  1.91s/it] 46%|████▋     | 1428/3086 [47:56<54:52,  1.99s/it] 46%|████▋     | 1429/3086 [47:58<54:27,  1.97s/it] 46%|████▋     | 1430/3086 [47:59<52:56,  1.92s/it]                                                   {'loss': 0.5765, 'grad_norm': 0.5057425498962402, 'learning_rate': 0.00010732339598185355, 'epoch': 0.46}
 46%|████▋     | 1430/3086 [47:59<52:56,  1.92s/it] 46%|████▋     | 1431/3086 [48:02<57:03,  2.07s/it] 46%|████▋     | 1432/3086 [48:04<56:39,  2.06s/it] 46%|████▋     | 1433/3086 [48:06<53:56,  1.96s/it] 46%|████▋     | 1434/3086 [48:08<54:02,  1.96s/it] 47%|████▋     | 1435/3086 [48:10<54:15,  1.97s/it] 47%|████▋     | 1436/3086 [48:12<57:35,  2.09s/it] 47%|████▋     | 1437/3086 [48:14<54:31,  1.98s/it] 47%|████▋     | 1438/3086 [48:16<56:43,  2.07s/it] 47%|████▋     | 1439/3086 [48:18<58:13,  2.12s/it] 47%|████▋     | 1440/3086 [48:21<1:00:54,  2.22s/it]                                                     {'loss': 0.5819, 'grad_norm': 0.5005276799201965, 'learning_rate': 0.0001066753078418665, 'epoch': 0.47}
 47%|████▋     | 1440/3086 [48:21<1:00:54,  2.22s/it] 47%|████▋     | 1441/3086 [48:23<59:01,  2.15s/it]   47%|████▋     | 1442/3086 [48:25<57:19,  2.09s/it] 47%|████▋     | 1443/3086 [48:27<57:13,  2.09s/it] 47%|████▋     | 1444/3086 [48:29<57:36,  2.10s/it] 47%|████▋     | 1445/3086 [48:31<57:38,  2.11s/it] 47%|████▋     | 1446/3086 [48:33<55:29,  2.03s/it] 47%|████▋     | 1447/3086 [48:35<56:39,  2.07s/it] 47%|████▋     | 1448/3086 [48:37<56:40,  2.08s/it] 47%|████▋     | 1449/3086 [48:39<58:17,  2.14s/it] 47%|████▋     | 1450/3086 [48:42<1:01:15,  2.25s/it]                                                     {'loss': 0.5719, 'grad_norm': 0.434171199798584, 'learning_rate': 0.00010602721970187946, 'epoch': 0.47}
 47%|████▋     | 1450/3086 [48:42<1:01:15,  2.25s/it] 47%|████▋     | 1451/3086 [48:44<58:24,  2.14s/it]   47%|████▋     | 1452/3086 [48:46<55:36,  2.04s/it] 47%|████▋     | 1453/3086 [48:48<55:50,  2.05s/it] 47%|████▋     | 1454/3086 [48:50<56:05,  2.06s/it] 47%|████▋     | 1455/3086 [48:52<54:32,  2.01s/it] 47%|████▋     | 1456/3086 [48:53<53:48,  1.98s/it] 47%|████▋     | 1457/3086 [48:55<49:20,  1.82s/it] 47%|████▋     | 1458/3086 [48:57<50:03,  1.85s/it] 47%|████▋     | 1459/3086 [48:59<49:50,  1.84s/it] 47%|████▋     | 1460/3086 [49:01<52:30,  1.94s/it]                                                   {'loss': 0.5625, 'grad_norm': 0.44738027453422546, 'learning_rate': 0.00010537913156189242, 'epoch': 0.47}
 47%|████▋     | 1460/3086 [49:01<52:30,  1.94s/it] 47%|████▋     | 1461/3086 [49:03<56:59,  2.10s/it] 47%|████▋     | 1462/3086 [49:05<56:54,  2.10s/it] 47%|████▋     | 1463/3086 [49:07<54:04,  2.00s/it] 47%|████▋     | 1464/3086 [49:09<54:12,  2.00s/it] 47%|████▋     | 1465/3086 [49:12<57:18,  2.12s/it] 48%|████▊     | 1466/3086 [49:13<53:52,  2.00s/it] 48%|████▊     | 1467/3086 [49:15<53:45,  1.99s/it] 48%|████▊     | 1468/3086 [49:17<53:30,  1.98s/it] 48%|████▊     | 1469/3086 [49:19<52:20,  1.94s/it] 48%|████▊     | 1470/3086 [49:21<52:39,  1.96s/it]                                                   {'loss': 0.577, 'grad_norm': 0.4152974486351013, 'learning_rate': 0.00010473104342190538, 'epoch': 0.48}
 48%|████▊     | 1470/3086 [49:21<52:39,  1.96s/it] 48%|████▊     | 1471/3086 [49:23<50:59,  1.89s/it] 48%|████▊     | 1472/3086 [49:25<52:47,  1.96s/it] 48%|████▊     | 1473/3086 [49:27<53:54,  2.01s/it] 48%|████▊     | 1474/3086 [49:29<53:17,  1.98s/it] 48%|████▊     | 1475/3086 [49:31<50:59,  1.90s/it] 48%|████▊     | 1476/3086 [49:33<53:39,  2.00s/it] 48%|████▊     | 1477/3086 [49:35<50:58,  1.90s/it] 48%|████▊     | 1478/3086 [49:37<55:33,  2.07s/it] 48%|████▊     | 1479/3086 [49:39<56:14,  2.10s/it] 48%|████▊     | 1480/3086 [49:41<57:34,  2.15s/it]                                                   {'loss': 0.5822, 'grad_norm': 0.5218152403831482, 'learning_rate': 0.00010408295528191833, 'epoch': 0.48}
 48%|████▊     | 1480/3086 [49:41<57:34,  2.15s/it] 48%|████▊     | 1481/3086 [49:43<54:53,  2.05s/it] 48%|████▊     | 1482/3086 [49:46<56:28,  2.11s/it] 48%|████▊     | 1483/3086 [49:47<53:13,  1.99s/it] 48%|████▊     | 1484/3086 [49:49<50:12,  1.88s/it] 48%|████▊     | 1485/3086 [49:51<49:00,  1.84s/it] 48%|████▊     | 1486/3086 [49:52<48:13,  1.81s/it] 48%|████▊     | 1487/3086 [49:54<49:27,  1.86s/it] 48%|████▊     | 1488/3086 [49:56<49:27,  1.86s/it] 48%|████▊     | 1489/3086 [49:58<48:43,  1.83s/it] 48%|████▊     | 1490/3086 [50:00<51:27,  1.93s/it]                                                   {'loss': 0.5758, 'grad_norm': 0.39933574199676514, 'learning_rate': 0.00010343486714193131, 'epoch': 0.48}
 48%|████▊     | 1490/3086 [50:00<51:27,  1.93s/it] 48%|████▊     | 1491/3086 [50:02<53:14,  2.00s/it] 48%|████▊     | 1492/3086 [50:04<54:13,  2.04s/it] 48%|████▊     | 1493/3086 [50:07<56:35,  2.13s/it] 48%|████▊     | 1494/3086 [50:09<55:12,  2.08s/it] 48%|████▊     | 1495/3086 [50:10<51:31,  1.94s/it] 48%|████▊     | 1496/3086 [50:13<53:24,  2.02s/it] 49%|████▊     | 1497/3086 [50:14<52:13,  1.97s/it] 49%|████▊     | 1498/3086 [50:16<53:04,  2.01s/it] 49%|████▊     | 1499/3086 [50:19<57:09,  2.16s/it] 49%|████▊     | 1500/3086 [50:21<53:20,  2.02s/it]                                                   {'loss': 0.5711, 'grad_norm': 0.3651389479637146, 'learning_rate': 0.00010278677900194426, 'epoch': 0.49}
 49%|████▊     | 1500/3086 [50:21<53:20,  2.02s/it] 49%|████▊     | 1501/3086 [50:23<54:18,  2.06s/it] 49%|████▊     | 1502/3086 [50:25<52:38,  1.99s/it] 49%|████▊     | 1503/3086 [50:27<52:22,  1.99s/it] 49%|████▊     | 1504/3086 [50:29<52:48,  2.00s/it] 49%|████▉     | 1505/3086 [50:31<56:47,  2.16s/it] 49%|████▉     | 1506/3086 [50:33<56:12,  2.13s/it] 49%|████▉     | 1507/3086 [50:35<53:28,  2.03s/it] 49%|████▉     | 1508/3086 [50:37<51:08,  1.94s/it] 49%|████▉     | 1509/3086 [50:39<51:01,  1.94s/it] 49%|████▉     | 1510/3086 [50:41<50:28,  1.92s/it]                                                   {'loss': 0.5626, 'grad_norm': 0.33136019110679626, 'learning_rate': 0.00010213869086195722, 'epoch': 0.49}
 49%|████▉     | 1510/3086 [50:41<50:28,  1.92s/it] 49%|████▉     | 1511/3086 [50:43<55:09,  2.10s/it] 49%|████▉     | 1512/3086 [50:45<55:31,  2.12s/it] 49%|████▉     | 1513/3086 [50:47<52:16,  1.99s/it] 49%|████▉     | 1514/3086 [50:49<49:27,  1.89s/it] 49%|████▉     | 1515/3086 [50:50<48:46,  1.86s/it] 49%|████▉     | 1516/3086 [50:53<51:18,  1.96s/it] 49%|████▉     | 1517/3086 [50:55<51:55,  1.99s/it] 49%|████▉     | 1518/3086 [50:57<52:20,  2.00s/it] 49%|████▉     | 1519/3086 [50:59<54:16,  2.08s/it] 49%|████▉     | 1520/3086 [51:01<52:44,  2.02s/it]                                                   {'loss': 0.5702, 'grad_norm': 0.3593662977218628, 'learning_rate': 0.0001014906027219702, 'epoch': 0.49}
 49%|████▉     | 1520/3086 [51:01<52:44,  2.02s/it] 49%|████▉     | 1521/3086 [51:03<51:14,  1.96s/it] 49%|████▉     | 1522/3086 [51:05<50:15,  1.93s/it] 49%|████▉     | 1523/3086 [51:06<50:05,  1.92s/it] 49%|████▉     | 1524/3086 [51:08<49:53,  1.92s/it] 49%|████▉     | 1525/3086 [51:11<51:39,  1.99s/it] 49%|████▉     | 1526/3086 [51:13<54:52,  2.11s/it] 49%|████▉     | 1527/3086 [51:15<52:47,  2.03s/it] 50%|████▉     | 1528/3086 [51:16<49:29,  1.91s/it] 50%|████▉     | 1529/3086 [51:18<50:20,  1.94s/it] 50%|████▉     | 1530/3086 [51:21<54:44,  2.11s/it]                                                   {'loss': 0.5813, 'grad_norm': 0.4409395158290863, 'learning_rate': 0.00010084251458198315, 'epoch': 0.5}
 50%|████▉     | 1530/3086 [51:21<54:44,  2.11s/it] 50%|████▉     | 1531/3086 [51:23<54:28,  2.10s/it] 50%|████▉     | 1532/3086 [51:25<55:54,  2.16s/it] 50%|████▉     | 1533/3086 [51:27<54:21,  2.10s/it] 50%|████▉     | 1534/3086 [51:29<52:19,  2.02s/it] 50%|████▉     | 1535/3086 [51:31<53:40,  2.08s/it] 50%|████▉     | 1536/3086 [51:33<53:30,  2.07s/it] 50%|████▉     | 1537/3086 [51:35<52:49,  2.05s/it] 50%|████▉     | 1538/3086 [51:37<52:22,  2.03s/it] 50%|████▉     | 1539/3086 [51:40<53:24,  2.07s/it] 50%|████▉     | 1540/3086 [51:42<53:34,  2.08s/it]                                                   {'loss': 0.5605, 'grad_norm': 0.4286367893218994, 'learning_rate': 0.00010019442644199612, 'epoch': 0.5}
 50%|████▉     | 1540/3086 [51:42<53:34,  2.08s/it] 50%|████▉     | 1541/3086 [51:44<52:29,  2.04s/it] 50%|████▉     | 1542/3086 [51:46<53:40,  2.09s/it] 50%|█████     | 1543/3086 [51:48<53:02,  2.06s/it] 50%|█████     | 1544/3086 [51:49<50:31,  1.97s/it] 50%|█████     | 1545/3086 [51:51<50:16,  1.96s/it] 50%|█████     | 1546/3086 [51:53<49:20,  1.92s/it] 50%|█████     | 1547/3086 [51:56<51:51,  2.02s/it] 50%|█████     | 1548/3086 [51:58<51:56,  2.03s/it] 50%|█████     | 1549/3086 [51:59<50:42,  1.98s/it] 50%|█████     | 1550/3086 [52:01<49:31,  1.93s/it]                                                   {'loss': 0.5671, 'grad_norm': 0.4429340362548828, 'learning_rate': 9.954633830200909e-05, 'epoch': 0.5}
 50%|█████     | 1550/3086 [52:01<49:31,  1.93s/it] 50%|█████     | 1551/3086 [52:03<51:20,  2.01s/it] 50%|█████     | 1552/3086 [52:05<50:28,  1.97s/it] 50%|█████     | 1553/3086 [52:08<52:41,  2.06s/it] 50%|█████     | 1554/3086 [52:10<52:43,  2.06s/it] 50%|█████     | 1555/3086 [52:11<49:54,  1.96s/it] 50%|█████     | 1556/3086 [52:13<49:22,  1.94s/it] 50%|█████     | 1557/3086 [52:15<47:58,  1.88s/it] 50%|█████     | 1558/3086 [52:18<53:40,  2.11s/it] 51%|█████     | 1559/3086 [52:20<53:32,  2.10s/it] 51%|█████     | 1560/3086 [52:22<50:56,  2.00s/it]                                                   {'loss': 0.5631, 'grad_norm': 0.49840590357780457, 'learning_rate': 9.889825016202204e-05, 'epoch': 0.51}
 51%|█████     | 1560/3086 [52:22<50:56,  2.00s/it] 51%|█████     | 1561/3086 [52:23<48:20,  1.90s/it] 51%|█████     | 1562/3086 [52:25<47:23,  1.87s/it] 51%|█████     | 1563/3086 [52:27<49:30,  1.95s/it] 51%|█████     | 1564/3086 [52:29<47:29,  1.87s/it] 51%|█████     | 1565/3086 [52:31<46:36,  1.84s/it] 51%|█████     | 1566/3086 [52:32<46:59,  1.85s/it] 51%|█████     | 1567/3086 [52:35<49:10,  1.94s/it] 51%|█████     | 1568/3086 [52:37<49:47,  1.97s/it] 51%|█████     | 1569/3086 [52:38<48:29,  1.92s/it] 51%|█████     | 1570/3086 [52:41<50:48,  2.01s/it]                                                   {'loss': 0.5754, 'grad_norm': 0.4271337687969208, 'learning_rate': 9.825016202203499e-05, 'epoch': 0.51}
 51%|█████     | 1570/3086 [52:41<50:48,  2.01s/it] 51%|█████     | 1571/3086 [52:44<58:18,  2.31s/it] 51%|█████     | 1572/3086 [52:45<53:53,  2.14s/it] 51%|█████     | 1573/3086 [52:48<57:15,  2.27s/it] 51%|█████     | 1574/3086 [52:50<55:00,  2.18s/it] 51%|█████     | 1575/3086 [52:52<50:52,  2.02s/it] 51%|█████     | 1576/3086 [52:54<54:30,  2.17s/it] 51%|█████     | 1577/3086 [52:56<51:07,  2.03s/it] 51%|█████     | 1578/3086 [52:57<48:15,  1.92s/it] 51%|█████     | 1579/3086 [53:00<50:22,  2.01s/it] 51%|█████     | 1580/3086 [53:02<49:37,  1.98s/it]                                                   {'loss': 0.5737, 'grad_norm': 0.4449998438358307, 'learning_rate': 9.760207388204796e-05, 'epoch': 0.51}
 51%|█████     | 1580/3086 [53:02<49:37,  1.98s/it] 51%|█████     | 1581/3086 [53:03<47:50,  1.91s/it] 51%|█████▏    | 1582/3086 [53:05<47:28,  1.89s/it] 51%|█████▏    | 1583/3086 [53:07<47:46,  1.91s/it] 51%|█████▏    | 1584/3086 [53:09<45:58,  1.84s/it] 51%|█████▏    | 1585/3086 [53:10<44:46,  1.79s/it] 51%|█████▏    | 1586/3086 [53:12<45:36,  1.82s/it] 51%|█████▏    | 1587/3086 [53:15<48:19,  1.93s/it] 51%|█████▏    | 1588/3086 [53:16<48:02,  1.92s/it] 51%|█████▏    | 1589/3086 [53:19<50:27,  2.02s/it] 52%|█████▏    | 1590/3086 [53:21<49:24,  1.98s/it]                                                   {'loss': 0.565, 'grad_norm': 0.3852003514766693, 'learning_rate': 9.695398574206093e-05, 'epoch': 0.52}
 52%|█████▏    | 1590/3086 [53:21<49:24,  1.98s/it] 52%|█████▏    | 1591/3086 [53:23<53:44,  2.16s/it] 52%|█████▏    | 1592/3086 [53:25<51:41,  2.08s/it] 52%|█████▏    | 1593/3086 [53:27<51:40,  2.08s/it] 52%|█████▏    | 1594/3086 [53:29<51:58,  2.09s/it] 52%|█████▏    | 1595/3086 [53:31<50:21,  2.03s/it] 52%|█████▏    | 1596/3086 [53:33<48:02,  1.93s/it] 52%|█████▏    | 1597/3086 [53:34<45:08,  1.82s/it] 52%|█████▏    | 1598/3086 [53:37<47:19,  1.91s/it] 52%|█████▏    | 1599/3086 [53:39<48:48,  1.97s/it] 52%|█████▏    | 1600/3086 [53:41<48:49,  1.97s/it]                                                   {'loss': 0.5686, 'grad_norm': 0.555983304977417, 'learning_rate': 9.630589760207389e-05, 'epoch': 0.52}
 52%|█████▏    | 1600/3086 [53:41<48:49,  1.97s/it] 52%|█████▏    | 1601/3086 [53:43<54:22,  2.20s/it] 52%|█████▏    | 1602/3086 [53:45<51:58,  2.10s/it] 52%|█████▏    | 1603/3086 [53:47<50:52,  2.06s/it] 52%|█████▏    | 1604/3086 [53:49<49:50,  2.02s/it] 52%|█████▏    | 1605/3086 [53:51<47:12,  1.91s/it] 52%|█████▏    | 1606/3086 [53:53<48:04,  1.95s/it] 52%|█████▏    | 1607/3086 [53:55<48:55,  1.98s/it] 52%|█████▏    | 1608/3086 [53:57<52:31,  2.13s/it] 52%|█████▏    | 1609/3086 [53:59<51:22,  2.09s/it] 52%|█████▏    | 1610/3086 [54:02<55:00,  2.24s/it]                                                   {'loss': 0.579, 'grad_norm': 0.5676233172416687, 'learning_rate': 9.565780946208685e-05, 'epoch': 0.52}
 52%|█████▏    | 1610/3086 [54:02<55:00,  2.24s/it] 52%|█████▏    | 1611/3086 [54:04<55:50,  2.27s/it] 52%|█████▏    | 1612/3086 [54:06<50:40,  2.06s/it] 52%|█████▏    | 1613/3086 [54:08<53:14,  2.17s/it] 52%|█████▏    | 1614/3086 [54:10<50:30,  2.06s/it] 52%|█████▏    | 1615/3086 [54:12<52:42,  2.15s/it] 52%|█████▏    | 1616/3086 [54:14<50:10,  2.05s/it] 52%|█████▏    | 1617/3086 [54:16<50:56,  2.08s/it] 52%|█████▏    | 1618/3086 [54:18<49:16,  2.01s/it] 52%|█████▏    | 1619/3086 [54:20<48:20,  1.98s/it] 52%|█████▏    | 1620/3086 [54:22<49:52,  2.04s/it]                                                   {'loss': 0.5633, 'grad_norm': 0.4218965172767639, 'learning_rate': 9.500972132209981e-05, 'epoch': 0.52}
 52%|█████▏    | 1620/3086 [54:22<49:52,  2.04s/it] 53%|█████▎    | 1621/3086 [54:24<48:33,  1.99s/it] 53%|█████▎    | 1622/3086 [54:26<46:50,  1.92s/it] 53%|█████▎    | 1623/3086 [54:28<49:52,  2.05s/it] 53%|█████▎    | 1624/3086 [54:30<45:57,  1.89s/it] 53%|█████▎    | 1625/3086 [54:32<47:18,  1.94s/it] 53%|█████▎    | 1626/3086 [54:34<47:55,  1.97s/it] 53%|█████▎    | 1627/3086 [54:36<48:26,  1.99s/it] 53%|█████▎    | 1628/3086 [54:38<47:02,  1.94s/it] 53%|█████▎    | 1629/3086 [54:40<52:23,  2.16s/it] 53%|█████▎    | 1630/3086 [54:43<54:22,  2.24s/it]                                                   {'loss': 0.5735, 'grad_norm': 0.3954465687274933, 'learning_rate': 9.436163318211278e-05, 'epoch': 0.53}
 53%|█████▎    | 1630/3086 [54:43<54:22,  2.24s/it] 53%|█████▎    | 1631/3086 [54:45<51:55,  2.14s/it] 53%|█████▎    | 1632/3086 [54:47<49:09,  2.03s/it] 53%|█████▎    | 1633/3086 [54:48<46:25,  1.92s/it] 53%|█████▎    | 1634/3086 [54:50<46:38,  1.93s/it] 53%|█████▎    | 1635/3086 [54:52<45:00,  1.86s/it] 53%|█████▎    | 1636/3086 [54:54<48:23,  2.00s/it] 53%|█████▎    | 1637/3086 [54:57<52:22,  2.17s/it] 53%|█████▎    | 1638/3086 [54:59<51:49,  2.15s/it] 53%|█████▎    | 1639/3086 [55:01<49:48,  2.06s/it] 53%|█████▎    | 1640/3086 [55:03<51:03,  2.12s/it]                                                   {'loss': 0.5743, 'grad_norm': 0.41715773940086365, 'learning_rate': 9.371354504212573e-05, 'epoch': 0.53}
 53%|█████▎    | 1640/3086 [55:03<51:03,  2.12s/it] 53%|█████▎    | 1641/3086 [55:05<49:54,  2.07s/it] 53%|█████▎    | 1642/3086 [55:07<48:45,  2.03s/it] 53%|█████▎    | 1643/3086 [55:09<47:54,  1.99s/it] 53%|█████▎    | 1644/3086 [55:11<48:11,  2.01s/it] 53%|█████▎    | 1645/3086 [55:13<50:10,  2.09s/it] 53%|█████▎    | 1646/3086 [55:15<48:32,  2.02s/it] 53%|█████▎    | 1647/3086 [55:17<48:49,  2.04s/it] 53%|█████▎    | 1648/3086 [55:19<48:14,  2.01s/it] 53%|█████▎    | 1649/3086 [55:21<48:27,  2.02s/it] 53%|█████▎    | 1650/3086 [55:23<47:07,  1.97s/it]                                                   {'loss': 0.5761, 'grad_norm': 0.43033650517463684, 'learning_rate': 9.306545690213869e-05, 'epoch': 0.53}
 53%|█████▎    | 1650/3086 [55:23<47:07,  1.97s/it] 53%|█████▎    | 1651/3086 [55:25<44:46,  1.87s/it] 54%|█████▎    | 1652/3086 [55:26<43:47,  1.83s/it] 54%|█████▎    | 1653/3086 [55:28<43:45,  1.83s/it] 54%|█████▎    | 1654/3086 [55:30<43:22,  1.82s/it] 54%|█████▎    | 1655/3086 [55:32<45:30,  1.91s/it] 54%|█████▎    | 1656/3086 [55:34<46:05,  1.93s/it] 54%|█████▎    | 1657/3086 [55:35<42:40,  1.79s/it] 54%|█████▎    | 1658/3086 [55:38<46:04,  1.94s/it] 54%|█████▍    | 1659/3086 [55:40<46:09,  1.94s/it] 54%|█████▍    | 1660/3086 [55:42<46:39,  1.96s/it]                                                   {'loss': 0.5725, 'grad_norm': 0.44629397988319397, 'learning_rate': 9.241736876215165e-05, 'epoch': 0.54}
 54%|█████▍    | 1660/3086 [55:42<46:39,  1.96s/it] 54%|█████▍    | 1661/3086 [55:44<45:32,  1.92s/it] 54%|█████▍    | 1662/3086 [55:46<46:48,  1.97s/it] 54%|█████▍    | 1663/3086 [55:47<45:16,  1.91s/it] 54%|█████▍    | 1664/3086 [55:49<44:02,  1.86s/it] 54%|█████▍    | 1665/3086 [55:51<46:04,  1.95s/it] 54%|█████▍    | 1666/3086 [55:53<47:07,  1.99s/it] 54%|█████▍    | 1667/3086 [55:56<50:29,  2.14s/it] 54%|█████▍    | 1668/3086 [55:58<48:17,  2.04s/it] 54%|█████▍    | 1669/3086 [55:59<46:30,  1.97s/it] 54%|█████▍    | 1670/3086 [56:01<45:11,  1.91s/it]                                                   {'loss': 0.5713, 'grad_norm': 0.3830808997154236, 'learning_rate': 9.176928062216462e-05, 'epoch': 0.54}
 54%|█████▍    | 1670/3086 [56:01<45:11,  1.91s/it] 54%|█████▍    | 1671/3086 [56:03<47:23,  2.01s/it] 54%|█████▍    | 1672/3086 [56:05<45:41,  1.94s/it] 54%|█████▍    | 1673/3086 [56:07<45:38,  1.94s/it] 54%|█████▍    | 1674/3086 [56:09<44:06,  1.87s/it] 54%|█████▍    | 1675/3086 [56:11<43:01,  1.83s/it] 54%|█████▍    | 1676/3086 [56:13<43:50,  1.87s/it] 54%|█████▍    | 1677/3086 [56:15<45:27,  1.94s/it] 54%|█████▍    | 1678/3086 [56:17<48:22,  2.06s/it] 54%|█████▍    | 1679/3086 [56:19<46:41,  1.99s/it] 54%|█████▍    | 1680/3086 [56:21<47:30,  2.03s/it]                                                   {'loss': 0.57, 'grad_norm': 0.40427690744400024, 'learning_rate': 9.112119248217757e-05, 'epoch': 0.54}
 54%|█████▍    | 1680/3086 [56:21<47:30,  2.03s/it] 54%|█████▍    | 1681/3086 [56:23<44:56,  1.92s/it] 55%|█████▍    | 1682/3086 [56:24<43:15,  1.85s/it] 55%|█████▍    | 1683/3086 [56:26<42:19,  1.81s/it] 55%|█████▍    | 1684/3086 [56:28<42:08,  1.80s/it] 55%|█████▍    | 1685/3086 [56:30<42:43,  1.83s/it] 55%|█████▍    | 1686/3086 [56:32<45:57,  1.97s/it] 55%|█████▍    | 1687/3086 [56:34<46:15,  1.98s/it] 55%|█████▍    | 1688/3086 [56:36<48:30,  2.08s/it] 55%|█████▍    | 1689/3086 [56:38<47:51,  2.06s/it] 55%|█████▍    | 1690/3086 [56:40<45:43,  1.97s/it]                                                   {'loss': 0.5674, 'grad_norm': 0.4293645918369293, 'learning_rate': 9.047310434219054e-05, 'epoch': 0.55}
 55%|█████▍    | 1690/3086 [56:40<45:43,  1.97s/it] 55%|█████▍    | 1691/3086 [56:42<45:29,  1.96s/it] 55%|█████▍    | 1692/3086 [56:44<44:28,  1.91s/it] 55%|█████▍    | 1693/3086 [56:46<46:27,  2.00s/it] 55%|█████▍    | 1694/3086 [56:48<47:07,  2.03s/it] 55%|█████▍    | 1695/3086 [56:50<47:34,  2.05s/it] 55%|█████▍    | 1696/3086 [56:52<47:02,  2.03s/it] 55%|█████▍    | 1697/3086 [56:55<53:18,  2.30s/it] 55%|█████▌    | 1698/3086 [56:57<48:30,  2.10s/it] 55%|█████▌    | 1699/3086 [56:59<48:06,  2.08s/it] 55%|█████▌    | 1700/3086 [57:01<47:56,  2.08s/it]                                                   {'loss': 0.5707, 'grad_norm': 0.3893655836582184, 'learning_rate': 8.982501620220351e-05, 'epoch': 0.55}
 55%|█████▌    | 1700/3086 [57:01<47:56,  2.08s/it] 55%|█████▌    | 1701/3086 [57:03<49:58,  2.16s/it] 55%|█████▌    | 1702/3086 [57:05<46:31,  2.02s/it] 55%|█████▌    | 1703/3086 [57:07<46:38,  2.02s/it] 55%|█████▌    | 1704/3086 [57:09<44:06,  1.91s/it] 55%|█████▌    | 1705/3086 [57:11<47:06,  2.05s/it] 55%|█████▌    | 1706/3086 [57:13<48:53,  2.13s/it] 55%|█████▌    | 1707/3086 [57:15<47:48,  2.08s/it] 55%|█████▌    | 1708/3086 [57:17<45:42,  1.99s/it] 55%|█████▌    | 1709/3086 [57:19<44:58,  1.96s/it] 55%|█████▌    | 1710/3086 [57:21<45:38,  1.99s/it]                                                   {'loss': 0.5682, 'grad_norm': 0.37282049655914307, 'learning_rate': 8.917692806221647e-05, 'epoch': 0.55}
 55%|█████▌    | 1710/3086 [57:21<45:38,  1.99s/it] 55%|█████▌    | 1711/3086 [57:24<51:01,  2.23s/it] 55%|█████▌    | 1712/3086 [57:26<53:08,  2.32s/it] 56%|█████▌    | 1713/3086 [57:29<52:05,  2.28s/it] 56%|█████▌    | 1714/3086 [57:30<47:43,  2.09s/it] 56%|█████▌    | 1715/3086 [57:33<49:38,  2.17s/it] 56%|█████▌    | 1716/3086 [57:34<46:51,  2.05s/it] 56%|█████▌    | 1717/3086 [57:36<42:36,  1.87s/it] 56%|█████▌    | 1718/3086 [57:38<47:19,  2.08s/it] 56%|█████▌    | 1719/3086 [57:40<44:56,  1.97s/it] 56%|█████▌    | 1720/3086 [57:42<42:32,  1.87s/it]                                                   {'loss': 0.5662, 'grad_norm': 0.45706290006637573, 'learning_rate': 8.852883992222943e-05, 'epoch': 0.56}
 56%|█████▌    | 1720/3086 [57:42<42:32,  1.87s/it] 56%|█████▌    | 1721/3086 [57:43<41:49,  1.84s/it] 56%|█████▌    | 1722/3086 [57:45<43:03,  1.89s/it] 56%|█████▌    | 1723/3086 [57:48<47:23,  2.09s/it] 56%|█████▌    | 1724/3086 [57:50<45:55,  2.02s/it] 56%|█████▌    | 1725/3086 [57:52<47:23,  2.09s/it] 56%|█████▌    | 1726/3086 [57:54<45:05,  1.99s/it] 56%|█████▌    | 1727/3086 [57:56<43:30,  1.92s/it] 56%|█████▌    | 1728/3086 [57:57<41:40,  1.84s/it] 56%|█████▌    | 1729/3086 [57:59<42:00,  1.86s/it] 56%|█████▌    | 1730/3086 [58:01<41:46,  1.85s/it]                                                   {'loss': 0.5765, 'grad_norm': 0.5171365141868591, 'learning_rate': 8.78807517822424e-05, 'epoch': 0.56}
 56%|█████▌    | 1730/3086 [58:01<41:46,  1.85s/it] 56%|█████▌    | 1731/3086 [58:03<44:21,  1.96s/it] 56%|█████▌    | 1732/3086 [58:06<46:38,  2.07s/it] 56%|█████▌    | 1733/3086 [58:07<45:49,  2.03s/it] 56%|█████▌    | 1734/3086 [58:09<45:32,  2.02s/it] 56%|█████▌    | 1735/3086 [58:11<44:53,  1.99s/it] 56%|█████▋    | 1736/3086 [58:14<45:31,  2.02s/it] 56%|█████▋    | 1737/3086 [58:16<45:31,  2.02s/it] 56%|█████▋    | 1738/3086 [58:17<43:08,  1.92s/it] 56%|█████▋    | 1739/3086 [58:19<45:17,  2.02s/it] 56%|█████▋    | 1740/3086 [58:22<50:20,  2.24s/it]                                                   {'loss': 0.5508, 'grad_norm': 0.3143834173679352, 'learning_rate': 8.723266364225535e-05, 'epoch': 0.56}
 56%|█████▋    | 1740/3086 [58:22<50:20,  2.24s/it] 56%|█████▋    | 1741/3086 [58:24<44:16,  1.97s/it] 56%|█████▋    | 1742/3086 [58:26<45:01,  2.01s/it] 56%|█████▋    | 1743/3086 [58:28<46:14,  2.07s/it] 57%|█████▋    | 1744/3086 [58:30<45:06,  2.02s/it] 57%|█████▋    | 1745/3086 [58:32<46:30,  2.08s/it] 57%|█████▋    | 1746/3086 [58:34<44:18,  1.98s/it] 57%|█████▋    | 1747/3086 [58:35<41:13,  1.85s/it] 57%|█████▋    | 1748/3086 [58:38<44:57,  2.02s/it] 57%|█████▋    | 1749/3086 [58:40<45:47,  2.06s/it] 57%|█████▋    | 1750/3086 [58:42<43:29,  1.95s/it]                                                   {'loss': 0.5813, 'grad_norm': 0.3768692910671234, 'learning_rate': 8.658457550226831e-05, 'epoch': 0.57}
 57%|█████▋    | 1750/3086 [58:42<43:29,  1.95s/it] 57%|█████▋    | 1751/3086 [58:43<41:16,  1.86s/it] 57%|█████▋    | 1752/3086 [58:45<41:33,  1.87s/it] 57%|█████▋    | 1753/3086 [58:47<40:47,  1.84s/it] 57%|█████▋    | 1754/3086 [58:48<39:22,  1.77s/it] 57%|█████▋    | 1755/3086 [58:50<39:34,  1.78s/it] 57%|█████▋    | 1756/3086 [58:53<43:21,  1.96s/it] 57%|█████▋    | 1757/3086 [58:54<42:15,  1.91s/it] 57%|█████▋    | 1758/3086 [58:57<43:25,  1.96s/it] 57%|█████▋    | 1759/3086 [58:59<44:14,  2.00s/it] 57%|█████▋    | 1760/3086 [59:01<43:59,  1.99s/it]                                                   {'loss': 0.5799, 'grad_norm': 0.44667863845825195, 'learning_rate': 8.593648736228127e-05, 'epoch': 0.57}
 57%|█████▋    | 1760/3086 [59:01<43:59,  1.99s/it] 57%|█████▋    | 1761/3086 [59:03<44:28,  2.01s/it] 57%|█████▋    | 1762/3086 [59:05<47:06,  2.13s/it] 57%|█████▋    | 1763/3086 [59:07<47:45,  2.17s/it] 57%|█████▋    | 1764/3086 [59:09<47:13,  2.14s/it] 57%|█████▋    | 1765/3086 [59:11<44:49,  2.04s/it] 57%|█████▋    | 1766/3086 [59:13<45:49,  2.08s/it] 57%|█████▋    | 1767/3086 [59:15<44:49,  2.04s/it] 57%|█████▋    | 1768/3086 [59:17<42:04,  1.92s/it] 57%|█████▋    | 1769/3086 [59:19<43:04,  1.96s/it] 57%|█████▋    | 1770/3086 [59:21<43:26,  1.98s/it]                                                   {'loss': 0.5814, 'grad_norm': 0.5626095533370972, 'learning_rate': 8.528839922229423e-05, 'epoch': 0.57}
 57%|█████▋    | 1770/3086 [59:21<43:26,  1.98s/it] 57%|█████▋    | 1771/3086 [59:23<42:26,  1.94s/it] 57%|█████▋    | 1772/3086 [59:25<44:21,  2.03s/it] 57%|█████▋    | 1773/3086 [59:27<42:55,  1.96s/it] 57%|█████▋    | 1774/3086 [59:29<44:55,  2.05s/it] 58%|█████▊    | 1775/3086 [59:31<44:17,  2.03s/it] 58%|█████▊    | 1776/3086 [59:34<46:52,  2.15s/it] 58%|█████▊    | 1777/3086 [59:35<45:21,  2.08s/it] 58%|█████▊    | 1778/3086 [59:38<48:35,  2.23s/it] 58%|█████▊    | 1779/3086 [59:40<48:16,  2.22s/it] 58%|█████▊    | 1780/3086 [59:42<45:05,  2.07s/it]                                                   {'loss': 0.5608, 'grad_norm': 0.43200457096099854, 'learning_rate': 8.46403110823072e-05, 'epoch': 0.58}
 58%|█████▊    | 1780/3086 [59:42<45:05,  2.07s/it] 58%|█████▊    | 1781/3086 [59:44<45:20,  2.08s/it] 58%|█████▊    | 1782/3086 [59:46<44:14,  2.04s/it] 58%|█████▊    | 1783/3086 [59:48<43:26,  2.00s/it] 58%|█████▊    | 1784/3086 [59:50<42:48,  1.97s/it] 58%|█████▊    | 1785/3086 [59:52<43:21,  2.00s/it] 58%|█████▊    | 1786/3086 [59:54<45:12,  2.09s/it] 58%|█████▊    | 1787/3086 [59:56<44:50,  2.07s/it] 58%|█████▊    | 1788/3086 [59:58<43:51,  2.03s/it] 58%|█████▊    | 1789/3086 [1:00:00<41:48,  1.93s/it] 58%|█████▊    | 1790/3086 [1:00:02<40:09,  1.86s/it]                                                     {'loss': 0.5715, 'grad_norm': 0.4768967628479004, 'learning_rate': 8.399222294232017e-05, 'epoch': 0.58}
 58%|█████▊    | 1790/3086 [1:00:02<40:09,  1.86s/it] 58%|█████▊    | 1791/3086 [1:00:03<39:57,  1.85s/it] 58%|█████▊    | 1792/3086 [1:00:06<42:27,  1.97s/it] 58%|█████▊    | 1793/3086 [1:00:08<43:55,  2.04s/it] 58%|█████▊    | 1794/3086 [1:00:10<42:23,  1.97s/it] 58%|█████▊    | 1795/3086 [1:00:12<44:12,  2.05s/it] 58%|█████▊    | 1796/3086 [1:00:14<41:41,  1.94s/it] 58%|█████▊    | 1797/3086 [1:00:15<40:26,  1.88s/it] 58%|█████▊    | 1798/3086 [1:00:17<41:21,  1.93s/it] 58%|█████▊    | 1799/3086 [1:00:20<46:06,  2.15s/it] 58%|█████▊    | 1800/3086 [1:00:22<44:28,  2.08s/it]                                                     {'loss': 0.5601, 'grad_norm': 0.5761062502861023, 'learning_rate': 8.334413480233312e-05, 'epoch': 0.58}
 58%|█████▊    | 1800/3086 [1:00:22<44:28,  2.08s/it] 58%|█████▊    | 1801/3086 [1:00:24<43:12,  2.02s/it] 58%|█████▊    | 1802/3086 [1:00:25<40:21,  1.89s/it] 58%|█████▊    | 1803/3086 [1:00:28<44:36,  2.09s/it] 58%|█████▊    | 1804/3086 [1:00:30<47:18,  2.21s/it] 58%|█████▊    | 1805/3086 [1:00:32<44:52,  2.10s/it] 59%|█████▊    | 1806/3086 [1:00:34<45:08,  2.12s/it] 59%|█████▊    | 1807/3086 [1:00:36<43:06,  2.02s/it] 59%|█████▊    | 1808/3086 [1:00:38<42:14,  1.98s/it] 59%|█████▊    | 1809/3086 [1:00:40<44:24,  2.09s/it] 59%|█████▊    | 1810/3086 [1:00:43<45:30,  2.14s/it]                                                     {'loss': 0.5577, 'grad_norm': 0.4593920409679413, 'learning_rate': 8.269604666234609e-05, 'epoch': 0.59}
 59%|█████▊    | 1810/3086 [1:00:43<45:30,  2.14s/it] 59%|█████▊    | 1811/3086 [1:00:45<46:21,  2.18s/it] 59%|█████▊    | 1812/3086 [1:00:47<43:45,  2.06s/it] 59%|█████▊    | 1813/3086 [1:00:49<42:54,  2.02s/it] 59%|█████▉    | 1814/3086 [1:00:51<42:29,  2.00s/it] 59%|█████▉    | 1815/3086 [1:00:54<47:49,  2.26s/it] 59%|█████▉    | 1816/3086 [1:00:56<46:46,  2.21s/it] 59%|█████▉    | 1817/3086 [1:00:58<46:02,  2.18s/it] 59%|█████▉    | 1818/3086 [1:00:59<42:31,  2.01s/it] 59%|█████▉    | 1819/3086 [1:01:01<41:20,  1.96s/it] 59%|█████▉    | 1820/3086 [1:01:03<40:29,  1.92s/it]                                                     {'loss': 0.5687, 'grad_norm': 0.46571633219718933, 'learning_rate': 8.204795852235904e-05, 'epoch': 0.59}
 59%|█████▉    | 1820/3086 [1:01:03<40:29,  1.92s/it] 59%|█████▉    | 1821/3086 [1:01:05<41:06,  1.95s/it] 59%|█████▉    | 1822/3086 [1:01:07<40:27,  1.92s/it] 59%|█████▉    | 1823/3086 [1:01:09<40:35,  1.93s/it] 59%|█████▉    | 1824/3086 [1:01:11<40:48,  1.94s/it] 59%|█████▉    | 1825/3086 [1:01:12<37:55,  1.80s/it] 59%|█████▉    | 1826/3086 [1:01:14<40:05,  1.91s/it] 59%|█████▉    | 1827/3086 [1:01:16<40:52,  1.95s/it] 59%|█████▉    | 1828/3086 [1:01:19<41:29,  1.98s/it] 59%|█████▉    | 1829/3086 [1:01:21<41:25,  1.98s/it] 59%|█████▉    | 1830/3086 [1:01:23<41:57,  2.00s/it]                                                     {'loss': 0.5615, 'grad_norm': 0.4337233901023865, 'learning_rate': 8.1399870382372e-05, 'epoch': 0.59}
 59%|█████▉    | 1830/3086 [1:01:23<41:57,  2.00s/it] 59%|█████▉    | 1831/3086 [1:01:25<45:10,  2.16s/it] 59%|█████▉    | 1832/3086 [1:01:27<44:44,  2.14s/it] 59%|█████▉    | 1833/3086 [1:01:29<42:38,  2.04s/it] 59%|█████▉    | 1834/3086 [1:01:31<42:00,  2.01s/it] 59%|█████▉    | 1835/3086 [1:01:33<40:34,  1.95s/it] 59%|█████▉    | 1836/3086 [1:01:35<41:02,  1.97s/it] 60%|█████▉    | 1837/3086 [1:01:37<43:35,  2.09s/it] 60%|█████▉    | 1838/3086 [1:01:39<41:56,  2.02s/it] 60%|█████▉    | 1839/3086 [1:01:41<44:28,  2.14s/it] 60%|█████▉    | 1840/3086 [1:01:43<43:33,  2.10s/it]                                                     {'loss': 0.5651, 'grad_norm': 0.48462173342704773, 'learning_rate': 8.075178224238496e-05, 'epoch': 0.6}
 60%|█████▉    | 1840/3086 [1:01:43<43:33,  2.10s/it] 60%|█████▉    | 1841/3086 [1:01:45<41:21,  1.99s/it] 60%|█████▉    | 1842/3086 [1:01:47<39:29,  1.90s/it] 60%|█████▉    | 1843/3086 [1:01:49<41:06,  1.98s/it] 60%|█████▉    | 1844/3086 [1:01:51<41:22,  2.00s/it] 60%|█████▉    | 1845/3086 [1:01:53<40:45,  1.97s/it] 60%|█████▉    | 1846/3086 [1:01:55<42:12,  2.04s/it] 60%|█████▉    | 1847/3086 [1:01:57<42:54,  2.08s/it] 60%|█████▉    | 1848/3086 [1:01:59<42:17,  2.05s/it] 60%|█████▉    | 1849/3086 [1:02:01<42:07,  2.04s/it] 60%|█████▉    | 1850/3086 [1:02:03<39:24,  1.91s/it]                                                     {'loss': 0.562, 'grad_norm': 0.4807625412940979, 'learning_rate': 8.010369410239793e-05, 'epoch': 0.6}
 60%|█████▉    | 1850/3086 [1:02:03<39:24,  1.91s/it] 60%|█████▉    | 1851/3086 [1:02:06<43:40,  2.12s/it] 60%|██████    | 1852/3086 [1:02:08<43:07,  2.10s/it] 60%|██████    | 1853/3086 [1:02:09<41:18,  2.01s/it] 60%|██████    | 1854/3086 [1:02:11<39:40,  1.93s/it] 60%|██████    | 1855/3086 [1:02:13<42:00,  2.05s/it] 60%|██████    | 1856/3086 [1:02:16<42:03,  2.05s/it] 60%|██████    | 1857/3086 [1:02:17<38:41,  1.89s/it] 60%|██████    | 1858/3086 [1:02:19<39:30,  1.93s/it] 60%|██████    | 1859/3086 [1:02:21<39:37,  1.94s/it] 60%|██████    | 1860/3086 [1:02:23<41:05,  2.01s/it]                                                     {'loss': 0.5798, 'grad_norm': 0.4368000626564026, 'learning_rate': 7.94556059624109e-05, 'epoch': 0.6}
 60%|██████    | 1860/3086 [1:02:23<41:05,  2.01s/it] 60%|██████    | 1861/3086 [1:02:25<42:33,  2.08s/it] 60%|██████    | 1862/3086 [1:02:28<43:06,  2.11s/it] 60%|██████    | 1863/3086 [1:02:29<40:42,  2.00s/it] 60%|██████    | 1864/3086 [1:02:31<39:32,  1.94s/it] 60%|██████    | 1865/3086 [1:02:33<38:36,  1.90s/it] 60%|██████    | 1866/3086 [1:02:35<39:26,  1.94s/it] 60%|██████    | 1867/3086 [1:02:37<40:10,  1.98s/it] 61%|██████    | 1868/3086 [1:02:39<38:42,  1.91s/it] 61%|██████    | 1869/3086 [1:02:41<37:49,  1.86s/it] 61%|██████    | 1870/3086 [1:02:43<41:36,  2.05s/it]                                                     {'loss': 0.5668, 'grad_norm': 0.4687539041042328, 'learning_rate': 7.880751782242385e-05, 'epoch': 0.61}
 61%|██████    | 1870/3086 [1:02:43<41:36,  2.05s/it] 61%|██████    | 1871/3086 [1:02:45<40:25,  2.00s/it] 61%|██████    | 1872/3086 [1:02:47<38:47,  1.92s/it] 61%|██████    | 1873/3086 [1:02:48<37:56,  1.88s/it] 61%|██████    | 1874/3086 [1:02:50<37:37,  1.86s/it] 61%|██████    | 1875/3086 [1:02:52<37:38,  1.87s/it] 61%|██████    | 1876/3086 [1:02:54<38:43,  1.92s/it] 61%|██████    | 1877/3086 [1:02:57<41:12,  2.04s/it] 61%|██████    | 1878/3086 [1:02:59<41:05,  2.04s/it] 61%|██████    | 1879/3086 [1:03:00<39:55,  1.98s/it] 61%|██████    | 1880/3086 [1:03:03<40:22,  2.01s/it]                                                     {'loss': 0.5634, 'grad_norm': 0.4368729293346405, 'learning_rate': 7.815942968243682e-05, 'epoch': 0.61}
 61%|██████    | 1880/3086 [1:03:03<40:22,  2.01s/it] 61%|██████    | 1881/3086 [1:03:05<40:35,  2.02s/it] 61%|██████    | 1882/3086 [1:03:07<40:45,  2.03s/it] 61%|██████    | 1883/3086 [1:03:09<40:50,  2.04s/it] 61%|██████    | 1884/3086 [1:03:10<37:53,  1.89s/it] 61%|██████    | 1885/3086 [1:03:12<36:08,  1.81s/it] 61%|██████    | 1886/3086 [1:03:14<35:37,  1.78s/it] 61%|██████    | 1887/3086 [1:03:16<37:11,  1.86s/it] 61%|██████    | 1888/3086 [1:03:18<41:34,  2.08s/it] 61%|██████    | 1889/3086 [1:03:20<40:31,  2.03s/it] 61%|██████    | 1890/3086 [1:03:22<39:22,  1.98s/it]                                                     {'loss': 0.5672, 'grad_norm': 0.4000854790210724, 'learning_rate': 7.751134154244978e-05, 'epoch': 0.61}
 61%|██████    | 1890/3086 [1:03:22<39:22,  1.98s/it] 61%|██████▏   | 1891/3086 [1:03:25<42:57,  2.16s/it] 61%|██████▏   | 1892/3086 [1:03:27<42:57,  2.16s/it] 61%|██████▏   | 1893/3086 [1:03:28<39:36,  1.99s/it] 61%|██████▏   | 1894/3086 [1:03:30<38:35,  1.94s/it] 61%|██████▏   | 1895/3086 [1:03:32<38:03,  1.92s/it] 61%|██████▏   | 1896/3086 [1:03:34<39:31,  1.99s/it] 61%|██████▏   | 1897/3086 [1:03:36<37:04,  1.87s/it] 62%|██████▏   | 1898/3086 [1:03:38<38:26,  1.94s/it] 62%|██████▏   | 1899/3086 [1:03:40<39:21,  1.99s/it] 62%|██████▏   | 1900/3086 [1:03:42<39:20,  1.99s/it]                                                     {'loss': 0.5604, 'grad_norm': 0.3510276973247528, 'learning_rate': 7.686325340246275e-05, 'epoch': 0.62}
 62%|██████▏   | 1900/3086 [1:03:42<39:20,  1.99s/it] 62%|██████▏   | 1901/3086 [1:03:44<39:12,  1.99s/it] 62%|██████▏   | 1902/3086 [1:03:45<36:47,  1.86s/it] 62%|██████▏   | 1903/3086 [1:03:47<37:34,  1.91s/it] 62%|██████▏   | 1904/3086 [1:03:50<39:45,  2.02s/it] 62%|██████▏   | 1905/3086 [1:03:52<40:22,  2.05s/it] 62%|██████▏   | 1906/3086 [1:03:54<39:47,  2.02s/it] 62%|██████▏   | 1907/3086 [1:03:56<39:10,  1.99s/it] 62%|██████▏   | 1908/3086 [1:03:58<38:27,  1.96s/it] 62%|██████▏   | 1909/3086 [1:04:00<38:12,  1.95s/it] 62%|██████▏   | 1910/3086 [1:04:01<37:59,  1.94s/it]                                                     {'loss': 0.562, 'grad_norm': 0.43603384494781494, 'learning_rate': 7.62151652624757e-05, 'epoch': 0.62}
 62%|██████▏   | 1910/3086 [1:04:01<37:59,  1.94s/it] 62%|██████▏   | 1911/3086 [1:04:03<36:03,  1.84s/it] 62%|██████▏   | 1912/3086 [1:04:05<38:57,  1.99s/it] 62%|██████▏   | 1913/3086 [1:04:08<40:15,  2.06s/it] 62%|██████▏   | 1914/3086 [1:04:09<37:45,  1.93s/it] 62%|██████▏   | 1915/3086 [1:04:12<41:46,  2.14s/it] 62%|██████▏   | 1916/3086 [1:04:14<41:05,  2.11s/it] 62%|██████▏   | 1917/3086 [1:04:16<40:40,  2.09s/it] 62%|██████▏   | 1918/3086 [1:04:18<39:26,  2.03s/it] 62%|██████▏   | 1919/3086 [1:04:20<38:53,  2.00s/it] 62%|██████▏   | 1920/3086 [1:04:22<39:35,  2.04s/it]                                                     {'loss': 0.5649, 'grad_norm': 0.46521756052970886, 'learning_rate': 7.556707712248866e-05, 'epoch': 0.62}
 62%|██████▏   | 1920/3086 [1:04:22<39:35,  2.04s/it] 62%|██████▏   | 1921/3086 [1:04:24<40:19,  2.08s/it] 62%|██████▏   | 1922/3086 [1:04:26<38:47,  2.00s/it] 62%|██████▏   | 1923/3086 [1:04:28<37:29,  1.93s/it] 62%|██████▏   | 1924/3086 [1:04:30<38:13,  1.97s/it] 62%|██████▏   | 1925/3086 [1:04:31<35:51,  1.85s/it] 62%|██████▏   | 1926/3086 [1:04:33<34:59,  1.81s/it] 62%|██████▏   | 1927/3086 [1:04:35<37:59,  1.97s/it] 62%|██████▏   | 1928/3086 [1:04:37<38:40,  2.00s/it] 63%|██████▎   | 1929/3086 [1:04:39<38:23,  1.99s/it] 63%|██████▎   | 1930/3086 [1:04:41<37:03,  1.92s/it]                                                     {'loss': 0.5582, 'grad_norm': 0.4233179986476898, 'learning_rate': 7.491898898250162e-05, 'epoch': 0.63}
 63%|██████▎   | 1930/3086 [1:04:41<37:03,  1.92s/it] 63%|██████▎   | 1931/3086 [1:04:43<35:57,  1.87s/it] 63%|██████▎   | 1932/3086 [1:04:45<35:35,  1.85s/it] 63%|██████▎   | 1933/3086 [1:04:47<35:03,  1.82s/it] 63%|██████▎   | 1934/3086 [1:04:49<36:11,  1.88s/it] 63%|██████▎   | 1935/3086 [1:04:50<36:19,  1.89s/it] 63%|██████▎   | 1936/3086 [1:04:52<34:58,  1.82s/it] 63%|██████▎   | 1937/3086 [1:04:54<34:13,  1.79s/it] 63%|██████▎   | 1938/3086 [1:04:56<36:05,  1.89s/it] 63%|██████▎   | 1939/3086 [1:04:58<37:12,  1.95s/it] 63%|██████▎   | 1940/3086 [1:05:00<35:17,  1.85s/it]                                                     {'loss': 0.5734, 'grad_norm': 0.4666590094566345, 'learning_rate': 7.427090084251459e-05, 'epoch': 0.63}
 63%|██████▎   | 1940/3086 [1:05:00<35:17,  1.85s/it] 63%|██████▎   | 1941/3086 [1:05:02<37:13,  1.95s/it] 63%|██████▎   | 1942/3086 [1:05:03<34:07,  1.79s/it] 63%|██████▎   | 1943/3086 [1:05:05<34:41,  1.82s/it] 63%|██████▎   | 1944/3086 [1:05:07<35:29,  1.86s/it] 63%|██████▎   | 1945/3086 [1:05:10<39:02,  2.05s/it] 63%|██████▎   | 1946/3086 [1:05:12<38:17,  2.02s/it] 63%|██████▎   | 1947/3086 [1:05:13<36:51,  1.94s/it] 63%|██████▎   | 1948/3086 [1:05:16<38:22,  2.02s/it] 63%|██████▎   | 1949/3086 [1:05:18<38:12,  2.02s/it] 63%|██████▎   | 1950/3086 [1:05:19<37:16,  1.97s/it]                                                     {'loss': 0.5652, 'grad_norm': 0.5203723311424255, 'learning_rate': 7.362281270252754e-05, 'epoch': 0.63}
 63%|██████▎   | 1950/3086 [1:05:19<37:16,  1.97s/it] 63%|██████▎   | 1951/3086 [1:05:21<36:58,  1.95s/it] 63%|██████▎   | 1952/3086 [1:05:23<36:26,  1.93s/it] 63%|██████▎   | 1953/3086 [1:05:25<36:22,  1.93s/it] 63%|██████▎   | 1954/3086 [1:05:27<35:32,  1.88s/it] 63%|██████▎   | 1955/3086 [1:05:30<40:34,  2.15s/it] 63%|██████▎   | 1956/3086 [1:05:31<37:28,  1.99s/it] 63%|██████▎   | 1957/3086 [1:05:33<38:41,  2.06s/it] 63%|██████▎   | 1958/3086 [1:05:36<39:27,  2.10s/it] 63%|██████▎   | 1959/3086 [1:05:38<40:18,  2.15s/it] 64%|██████▎   | 1960/3086 [1:05:40<37:15,  1.99s/it]                                                     {'loss': 0.5679, 'grad_norm': 0.4950275123119354, 'learning_rate': 7.297472456254051e-05, 'epoch': 0.64}
 64%|██████▎   | 1960/3086 [1:05:40<37:15,  1.99s/it] 64%|██████▎   | 1961/3086 [1:05:42<39:06,  2.09s/it] 64%|██████▎   | 1962/3086 [1:05:44<37:00,  1.98s/it] 64%|██████▎   | 1963/3086 [1:05:45<36:38,  1.96s/it] 64%|██████▎   | 1964/3086 [1:05:48<39:27,  2.11s/it] 64%|██████▎   | 1965/3086 [1:05:51<44:53,  2.40s/it] 64%|██████▎   | 1966/3086 [1:05:53<41:43,  2.24s/it] 64%|██████▎   | 1967/3086 [1:05:55<39:37,  2.12s/it] 64%|██████▍   | 1968/3086 [1:05:56<37:05,  1.99s/it] 64%|██████▍   | 1969/3086 [1:05:59<40:04,  2.15s/it] 64%|██████▍   | 1970/3086 [1:06:01<38:46,  2.08s/it]                                                     {'loss': 0.5562, 'grad_norm': 0.4117283225059509, 'learning_rate': 7.232663642255348e-05, 'epoch': 0.64}
 64%|██████▍   | 1970/3086 [1:06:01<38:46,  2.08s/it] 64%|██████▍   | 1971/3086 [1:06:03<36:35,  1.97s/it] 64%|██████▍   | 1972/3086 [1:06:05<39:15,  2.11s/it] 64%|██████▍   | 1973/3086 [1:06:07<37:01,  2.00s/it] 64%|██████▍   | 1974/3086 [1:06:09<38:40,  2.09s/it] 64%|██████▍   | 1975/3086 [1:06:11<38:12,  2.06s/it] 64%|██████▍   | 1976/3086 [1:06:13<38:29,  2.08s/it] 64%|██████▍   | 1977/3086 [1:06:15<36:33,  1.98s/it] 64%|██████▍   | 1978/3086 [1:06:17<37:41,  2.04s/it] 64%|██████▍   | 1979/3086 [1:06:19<36:30,  1.98s/it] 64%|██████▍   | 1980/3086 [1:06:21<38:20,  2.08s/it]                                                     {'loss': 0.5663, 'grad_norm': 0.3275047242641449, 'learning_rate': 7.167854828256644e-05, 'epoch': 0.64}
 64%|██████▍   | 1980/3086 [1:06:21<38:20,  2.08s/it] 64%|██████▍   | 1981/3086 [1:06:23<37:12,  2.02s/it] 64%|██████▍   | 1982/3086 [1:06:26<40:06,  2.18s/it] 64%|██████▍   | 1983/3086 [1:06:27<37:33,  2.04s/it] 64%|██████▍   | 1984/3086 [1:06:30<40:39,  2.21s/it] 64%|██████▍   | 1985/3086 [1:06:32<38:08,  2.08s/it] 64%|██████▍   | 1986/3086 [1:06:34<38:00,  2.07s/it] 64%|██████▍   | 1987/3086 [1:06:36<38:25,  2.10s/it] 64%|██████▍   | 1988/3086 [1:06:38<39:45,  2.17s/it] 64%|██████▍   | 1989/3086 [1:06:41<40:01,  2.19s/it] 64%|██████▍   | 1990/3086 [1:06:43<39:24,  2.16s/it]                                                     {'loss': 0.5607, 'grad_norm': 0.3805083930492401, 'learning_rate': 7.10304601425794e-05, 'epoch': 0.64}
 64%|██████▍   | 1990/3086 [1:06:43<39:24,  2.16s/it] 65%|██████▍   | 1991/3086 [1:06:45<39:38,  2.17s/it] 65%|██████▍   | 1992/3086 [1:06:47<39:31,  2.17s/it] 65%|██████▍   | 1993/3086 [1:06:49<39:14,  2.15s/it] 65%|██████▍   | 1994/3086 [1:06:51<39:19,  2.16s/it] 65%|██████▍   | 1995/3086 [1:06:53<36:25,  2.00s/it] 65%|██████▍   | 1996/3086 [1:06:55<36:30,  2.01s/it] 65%|██████▍   | 1997/3086 [1:06:57<34:59,  1.93s/it] 65%|██████▍   | 1998/3086 [1:06:59<36:01,  1.99s/it] 65%|██████▍   | 1999/3086 [1:07:01<37:10,  2.05s/it] 65%|██████▍   | 2000/3086 [1:07:04<39:46,  2.20s/it]                                                     {'loss': 0.5569, 'grad_norm': 0.4823559820652008, 'learning_rate': 7.038237200259235e-05, 'epoch': 0.65}
 65%|██████▍   | 2000/3086 [1:07:04<39:46,  2.20s/it][INFO|trainer.py:3503] 2024-11-12 04:44:56,729 >> Saving model checkpoint to /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/xlmr-qwe2.5-math7b-metamath/checkpoint-2000
[INFO|configuration_utils.py:472] 2024-11-12 04:44:56,736 >> Configuration saved in /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/xlmr-qwe2.5-math7b-metamath/checkpoint-2000/config.json
[INFO|tokenization_utils_base.py:2684] 2024-11-12 04:44:56,901 >> tokenizer config file saved in /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/xlmr-qwe2.5-math7b-metamath/checkpoint-2000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2024-11-12 04:44:56,903 >> Special tokens file saved in /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/xlmr-qwe2.5-math7b-metamath/checkpoint-2000/special_tokens_map.json
 65%|██████▍   | 2001/3086 [1:07:16<1:33:11,  5.15s/it] 65%|██████▍   | 2002/3086 [1:07:18<1:18:44,  4.36s/it] 65%|██████▍   | 2003/3086 [1:07:20<1:07:00,  3.71s/it] 65%|██████▍   | 2004/3086 [1:07:22<57:30,  3.19s/it]   65%|██████▍   | 2005/3086 [1:07:24<50:25,  2.80s/it] 65%|██████▌   | 2006/3086 [1:07:26<45:12,  2.51s/it] 65%|██████▌   | 2007/3086 [1:07:28<42:32,  2.37s/it] 65%|██████▌   | 2008/3086 [1:07:30<38:43,  2.16s/it] 65%|██████▌   | 2009/3086 [1:07:32<36:38,  2.04s/it] 65%|██████▌   | 2010/3086 [1:07:33<34:46,  1.94s/it]                                                     {'loss': 0.5621, 'grad_norm': 0.42983338236808777, 'learning_rate': 6.973428386260532e-05, 'epoch': 0.65}
 65%|██████▌   | 2010/3086 [1:07:33<34:46,  1.94s/it] 65%|██████▌   | 2011/3086 [1:07:35<34:01,  1.90s/it] 65%|██████▌   | 2012/3086 [1:07:37<33:20,  1.86s/it] 65%|██████▌   | 2013/3086 [1:07:39<32:36,  1.82s/it] 65%|██████▌   | 2014/3086 [1:07:41<35:53,  2.01s/it] 65%|██████▌   | 2015/3086 [1:07:43<34:58,  1.96s/it] 65%|██████▌   | 2016/3086 [1:07:45<35:14,  1.98s/it] 65%|██████▌   | 2017/3086 [1:07:47<37:45,  2.12s/it] 65%|██████▌   | 2018/3086 [1:07:49<36:49,  2.07s/it] 65%|██████▌   | 2019/3086 [1:07:51<34:45,  1.95s/it] 65%|██████▌   | 2020/3086 [1:07:53<35:36,  2.00s/it]                                                     {'loss': 0.5592, 'grad_norm': 0.4241192042827606, 'learning_rate': 6.908619572261827e-05, 'epoch': 0.65}
 65%|██████▌   | 2020/3086 [1:07:53<35:36,  2.00s/it] 65%|██████▌   | 2021/3086 [1:07:55<36:29,  2.06s/it] 66%|██████▌   | 2022/3086 [1:07:57<35:51,  2.02s/it] 66%|██████▌   | 2023/3086 [1:07:59<36:47,  2.08s/it] 66%|██████▌   | 2024/3086 [1:08:01<34:47,  1.97s/it] 66%|██████▌   | 2025/3086 [1:08:03<36:14,  2.05s/it] 66%|██████▌   | 2026/3086 [1:08:05<36:39,  2.08s/it] 66%|██████▌   | 2027/3086 [1:08:07<35:08,  1.99s/it] 66%|██████▌   | 2028/3086 [1:08:09<33:22,  1.89s/it] 66%|██████▌   | 2029/3086 [1:08:11<33:55,  1.93s/it] 66%|██████▌   | 2030/3086 [1:08:13<32:12,  1.83s/it]                                                     {'loss': 0.5531, 'grad_norm': 0.4738033413887024, 'learning_rate': 6.843810758263124e-05, 'epoch': 0.66}
 66%|██████▌   | 2030/3086 [1:08:13<32:12,  1.83s/it] 66%|██████▌   | 2031/3086 [1:08:14<31:43,  1.80s/it] 66%|██████▌   | 2032/3086 [1:08:16<31:24,  1.79s/it] 66%|██████▌   | 2033/3086 [1:08:18<32:13,  1.84s/it] 66%|██████▌   | 2034/3086 [1:08:20<31:48,  1.81s/it] 66%|██████▌   | 2035/3086 [1:08:22<34:01,  1.94s/it] 66%|██████▌   | 2036/3086 [1:08:24<35:08,  2.01s/it] 66%|██████▌   | 2037/3086 [1:08:26<36:10,  2.07s/it] 66%|██████▌   | 2038/3086 [1:08:29<37:14,  2.13s/it] 66%|██████▌   | 2039/3086 [1:08:30<34:48,  1.99s/it] 66%|██████▌   | 2040/3086 [1:08:32<34:24,  1.97s/it]                                                     {'loss': 0.5663, 'grad_norm': 0.4622592329978943, 'learning_rate': 6.77900194426442e-05, 'epoch': 0.66}
 66%|██████▌   | 2040/3086 [1:08:32<34:24,  1.97s/it] 66%|██████▌   | 2041/3086 [1:08:34<34:45,  2.00s/it] 66%|██████▌   | 2042/3086 [1:08:36<33:07,  1.90s/it] 66%|██████▌   | 2043/3086 [1:08:38<33:56,  1.95s/it] 66%|██████▌   | 2044/3086 [1:08:40<35:54,  2.07s/it] 66%|██████▋   | 2045/3086 [1:08:42<33:59,  1.96s/it] 66%|██████▋   | 2046/3086 [1:08:44<34:10,  1.97s/it] 66%|██████▋   | 2047/3086 [1:08:46<35:49,  2.07s/it] 66%|██████▋   | 2048/3086 [1:08:49<37:33,  2.17s/it] 66%|██████▋   | 2049/3086 [1:08:51<39:49,  2.30s/it] 66%|██████▋   | 2050/3086 [1:08:54<40:36,  2.35s/it]                                                     {'loss': 0.5712, 'grad_norm': 0.4380711317062378, 'learning_rate': 6.714193130265717e-05, 'epoch': 0.66}
 66%|██████▋   | 2050/3086 [1:08:54<40:36,  2.35s/it] 66%|██████▋   | 2051/3086 [1:08:56<40:07,  2.33s/it] 66%|██████▋   | 2052/3086 [1:08:58<37:09,  2.16s/it] 67%|██████▋   | 2053/3086 [1:09:00<35:27,  2.06s/it] 67%|██████▋   | 2054/3086 [1:09:01<33:43,  1.96s/it] 67%|██████▋   | 2055/3086 [1:09:04<35:47,  2.08s/it] 67%|██████▋   | 2056/3086 [1:09:06<37:17,  2.17s/it] 67%|██████▋   | 2057/3086 [1:09:08<34:31,  2.01s/it] 67%|██████▋   | 2058/3086 [1:09:10<33:36,  1.96s/it] 67%|██████▋   | 2059/3086 [1:09:12<32:52,  1.92s/it] 67%|██████▋   | 2060/3086 [1:09:15<40:10,  2.35s/it]                                                     {'loss': 0.5662, 'grad_norm': 0.36009344458580017, 'learning_rate': 6.649384316267012e-05, 'epoch': 0.67}
 67%|██████▋   | 2060/3086 [1:09:15<40:10,  2.35s/it] 67%|██████▋   | 2061/3086 [1:09:17<39:04,  2.29s/it] 67%|██████▋   | 2062/3086 [1:09:19<36:48,  2.16s/it] 67%|██████▋   | 2063/3086 [1:09:21<36:06,  2.12s/it] 67%|██████▋   | 2064/3086 [1:09:23<36:12,  2.13s/it] 67%|██████▋   | 2065/3086 [1:09:25<34:42,  2.04s/it] 67%|██████▋   | 2066/3086 [1:09:27<33:42,  1.98s/it] 67%|██████▋   | 2067/3086 [1:09:29<32:52,  1.94s/it] 67%|██████▋   | 2068/3086 [1:09:30<32:57,  1.94s/it] 67%|██████▋   | 2069/3086 [1:09:33<34:48,  2.05s/it] 67%|██████▋   | 2070/3086 [1:09:35<34:11,  2.02s/it]                                                     {'loss': 0.5677, 'grad_norm': 0.5377602577209473, 'learning_rate': 6.584575502268309e-05, 'epoch': 0.67}
 67%|██████▋   | 2070/3086 [1:09:35<34:11,  2.02s/it] 67%|██████▋   | 2071/3086 [1:09:37<33:46,  2.00s/it] 67%|██████▋   | 2072/3086 [1:09:38<32:47,  1.94s/it] 67%|██████▋   | 2073/3086 [1:09:40<32:16,  1.91s/it] 67%|██████▋   | 2074/3086 [1:09:42<33:00,  1.96s/it] 67%|██████▋   | 2075/3086 [1:09:44<31:11,  1.85s/it] 67%|██████▋   | 2076/3086 [1:09:46<30:36,  1.82s/it] 67%|██████▋   | 2077/3086 [1:09:48<31:43,  1.89s/it] 67%|██████▋   | 2078/3086 [1:09:50<30:56,  1.84s/it] 67%|██████▋   | 2079/3086 [1:09:52<31:44,  1.89s/it] 67%|██████▋   | 2080/3086 [1:09:53<30:57,  1.85s/it]                                                     {'loss': 0.5615, 'grad_norm': 0.4875279664993286, 'learning_rate': 6.519766688269606e-05, 'epoch': 0.67}
 67%|██████▋   | 2080/3086 [1:09:53<30:57,  1.85s/it] 67%|██████▋   | 2081/3086 [1:09:55<30:19,  1.81s/it] 67%|██████▋   | 2082/3086 [1:09:57<31:18,  1.87s/it] 67%|██████▋   | 2083/3086 [1:09:59<32:18,  1.93s/it] 68%|██████▊   | 2084/3086 [1:10:01<32:58,  1.97s/it] 68%|██████▊   | 2085/3086 [1:10:03<33:00,  1.98s/it] 68%|██████▊   | 2086/3086 [1:10:06<35:18,  2.12s/it] 68%|██████▊   | 2087/3086 [1:10:08<34:37,  2.08s/it] 68%|██████▊   | 2088/3086 [1:10:10<34:39,  2.08s/it] 68%|██████▊   | 2089/3086 [1:10:11<32:44,  1.97s/it] 68%|██████▊   | 2090/3086 [1:10:13<31:30,  1.90s/it]                                                     {'loss': 0.5621, 'grad_norm': 0.4155607521533966, 'learning_rate': 6.454957874270901e-05, 'epoch': 0.68}
 68%|██████▊   | 2090/3086 [1:10:13<31:30,  1.90s/it] 68%|██████▊   | 2091/3086 [1:10:15<31:43,  1.91s/it] 68%|██████▊   | 2092/3086 [1:10:18<35:43,  2.16s/it] 68%|██████▊   | 2093/3086 [1:10:19<33:25,  2.02s/it] 68%|██████▊   | 2094/3086 [1:10:22<36:46,  2.22s/it] 68%|██████▊   | 2095/3086 [1:10:24<34:42,  2.10s/it] 68%|██████▊   | 2096/3086 [1:10:26<34:17,  2.08s/it] 68%|██████▊   | 2097/3086 [1:10:28<34:58,  2.12s/it] 68%|██████▊   | 2098/3086 [1:10:30<32:31,  1.98s/it] 68%|██████▊   | 2099/3086 [1:10:32<33:05,  2.01s/it] 68%|██████▊   | 2100/3086 [1:10:34<33:19,  2.03s/it]                                                     {'loss': 0.5675, 'grad_norm': 0.47796452045440674, 'learning_rate': 6.390149060272196e-05, 'epoch': 0.68}
 68%|██████▊   | 2100/3086 [1:10:34<33:19,  2.03s/it] 68%|██████▊   | 2101/3086 [1:10:36<30:26,  1.85s/it] 68%|██████▊   | 2102/3086 [1:10:37<30:28,  1.86s/it] 68%|██████▊   | 2103/3086 [1:10:40<32:17,  1.97s/it] 68%|██████▊   | 2104/3086 [1:10:42<34:07,  2.08s/it] 68%|██████▊   | 2105/3086 [1:10:44<33:01,  2.02s/it] 68%|██████▊   | 2106/3086 [1:10:46<32:48,  2.01s/it] 68%|██████▊   | 2107/3086 [1:10:48<33:00,  2.02s/it] 68%|██████▊   | 2108/3086 [1:10:51<36:54,  2.26s/it] 68%|██████▊   | 2109/3086 [1:10:53<34:46,  2.14s/it] 68%|██████▊   | 2110/3086 [1:10:54<33:27,  2.06s/it]                                                     {'loss': 0.5598, 'grad_norm': 0.4005405902862549, 'learning_rate': 6.325340246273493e-05, 'epoch': 0.68}
 68%|██████▊   | 2110/3086 [1:10:54<33:27,  2.06s/it] 68%|██████▊   | 2111/3086 [1:10:57<34:34,  2.13s/it] 68%|██████▊   | 2112/3086 [1:10:58<31:50,  1.96s/it] 68%|██████▊   | 2113/3086 [1:11:00<29:40,  1.83s/it] 69%|██████▊   | 2114/3086 [1:11:02<29:34,  1.83s/it] 69%|██████▊   | 2115/3086 [1:11:04<33:54,  2.09s/it] 69%|██████▊   | 2116/3086 [1:11:07<35:20,  2.19s/it] 69%|██████▊   | 2117/3086 [1:11:09<34:57,  2.17s/it] 69%|██████▊   | 2118/3086 [1:11:11<33:51,  2.10s/it] 69%|██████▊   | 2119/3086 [1:11:13<36:23,  2.26s/it] 69%|██████▊   | 2120/3086 [1:11:15<35:20,  2.20s/it]                                                     {'loss': 0.5622, 'grad_norm': 0.4402691125869751, 'learning_rate': 6.26053143227479e-05, 'epoch': 0.69}
 69%|██████▊   | 2120/3086 [1:11:15<35:20,  2.20s/it] 69%|██████▊   | 2121/3086 [1:11:18<35:17,  2.19s/it] 69%|██████▉   | 2122/3086 [1:11:20<34:36,  2.15s/it] 69%|██████▉   | 2123/3086 [1:11:21<31:25,  1.96s/it] 69%|██████▉   | 2124/3086 [1:11:23<31:23,  1.96s/it] 69%|██████▉   | 2125/3086 [1:11:25<31:06,  1.94s/it] 69%|██████▉   | 2126/3086 [1:11:27<30:13,  1.89s/it] 69%|██████▉   | 2127/3086 [1:11:29<30:26,  1.90s/it] 69%|██████▉   | 2128/3086 [1:11:31<29:40,  1.86s/it] 69%|██████▉   | 2129/3086 [1:11:32<28:43,  1.80s/it] 69%|██████▉   | 2130/3086 [1:11:34<28:19,  1.78s/it]                                                     {'loss': 0.5707, 'grad_norm': 0.4744281470775604, 'learning_rate': 6.195722618276086e-05, 'epoch': 0.69}
 69%|██████▉   | 2130/3086 [1:11:34<28:19,  1.78s/it] 69%|██████▉   | 2131/3086 [1:11:37<32:50,  2.06s/it] 69%|██████▉   | 2132/3086 [1:11:39<33:23,  2.10s/it] 69%|██████▉   | 2133/3086 [1:11:41<32:32,  2.05s/it] 69%|██████▉   | 2134/3086 [1:11:43<33:43,  2.13s/it] 69%|██████▉   | 2135/3086 [1:11:45<31:07,  1.96s/it] 69%|██████▉   | 2136/3086 [1:11:47<31:02,  1.96s/it] 69%|██████▉   | 2137/3086 [1:11:49<31:43,  2.01s/it] 69%|██████▉   | 2138/3086 [1:11:51<31:27,  1.99s/it] 69%|██████▉   | 2139/3086 [1:11:53<30:59,  1.96s/it] 69%|██████▉   | 2140/3086 [1:11:55<32:26,  2.06s/it]                                                     {'loss': 0.5589, 'grad_norm': 0.40476104617118835, 'learning_rate': 6.130913804277382e-05, 'epoch': 0.69}
 69%|██████▉   | 2140/3086 [1:11:55<32:26,  2.06s/it] 69%|██████▉   | 2141/3086 [1:11:57<31:21,  1.99s/it] 69%|██████▉   | 2142/3086 [1:11:59<34:09,  2.17s/it] 69%|██████▉   | 2143/3086 [1:12:01<31:34,  2.01s/it] 69%|██████▉   | 2144/3086 [1:12:03<33:28,  2.13s/it] 70%|██████▉   | 2145/3086 [1:12:06<35:48,  2.28s/it] 70%|██████▉   | 2146/3086 [1:12:08<33:29,  2.14s/it] 70%|██████▉   | 2147/3086 [1:12:09<30:57,  1.98s/it] 70%|██████▉   | 2148/3086 [1:12:11<30:47,  1.97s/it] 70%|██████▉   | 2149/3086 [1:12:14<32:26,  2.08s/it] 70%|██████▉   | 2150/3086 [1:12:16<32:01,  2.05s/it]                                                     {'loss': 0.5658, 'grad_norm': 0.38269704580307007, 'learning_rate': 6.0661049902786785e-05, 'epoch': 0.7}
 70%|██████▉   | 2150/3086 [1:12:16<32:01,  2.05s/it] 70%|██████▉   | 2151/3086 [1:12:18<31:20,  2.01s/it] 70%|██████▉   | 2152/3086 [1:12:20<31:09,  2.00s/it] 70%|██████▉   | 2153/3086 [1:12:21<30:10,  1.94s/it] 70%|██████▉   | 2154/3086 [1:12:23<29:35,  1.91s/it] 70%|██████▉   | 2155/3086 [1:12:25<29:24,  1.90s/it] 70%|██████▉   | 2156/3086 [1:12:28<32:20,  2.09s/it] 70%|██████▉   | 2157/3086 [1:12:30<32:41,  2.11s/it] 70%|██████▉   | 2158/3086 [1:12:32<34:27,  2.23s/it] 70%|██████▉   | 2159/3086 [1:12:34<34:08,  2.21s/it] 70%|██████▉   | 2160/3086 [1:12:36<31:40,  2.05s/it]                                                     {'loss': 0.5617, 'grad_norm': 0.49524787068367004, 'learning_rate': 6.0012961762799745e-05, 'epoch': 0.7}
 70%|██████▉   | 2160/3086 [1:12:36<31:40,  2.05s/it] 70%|███████   | 2161/3086 [1:12:38<32:42,  2.12s/it] 70%|███████   | 2162/3086 [1:12:40<32:41,  2.12s/it] 70%|███████   | 2163/3086 [1:12:43<34:46,  2.26s/it] 70%|███████   | 2164/3086 [1:12:45<34:24,  2.24s/it] 70%|███████   | 2165/3086 [1:12:47<34:09,  2.23s/it] 70%|███████   | 2166/3086 [1:12:49<32:37,  2.13s/it] 70%|███████   | 2167/3086 [1:12:52<33:33,  2.19s/it] 70%|███████   | 2168/3086 [1:12:54<33:12,  2.17s/it] 70%|███████   | 2169/3086 [1:12:56<31:08,  2.04s/it] 70%|███████   | 2170/3086 [1:12:58<34:34,  2.26s/it]                                                     {'loss': 0.5638, 'grad_norm': 0.3462851643562317, 'learning_rate': 5.93648736228127e-05, 'epoch': 0.7}
 70%|███████   | 2170/3086 [1:12:58<34:34,  2.26s/it] 70%|███████   | 2171/3086 [1:13:00<31:29,  2.06s/it] 70%|███████   | 2172/3086 [1:13:02<31:53,  2.09s/it] 70%|███████   | 2173/3086 [1:13:05<34:12,  2.25s/it] 70%|███████   | 2174/3086 [1:13:07<33:52,  2.23s/it] 70%|███████   | 2175/3086 [1:13:09<34:23,  2.26s/it] 71%|███████   | 2176/3086 [1:13:11<32:13,  2.13s/it] 71%|███████   | 2177/3086 [1:13:13<30:23,  2.01s/it] 71%|███████   | 2178/3086 [1:13:15<30:57,  2.05s/it] 71%|███████   | 2179/3086 [1:13:17<31:13,  2.07s/it] 71%|███████   | 2180/3086 [1:13:19<30:50,  2.04s/it]                                                     {'loss': 0.5706, 'grad_norm': 0.4464900493621826, 'learning_rate': 5.8716785482825665e-05, 'epoch': 0.71}
 71%|███████   | 2180/3086 [1:13:19<30:50,  2.04s/it] 71%|███████   | 2181/3086 [1:13:21<31:40,  2.10s/it] 71%|███████   | 2182/3086 [1:13:23<31:14,  2.07s/it] 71%|███████   | 2183/3086 [1:13:26<35:40,  2.37s/it] 71%|███████   | 2184/3086 [1:13:28<32:10,  2.14s/it] 71%|███████   | 2185/3086 [1:13:30<31:40,  2.11s/it] 71%|███████   | 2186/3086 [1:13:32<29:16,  1.95s/it] 71%|███████   | 2187/3086 [1:13:33<29:11,  1.95s/it] 71%|███████   | 2188/3086 [1:13:35<27:57,  1.87s/it] 71%|███████   | 2189/3086 [1:13:37<29:07,  1.95s/it] 71%|███████   | 2190/3086 [1:13:39<27:54,  1.87s/it]                                                     {'loss': 0.5643, 'grad_norm': 0.3884883522987366, 'learning_rate': 5.806869734283863e-05, 'epoch': 0.71}
 71%|███████   | 2190/3086 [1:13:39<27:54,  1.87s/it] 71%|███████   | 2191/3086 [1:13:41<26:56,  1.81s/it] 71%|███████   | 2192/3086 [1:13:42<26:10,  1.76s/it] 71%|███████   | 2193/3086 [1:13:44<25:59,  1.75s/it] 71%|███████   | 2194/3086 [1:13:46<26:26,  1.78s/it] 71%|███████   | 2195/3086 [1:13:48<25:59,  1.75s/it] 71%|███████   | 2196/3086 [1:13:49<26:48,  1.81s/it] 71%|███████   | 2197/3086 [1:13:52<30:39,  2.07s/it] 71%|███████   | 2198/3086 [1:13:54<31:30,  2.13s/it] 71%|███████▏  | 2199/3086 [1:13:56<30:53,  2.09s/it] 71%|███████▏  | 2200/3086 [1:13:58<29:53,  2.02s/it]                                                     {'loss': 0.5753, 'grad_norm': 0.4535089135169983, 'learning_rate': 5.742060920285159e-05, 'epoch': 0.71}
 71%|███████▏  | 2200/3086 [1:13:58<29:53,  2.02s/it] 71%|███████▏  | 2201/3086 [1:14:00<27:43,  1.88s/it] 71%|███████▏  | 2202/3086 [1:14:02<27:16,  1.85s/it] 71%|███████▏  | 2203/3086 [1:14:04<29:27,  2.00s/it] 71%|███████▏  | 2204/3086 [1:14:06<30:59,  2.11s/it] 71%|███████▏  | 2205/3086 [1:14:09<33:08,  2.26s/it] 71%|███████▏  | 2206/3086 [1:14:12<35:01,  2.39s/it] 72%|███████▏  | 2207/3086 [1:14:14<33:56,  2.32s/it] 72%|███████▏  | 2208/3086 [1:14:16<32:34,  2.23s/it] 72%|███████▏  | 2209/3086 [1:14:18<33:10,  2.27s/it] 72%|███████▏  | 2210/3086 [1:14:20<31:09,  2.13s/it]                                                     {'loss': 0.5606, 'grad_norm': 0.4135643541812897, 'learning_rate': 5.6772521062864546e-05, 'epoch': 0.72}
 72%|███████▏  | 2210/3086 [1:14:20<31:09,  2.13s/it] 72%|███████▏  | 2211/3086 [1:14:22<31:22,  2.15s/it] 72%|███████▏  | 2212/3086 [1:14:24<28:53,  1.98s/it] 72%|███████▏  | 2213/3086 [1:14:26<28:21,  1.95s/it] 72%|███████▏  | 2214/3086 [1:14:28<28:35,  1.97s/it] 72%|███████▏  | 2215/3086 [1:14:30<30:25,  2.10s/it] 72%|███████▏  | 2216/3086 [1:14:32<30:16,  2.09s/it] 72%|███████▏  | 2217/3086 [1:14:34<31:11,  2.15s/it] 72%|███████▏  | 2218/3086 [1:14:36<30:05,  2.08s/it] 72%|███████▏  | 2219/3086 [1:14:38<29:05,  2.01s/it] 72%|███████▏  | 2220/3086 [1:14:40<28:25,  1.97s/it]                                                     {'loss': 0.5655, 'grad_norm': 0.7628006339073181, 'learning_rate': 5.612443292287751e-05, 'epoch': 0.72}
 72%|███████▏  | 2220/3086 [1:14:40<28:25,  1.97s/it] 72%|███████▏  | 2221/3086 [1:14:42<27:18,  1.89s/it] 72%|███████▏  | 2222/3086 [1:14:44<28:32,  1.98s/it] 72%|███████▏  | 2223/3086 [1:14:46<27:48,  1.93s/it] 72%|███████▏  | 2224/3086 [1:14:48<27:01,  1.88s/it] 72%|███████▏  | 2225/3086 [1:14:50<29:13,  2.04s/it] 72%|███████▏  | 2226/3086 [1:14:52<29:01,  2.02s/it] 72%|███████▏  | 2227/3086 [1:14:54<28:35,  2.00s/it] 72%|███████▏  | 2228/3086 [1:14:56<27:19,  1.91s/it] 72%|███████▏  | 2229/3086 [1:14:58<27:29,  1.92s/it] 72%|███████▏  | 2230/3086 [1:15:00<29:00,  2.03s/it]                                                     {'loss': 0.5663, 'grad_norm': 0.41487154364585876, 'learning_rate': 5.547634478289048e-05, 'epoch': 0.72}
 72%|███████▏  | 2230/3086 [1:15:00<29:00,  2.03s/it] 72%|███████▏  | 2231/3086 [1:15:02<28:37,  2.01s/it] 72%|███████▏  | 2232/3086 [1:15:04<28:53,  2.03s/it] 72%|███████▏  | 2233/3086 [1:15:06<29:26,  2.07s/it] 72%|███████▏  | 2234/3086 [1:15:08<28:10,  1.98s/it] 72%|███████▏  | 2235/3086 [1:15:10<30:29,  2.15s/it] 72%|███████▏  | 2236/3086 [1:15:12<28:51,  2.04s/it] 72%|███████▏  | 2237/3086 [1:15:14<28:45,  2.03s/it] 73%|███████▎  | 2238/3086 [1:15:16<26:54,  1.90s/it] 73%|███████▎  | 2239/3086 [1:15:18<30:20,  2.15s/it] 73%|███████▎  | 2240/3086 [1:15:21<29:58,  2.13s/it]                                                     {'loss': 0.5594, 'grad_norm': 0.4295140504837036, 'learning_rate': 5.482825664290344e-05, 'epoch': 0.73}
 73%|███████▎  | 2240/3086 [1:15:21<29:58,  2.13s/it] 73%|███████▎  | 2241/3086 [1:15:23<29:52,  2.12s/it] 73%|███████▎  | 2242/3086 [1:15:24<27:20,  1.94s/it] 73%|███████▎  | 2243/3086 [1:15:27<29:20,  2.09s/it] 73%|███████▎  | 2244/3086 [1:15:28<27:40,  1.97s/it] 73%|███████▎  | 2245/3086 [1:15:30<28:32,  2.04s/it] 73%|███████▎  | 2246/3086 [1:15:33<29:50,  2.13s/it] 73%|███████▎  | 2247/3086 [1:15:34<27:43,  1.98s/it] 73%|███████▎  | 2248/3086 [1:15:37<28:39,  2.05s/it] 73%|███████▎  | 2249/3086 [1:15:39<29:19,  2.10s/it] 73%|███████▎  | 2250/3086 [1:15:41<28:44,  2.06s/it]                                                     {'loss': 0.5606, 'grad_norm': 0.4296422302722931, 'learning_rate': 5.418016850291639e-05, 'epoch': 0.73}
 73%|███████▎  | 2250/3086 [1:15:41<28:44,  2.06s/it] 73%|███████▎  | 2251/3086 [1:15:43<27:44,  1.99s/it] 73%|███████▎  | 2252/3086 [1:15:45<29:24,  2.12s/it] 73%|███████▎  | 2253/3086 [1:15:47<28:02,  2.02s/it] 73%|███████▎  | 2254/3086 [1:15:50<31:04,  2.24s/it] 73%|███████▎  | 2255/3086 [1:15:52<31:05,  2.24s/it] 73%|███████▎  | 2256/3086 [1:15:54<29:19,  2.12s/it] 73%|███████▎  | 2257/3086 [1:15:56<29:18,  2.12s/it] 73%|███████▎  | 2258/3086 [1:15:58<30:13,  2.19s/it] 73%|███████▎  | 2259/3086 [1:16:01<31:41,  2.30s/it] 73%|███████▎  | 2260/3086 [1:16:02<28:35,  2.08s/it]                                                     {'loss': 0.5653, 'grad_norm': 0.4500567317008972, 'learning_rate': 5.353208036292936e-05, 'epoch': 0.73}
 73%|███████▎  | 2260/3086 [1:16:02<28:35,  2.08s/it] 73%|███████▎  | 2261/3086 [1:16:04<28:14,  2.05s/it] 73%|███████▎  | 2262/3086 [1:16:07<29:59,  2.18s/it] 73%|███████▎  | 2263/3086 [1:16:09<29:00,  2.11s/it] 73%|███████▎  | 2264/3086 [1:16:11<28:42,  2.10s/it] 73%|███████▎  | 2265/3086 [1:16:14<31:43,  2.32s/it] 73%|███████▎  | 2266/3086 [1:16:15<29:38,  2.17s/it] 73%|███████▎  | 2267/3086 [1:16:17<28:11,  2.06s/it] 73%|███████▎  | 2268/3086 [1:16:19<26:21,  1.93s/it] 74%|███████▎  | 2269/3086 [1:16:21<26:04,  1.92s/it] 74%|███████▎  | 2270/3086 [1:16:23<25:10,  1.85s/it]                                                     {'loss': 0.5756, 'grad_norm': 0.5183635354042053, 'learning_rate': 5.2883992222942326e-05, 'epoch': 0.74}
 74%|███████▎  | 2270/3086 [1:16:23<25:10,  1.85s/it] 74%|███████▎  | 2271/3086 [1:16:24<25:23,  1.87s/it] 74%|███████▎  | 2272/3086 [1:16:26<26:01,  1.92s/it] 74%|███████▎  | 2273/3086 [1:16:28<25:48,  1.90s/it] 74%|███████▎  | 2274/3086 [1:16:30<24:14,  1.79s/it] 74%|███████▎  | 2275/3086 [1:16:32<25:10,  1.86s/it] 74%|███████▍  | 2276/3086 [1:16:33<24:03,  1.78s/it] 74%|███████▍  | 2277/3086 [1:16:35<23:00,  1.71s/it] 74%|███████▍  | 2278/3086 [1:16:37<22:47,  1.69s/it] 74%|███████▍  | 2279/3086 [1:16:39<24:05,  1.79s/it] 74%|███████▍  | 2280/3086 [1:16:40<24:04,  1.79s/it]                                                     {'loss': 0.5687, 'grad_norm': 0.45422670245170593, 'learning_rate': 5.2235904082955286e-05, 'epoch': 0.74}
 74%|███████▍  | 2280/3086 [1:16:40<24:04,  1.79s/it] 74%|███████▍  | 2281/3086 [1:16:43<25:45,  1.92s/it] 74%|███████▍  | 2282/3086 [1:16:45<25:37,  1.91s/it] 74%|███████▍  | 2283/3086 [1:16:46<25:13,  1.88s/it] 74%|███████▍  | 2284/3086 [1:16:48<25:55,  1.94s/it] 74%|███████▍  | 2285/3086 [1:16:50<25:58,  1.95s/it] 74%|███████▍  | 2286/3086 [1:16:52<24:47,  1.86s/it] 74%|███████▍  | 2287/3086 [1:16:54<23:58,  1.80s/it] 74%|███████▍  | 2288/3086 [1:16:56<25:11,  1.89s/it] 74%|███████▍  | 2289/3086 [1:16:58<24:20,  1.83s/it] 74%|███████▍  | 2290/3086 [1:16:59<24:31,  1.85s/it]                                                     {'loss': 0.5615, 'grad_norm': 0.5352483987808228, 'learning_rate': 5.158781594296824e-05, 'epoch': 0.74}
 74%|███████▍  | 2290/3086 [1:16:59<24:31,  1.85s/it] 74%|███████▍  | 2291/3086 [1:17:01<24:00,  1.81s/it] 74%|███████▍  | 2292/3086 [1:17:03<24:55,  1.88s/it] 74%|███████▍  | 2293/3086 [1:17:05<24:28,  1.85s/it] 74%|███████▍  | 2294/3086 [1:17:07<25:36,  1.94s/it] 74%|███████▍  | 2295/3086 [1:17:09<23:45,  1.80s/it] 74%|███████▍  | 2296/3086 [1:17:11<24:24,  1.85s/it] 74%|███████▍  | 2297/3086 [1:17:13<26:04,  1.98s/it] 74%|███████▍  | 2298/3086 [1:17:15<26:51,  2.05s/it] 74%|███████▍  | 2299/3086 [1:17:17<27:39,  2.11s/it] 75%|███████▍  | 2300/3086 [1:17:19<26:20,  2.01s/it]                                                     {'loss': 0.5634, 'grad_norm': 0.3997620642185211, 'learning_rate': 5.0939727802981207e-05, 'epoch': 0.75}
 75%|███████▍  | 2300/3086 [1:17:19<26:20,  2.01s/it] 75%|███████▍  | 2301/3086 [1:17:21<24:59,  1.91s/it] 75%|███████▍  | 2302/3086 [1:17:23<24:16,  1.86s/it] 75%|███████▍  | 2303/3086 [1:17:25<25:13,  1.93s/it] 75%|███████▍  | 2304/3086 [1:17:27<25:07,  1.93s/it] 75%|███████▍  | 2305/3086 [1:17:28<24:22,  1.87s/it] 75%|███████▍  | 2306/3086 [1:17:31<25:37,  1.97s/it] 75%|███████▍  | 2307/3086 [1:17:32<25:06,  1.93s/it] 75%|███████▍  | 2308/3086 [1:17:34<24:08,  1.86s/it] 75%|███████▍  | 2309/3086 [1:17:36<24:42,  1.91s/it] 75%|███████▍  | 2310/3086 [1:17:38<24:07,  1.87s/it]                                                     {'loss': 0.5618, 'grad_norm': 0.5633761882781982, 'learning_rate': 5.0291639662994173e-05, 'epoch': 0.75}
 75%|███████▍  | 2310/3086 [1:17:38<24:07,  1.87s/it] 75%|███████▍  | 2311/3086 [1:17:40<23:37,  1.83s/it] 75%|███████▍  | 2312/3086 [1:17:41<23:04,  1.79s/it] 75%|███████▍  | 2313/3086 [1:17:43<24:06,  1.87s/it] 75%|███████▍  | 2314/3086 [1:17:45<24:23,  1.90s/it] 75%|███████▌  | 2315/3086 [1:17:47<23:59,  1.87s/it] 75%|███████▌  | 2316/3086 [1:17:49<23:50,  1.86s/it] 75%|███████▌  | 2317/3086 [1:17:51<23:02,  1.80s/it] 75%|███████▌  | 2318/3086 [1:17:52<23:16,  1.82s/it] 75%|███████▌  | 2319/3086 [1:17:55<25:22,  1.98s/it] 75%|███████▌  | 2320/3086 [1:17:57<24:36,  1.93s/it]                                                     {'loss': 0.5602, 'grad_norm': 0.4113883078098297, 'learning_rate': 4.9643551523007133e-05, 'epoch': 0.75}
 75%|███████▌  | 2320/3086 [1:17:57<24:36,  1.93s/it] 75%|███████▌  | 2321/3086 [1:17:59<24:52,  1.95s/it] 75%|███████▌  | 2322/3086 [1:18:01<25:04,  1.97s/it] 75%|███████▌  | 2323/3086 [1:18:02<23:33,  1.85s/it] 75%|███████▌  | 2324/3086 [1:18:04<23:18,  1.84s/it] 75%|███████▌  | 2325/3086 [1:18:07<25:54,  2.04s/it] 75%|███████▌  | 2326/3086 [1:18:09<25:41,  2.03s/it] 75%|███████▌  | 2327/3086 [1:18:11<26:46,  2.12s/it] 75%|███████▌  | 2328/3086 [1:18:13<26:34,  2.10s/it] 75%|███████▌  | 2329/3086 [1:18:15<26:35,  2.11s/it] 76%|███████▌  | 2330/3086 [1:18:17<25:33,  2.03s/it]                                                     {'loss': 0.5596, 'grad_norm': 0.44564056396484375, 'learning_rate': 4.8995463383020094e-05, 'epoch': 0.76}
 76%|███████▌  | 2330/3086 [1:18:17<25:33,  2.03s/it] 76%|███████▌  | 2331/3086 [1:18:19<26:48,  2.13s/it] 76%|███████▌  | 2332/3086 [1:18:21<26:25,  2.10s/it] 76%|███████▌  | 2333/3086 [1:18:23<26:17,  2.10s/it] 76%|███████▌  | 2334/3086 [1:18:26<26:54,  2.15s/it] 76%|███████▌  | 2335/3086 [1:18:28<26:00,  2.08s/it] 76%|███████▌  | 2336/3086 [1:18:29<24:09,  1.93s/it] 76%|███████▌  | 2337/3086 [1:18:31<25:15,  2.02s/it] 76%|███████▌  | 2338/3086 [1:18:33<24:29,  1.96s/it] 76%|███████▌  | 2339/3086 [1:18:35<24:28,  1.97s/it] 76%|███████▌  | 2340/3086 [1:18:37<24:23,  1.96s/it]                                                     {'loss': 0.5558, 'grad_norm': 0.47418203949928284, 'learning_rate': 4.8347375243033054e-05, 'epoch': 0.76}
 76%|███████▌  | 2340/3086 [1:18:37<24:23,  1.96s/it] 76%|███████▌  | 2341/3086 [1:18:39<24:05,  1.94s/it] 76%|███████▌  | 2342/3086 [1:18:41<25:11,  2.03s/it] 76%|███████▌  | 2343/3086 [1:18:43<24:45,  2.00s/it] 76%|███████▌  | 2344/3086 [1:18:45<25:24,  2.05s/it] 76%|███████▌  | 2345/3086 [1:18:47<25:02,  2.03s/it] 76%|███████▌  | 2346/3086 [1:18:49<24:02,  1.95s/it] 76%|███████▌  | 2347/3086 [1:18:52<25:40,  2.08s/it] 76%|███████▌  | 2348/3086 [1:18:54<26:21,  2.14s/it] 76%|███████▌  | 2349/3086 [1:18:56<26:55,  2.19s/it] 76%|███████▌  | 2350/3086 [1:18:58<26:06,  2.13s/it]                                                     {'loss': 0.5463, 'grad_norm': 0.4385869801044464, 'learning_rate': 4.7699287103046014e-05, 'epoch': 0.76}
 76%|███████▌  | 2350/3086 [1:18:58<26:06,  2.13s/it] 76%|███████▌  | 2351/3086 [1:19:00<24:36,  2.01s/it] 76%|███████▌  | 2352/3086 [1:19:02<23:52,  1.95s/it] 76%|███████▌  | 2353/3086 [1:19:04<23:48,  1.95s/it] 76%|███████▋  | 2354/3086 [1:19:05<22:56,  1.88s/it] 76%|███████▋  | 2355/3086 [1:19:07<23:41,  1.94s/it] 76%|███████▋  | 2356/3086 [1:19:10<24:57,  2.05s/it] 76%|███████▋  | 2357/3086 [1:19:11<23:23,  1.92s/it] 76%|███████▋  | 2358/3086 [1:19:13<23:14,  1.92s/it] 76%|███████▋  | 2359/3086 [1:19:15<23:40,  1.95s/it] 76%|███████▋  | 2360/3086 [1:19:18<27:35,  2.28s/it]                                                     {'loss': 0.5543, 'grad_norm': 0.38820281624794006, 'learning_rate': 4.705119896305898e-05, 'epoch': 0.76}
 76%|███████▋  | 2360/3086 [1:19:18<27:35,  2.28s/it] 77%|███████▋  | 2361/3086 [1:19:20<25:37,  2.12s/it] 77%|███████▋  | 2362/3086 [1:19:22<25:27,  2.11s/it] 77%|███████▋  | 2363/3086 [1:19:24<25:27,  2.11s/it] 77%|███████▋  | 2364/3086 [1:19:26<24:34,  2.04s/it] 77%|███████▋  | 2365/3086 [1:19:28<23:59,  2.00s/it] 77%|███████▋  | 2366/3086 [1:19:30<24:26,  2.04s/it] 77%|███████▋  | 2367/3086 [1:19:32<24:27,  2.04s/it] 77%|███████▋  | 2368/3086 [1:19:34<24:54,  2.08s/it] 77%|███████▋  | 2369/3086 [1:19:36<23:53,  2.00s/it] 77%|███████▋  | 2370/3086 [1:19:38<23:37,  1.98s/it]                                                     {'loss': 0.5524, 'grad_norm': 0.43254581093788147, 'learning_rate': 4.640311082307194e-05, 'epoch': 0.77}
 77%|███████▋  | 2370/3086 [1:19:38<23:37,  1.98s/it] 77%|███████▋  | 2371/3086 [1:19:41<26:51,  2.25s/it] 77%|███████▋  | 2372/3086 [1:19:43<25:29,  2.14s/it] 77%|███████▋  | 2373/3086 [1:19:45<25:01,  2.11s/it] 77%|███████▋  | 2374/3086 [1:19:47<25:45,  2.17s/it] 77%|███████▋  | 2375/3086 [1:19:49<25:08,  2.12s/it] 77%|███████▋  | 2376/3086 [1:19:51<25:28,  2.15s/it] 77%|███████▋  | 2377/3086 [1:19:54<25:19,  2.14s/it] 77%|███████▋  | 2378/3086 [1:19:56<24:55,  2.11s/it] 77%|███████▋  | 2379/3086 [1:19:58<24:54,  2.11s/it] 77%|███████▋  | 2380/3086 [1:20:00<24:19,  2.07s/it]                                                     {'loss': 0.5518, 'grad_norm': 0.3576596975326538, 'learning_rate': 4.57550226830849e-05, 'epoch': 0.77}
 77%|███████▋  | 2380/3086 [1:20:00<24:19,  2.07s/it] 77%|███████▋  | 2381/3086 [1:20:01<23:20,  1.99s/it] 77%|███████▋  | 2382/3086 [1:20:04<25:51,  2.20s/it] 77%|███████▋  | 2383/3086 [1:20:06<24:31,  2.09s/it] 77%|███████▋  | 2384/3086 [1:20:08<23:00,  1.97s/it] 77%|███████▋  | 2385/3086 [1:20:10<22:37,  1.94s/it] 77%|███████▋  | 2386/3086 [1:20:12<22:38,  1.94s/it] 77%|███████▋  | 2387/3086 [1:20:13<22:18,  1.91s/it] 77%|███████▋  | 2388/3086 [1:20:15<21:44,  1.87s/it] 77%|███████▋  | 2389/3086 [1:20:17<21:32,  1.85s/it] 77%|███████▋  | 2390/3086 [1:20:18<20:22,  1.76s/it]                                                     {'loss': 0.5542, 'grad_norm': 0.34689876437187195, 'learning_rate': 4.510693454309786e-05, 'epoch': 0.77}
 77%|███████▋  | 2390/3086 [1:20:18<20:22,  1.76s/it] 77%|███████▋  | 2391/3086 [1:20:21<21:50,  1.88s/it] 78%|███████▊  | 2392/3086 [1:20:23<23:28,  2.03s/it] 78%|███████▊  | 2393/3086 [1:20:25<22:22,  1.94s/it] 78%|███████▊  | 2394/3086 [1:20:27<22:05,  1.92s/it] 78%|███████▊  | 2395/3086 [1:20:29<22:04,  1.92s/it] 78%|███████▊  | 2396/3086 [1:20:31<23:37,  2.05s/it] 78%|███████▊  | 2397/3086 [1:20:33<22:42,  1.98s/it] 78%|███████▊  | 2398/3086 [1:20:35<23:50,  2.08s/it] 78%|███████▊  | 2399/3086 [1:20:37<23:46,  2.08s/it] 78%|███████▊  | 2400/3086 [1:20:39<22:28,  1.97s/it]                                                     {'loss': 0.5576, 'grad_norm': 0.4467865228652954, 'learning_rate': 4.445884640311083e-05, 'epoch': 0.78}
 78%|███████▊  | 2400/3086 [1:20:39<22:28,  1.97s/it] 78%|███████▊  | 2401/3086 [1:20:40<21:19,  1.87s/it] 78%|███████▊  | 2402/3086 [1:20:42<21:22,  1.88s/it] 78%|███████▊  | 2403/3086 [1:20:44<21:40,  1.90s/it] 78%|███████▊  | 2404/3086 [1:20:46<21:31,  1.89s/it] 78%|███████▊  | 2405/3086 [1:20:48<20:51,  1.84s/it] 78%|███████▊  | 2406/3086 [1:20:49<19:57,  1.76s/it] 78%|███████▊  | 2407/3086 [1:20:51<20:04,  1.77s/it] 78%|███████▊  | 2408/3086 [1:20:53<19:59,  1.77s/it] 78%|███████▊  | 2409/3086 [1:20:57<26:49,  2.38s/it] 78%|███████▊  | 2410/3086 [1:20:58<24:20,  2.16s/it]                                                     {'loss': 0.5551, 'grad_norm': 0.42129796743392944, 'learning_rate': 4.381075826312379e-05, 'epoch': 0.78}
 78%|███████▊  | 2410/3086 [1:20:58<24:20,  2.16s/it] 78%|███████▊  | 2411/3086 [1:21:01<25:17,  2.25s/it] 78%|███████▊  | 2412/3086 [1:21:04<26:47,  2.38s/it] 78%|███████▊  | 2413/3086 [1:21:06<26:17,  2.34s/it] 78%|███████▊  | 2414/3086 [1:21:08<24:43,  2.21s/it] 78%|███████▊  | 2415/3086 [1:21:09<22:58,  2.05s/it] 78%|███████▊  | 2416/3086 [1:21:11<22:09,  1.98s/it] 78%|███████▊  | 2417/3086 [1:21:13<21:02,  1.89s/it] 78%|███████▊  | 2418/3086 [1:21:14<19:45,  1.77s/it] 78%|███████▊  | 2419/3086 [1:21:16<20:26,  1.84s/it] 78%|███████▊  | 2420/3086 [1:21:18<20:14,  1.82s/it]                                                     {'loss': 0.5602, 'grad_norm': 0.45007964968681335, 'learning_rate': 4.316267012313675e-05, 'epoch': 0.78}
 78%|███████▊  | 2420/3086 [1:21:18<20:14,  1.82s/it] 78%|███████▊  | 2421/3086 [1:21:20<20:17,  1.83s/it] 78%|███████▊  | 2422/3086 [1:21:22<20:09,  1.82s/it] 79%|███████▊  | 2423/3086 [1:21:24<20:14,  1.83s/it] 79%|███████▊  | 2424/3086 [1:21:26<20:26,  1.85s/it] 79%|███████▊  | 2425/3086 [1:21:28<21:33,  1.96s/it] 79%|███████▊  | 2426/3086 [1:21:30<22:02,  2.00s/it] 79%|███████▊  | 2427/3086 [1:21:32<21:07,  1.92s/it] 79%|███████▊  | 2428/3086 [1:21:34<20:44,  1.89s/it] 79%|███████▊  | 2429/3086 [1:21:35<20:45,  1.90s/it] 79%|███████▊  | 2430/3086 [1:21:37<20:11,  1.85s/it]                                                     {'loss': 0.5629, 'grad_norm': 0.35195454955101013, 'learning_rate': 4.251458198314971e-05, 'epoch': 0.79}
 79%|███████▊  | 2430/3086 [1:21:37<20:11,  1.85s/it] 79%|███████▉  | 2431/3086 [1:21:39<19:50,  1.82s/it] 79%|███████▉  | 2432/3086 [1:21:42<22:47,  2.09s/it] 79%|███████▉  | 2433/3086 [1:21:44<23:41,  2.18s/it] 79%|███████▉  | 2434/3086 [1:21:46<23:11,  2.13s/it] 79%|███████▉  | 2435/3086 [1:21:49<24:37,  2.27s/it] 79%|███████▉  | 2436/3086 [1:21:50<23:09,  2.14s/it] 79%|███████▉  | 2437/3086 [1:21:52<22:00,  2.03s/it] 79%|███████▉  | 2438/3086 [1:21:54<21:11,  1.96s/it] 79%|███████▉  | 2439/3086 [1:21:56<20:32,  1.90s/it] 79%|███████▉  | 2440/3086 [1:21:58<20:36,  1.91s/it]                                                     {'loss': 0.5551, 'grad_norm': 0.5173625349998474, 'learning_rate': 4.1866493843162675e-05, 'epoch': 0.79}
 79%|███████▉  | 2440/3086 [1:21:58<20:36,  1.91s/it] 79%|███████▉  | 2441/3086 [1:22:00<23:07,  2.15s/it] 79%|███████▉  | 2442/3086 [1:22:02<21:01,  1.96s/it] 79%|███████▉  | 2443/3086 [1:22:04<20:50,  1.95s/it] 79%|███████▉  | 2444/3086 [1:22:06<21:17,  1.99s/it] 79%|███████▉  | 2445/3086 [1:22:08<21:32,  2.02s/it] 79%|███████▉  | 2446/3086 [1:22:11<23:23,  2.19s/it] 79%|███████▉  | 2447/3086 [1:22:13<24:10,  2.27s/it] 79%|███████▉  | 2448/3086 [1:22:15<23:15,  2.19s/it] 79%|███████▉  | 2449/3086 [1:22:17<21:19,  2.01s/it] 79%|███████▉  | 2450/3086 [1:22:18<20:27,  1.93s/it]                                                     {'loss': 0.5701, 'grad_norm': 0.5397305488586426, 'learning_rate': 4.1218405703175635e-05, 'epoch': 0.79}
 79%|███████▉  | 2450/3086 [1:22:18<20:27,  1.93s/it] 79%|███████▉  | 2451/3086 [1:22:20<19:17,  1.82s/it] 79%|███████▉  | 2452/3086 [1:22:22<20:58,  1.99s/it] 79%|███████▉  | 2453/3086 [1:22:24<20:58,  1.99s/it] 80%|███████▉  | 2454/3086 [1:22:27<23:17,  2.21s/it] 80%|███████▉  | 2455/3086 [1:22:29<22:59,  2.19s/it] 80%|███████▉  | 2456/3086 [1:22:32<24:23,  2.32s/it] 80%|███████▉  | 2457/3086 [1:22:34<23:58,  2.29s/it] 80%|███████▉  | 2458/3086 [1:22:36<21:58,  2.10s/it] 80%|███████▉  | 2459/3086 [1:22:38<22:38,  2.17s/it] 80%|███████▉  | 2460/3086 [1:22:40<21:49,  2.09s/it]                                                     {'loss': 0.548, 'grad_norm': 0.42642271518707275, 'learning_rate': 4.0570317563188595e-05, 'epoch': 0.8}
 80%|███████▉  | 2460/3086 [1:22:40<21:49,  2.09s/it] 80%|███████▉  | 2461/3086 [1:22:43<23:10,  2.22s/it] 80%|███████▉  | 2462/3086 [1:22:45<23:41,  2.28s/it] 80%|███████▉  | 2463/3086 [1:22:47<21:52,  2.11s/it] 80%|███████▉  | 2464/3086 [1:22:48<20:59,  2.03s/it] 80%|███████▉  | 2465/3086 [1:22:50<19:59,  1.93s/it] 80%|███████▉  | 2466/3086 [1:22:52<19:10,  1.86s/it] 80%|███████▉  | 2467/3086 [1:22:54<19:10,  1.86s/it] 80%|███████▉  | 2468/3086 [1:22:56<20:06,  1.95s/it] 80%|████████  | 2469/3086 [1:22:58<21:07,  2.05s/it] 80%|████████  | 2470/3086 [1:23:00<21:03,  2.05s/it]                                                     {'loss': 0.5616, 'grad_norm': 0.40985825657844543, 'learning_rate': 3.9922229423201555e-05, 'epoch': 0.8}
 80%|████████  | 2470/3086 [1:23:00<21:03,  2.05s/it] 80%|████████  | 2471/3086 [1:23:02<21:25,  2.09s/it] 80%|████████  | 2472/3086 [1:23:05<21:33,  2.11s/it] 80%|████████  | 2473/3086 [1:23:07<21:43,  2.13s/it] 80%|████████  | 2474/3086 [1:23:09<21:31,  2.11s/it] 80%|████████  | 2475/3086 [1:23:10<20:01,  1.97s/it] 80%|████████  | 2476/3086 [1:23:12<19:27,  1.91s/it] 80%|████████  | 2477/3086 [1:23:14<20:07,  1.98s/it] 80%|████████  | 2478/3086 [1:23:16<19:16,  1.90s/it] 80%|████████  | 2479/3086 [1:23:18<18:58,  1.87s/it] 80%|████████  | 2480/3086 [1:23:20<19:19,  1.91s/it]                                                     {'loss': 0.5469, 'grad_norm': 0.431385338306427, 'learning_rate': 3.927414128321452e-05, 'epoch': 0.8}
 80%|████████  | 2480/3086 [1:23:20<19:19,  1.91s/it] 80%|████████  | 2481/3086 [1:23:22<20:45,  2.06s/it] 80%|████████  | 2482/3086 [1:23:24<20:47,  2.07s/it] 80%|████████  | 2483/3086 [1:23:26<20:31,  2.04s/it] 80%|████████  | 2484/3086 [1:23:28<20:24,  2.03s/it] 81%|████████  | 2485/3086 [1:23:31<21:05,  2.11s/it] 81%|████████  | 2486/3086 [1:23:33<21:10,  2.12s/it] 81%|████████  | 2487/3086 [1:23:35<21:03,  2.11s/it] 81%|████████  | 2488/3086 [1:23:37<21:02,  2.11s/it] 81%|████████  | 2489/3086 [1:23:39<22:05,  2.22s/it] 81%|████████  | 2490/3086 [1:23:41<20:53,  2.10s/it]                                                     {'loss': 0.5479, 'grad_norm': 0.29912012815475464, 'learning_rate': 3.862605314322748e-05, 'epoch': 0.81}
 81%|████████  | 2490/3086 [1:23:41<20:53,  2.10s/it] 81%|████████  | 2491/3086 [1:23:44<21:19,  2.15s/it] 81%|████████  | 2492/3086 [1:23:45<20:31,  2.07s/it] 81%|████████  | 2493/3086 [1:23:47<19:42,  1.99s/it] 81%|████████  | 2494/3086 [1:23:50<20:24,  2.07s/it] 81%|████████  | 2495/3086 [1:23:52<20:26,  2.07s/it] 81%|████████  | 2496/3086 [1:23:54<20:30,  2.09s/it] 81%|████████  | 2497/3086 [1:23:56<20:24,  2.08s/it] 81%|████████  | 2498/3086 [1:23:58<19:51,  2.03s/it] 81%|████████  | 2499/3086 [1:23:59<18:41,  1.91s/it] 81%|████████  | 2500/3086 [1:24:01<18:07,  1.86s/it]                                                     {'loss': 0.557, 'grad_norm': 0.43873047828674316, 'learning_rate': 3.797796500324044e-05, 'epoch': 0.81}
 81%|████████  | 2500/3086 [1:24:01<18:07,  1.86s/it] 81%|████████  | 2501/3086 [1:24:03<19:17,  1.98s/it] 81%|████████  | 2502/3086 [1:24:05<19:15,  1.98s/it] 81%|████████  | 2503/3086 [1:24:07<19:17,  1.99s/it] 81%|████████  | 2504/3086 [1:24:09<18:39,  1.92s/it] 81%|████████  | 2505/3086 [1:24:11<18:36,  1.92s/it] 81%|████████  | 2506/3086 [1:24:13<18:57,  1.96s/it] 81%|████████  | 2507/3086 [1:24:15<19:58,  2.07s/it] 81%|████████▏ | 2508/3086 [1:24:18<21:15,  2.21s/it] 81%|████████▏ | 2509/3086 [1:24:20<20:18,  2.11s/it] 81%|████████▏ | 2510/3086 [1:24:22<20:28,  2.13s/it]                                                     {'loss': 0.5523, 'grad_norm': 0.4239148199558258, 'learning_rate': 3.73298768632534e-05, 'epoch': 0.81}
 81%|████████▏ | 2510/3086 [1:24:22<20:28,  2.13s/it] 81%|████████▏ | 2511/3086 [1:24:24<21:22,  2.23s/it] 81%|████████▏ | 2512/3086 [1:24:26<20:33,  2.15s/it] 81%|████████▏ | 2513/3086 [1:24:28<19:01,  1.99s/it] 81%|████████▏ | 2514/3086 [1:24:30<17:56,  1.88s/it] 81%|████████▏ | 2515/3086 [1:24:31<17:21,  1.82s/it] 82%|████████▏ | 2516/3086 [1:24:34<18:56,  1.99s/it] 82%|████████▏ | 2517/3086 [1:24:36<19:32,  2.06s/it] 82%|████████▏ | 2518/3086 [1:24:38<18:41,  1.98s/it] 82%|████████▏ | 2519/3086 [1:24:39<17:30,  1.85s/it] 82%|████████▏ | 2520/3086 [1:24:41<16:49,  1.78s/it]                                                     {'loss': 0.5554, 'grad_norm': 0.4451698660850525, 'learning_rate': 3.668178872326636e-05, 'epoch': 0.82}
 82%|████████▏ | 2520/3086 [1:24:41<16:49,  1.78s/it] 82%|████████▏ | 2521/3086 [1:24:43<16:38,  1.77s/it] 82%|████████▏ | 2522/3086 [1:24:45<17:33,  1.87s/it] 82%|████████▏ | 2523/3086 [1:24:47<18:07,  1.93s/it] 82%|████████▏ | 2524/3086 [1:24:49<18:04,  1.93s/it] 82%|████████▏ | 2525/3086 [1:24:51<18:28,  1.98s/it] 82%|████████▏ | 2526/3086 [1:24:53<18:00,  1.93s/it] 82%|████████▏ | 2527/3086 [1:24:55<19:10,  2.06s/it] 82%|████████▏ | 2528/3086 [1:24:58<20:50,  2.24s/it] 82%|████████▏ | 2529/3086 [1:25:00<21:01,  2.26s/it] 82%|████████▏ | 2530/3086 [1:25:02<20:46,  2.24s/it]                                                     {'loss': 0.5495, 'grad_norm': 0.49033525586128235, 'learning_rate': 3.603370058327933e-05, 'epoch': 0.82}
 82%|████████▏ | 2530/3086 [1:25:02<20:46,  2.24s/it] 82%|████████▏ | 2531/3086 [1:25:04<19:46,  2.14s/it] 82%|████████▏ | 2532/3086 [1:25:06<18:30,  2.00s/it] 82%|████████▏ | 2533/3086 [1:25:07<17:36,  1.91s/it] 82%|████████▏ | 2534/3086 [1:25:09<17:55,  1.95s/it] 82%|████████▏ | 2535/3086 [1:25:12<19:11,  2.09s/it] 82%|████████▏ | 2536/3086 [1:25:14<19:22,  2.11s/it] 82%|████████▏ | 2537/3086 [1:25:16<19:25,  2.12s/it] 82%|████████▏ | 2538/3086 [1:25:18<18:50,  2.06s/it] 82%|████████▏ | 2539/3086 [1:25:20<17:46,  1.95s/it] 82%|████████▏ | 2540/3086 [1:25:22<18:59,  2.09s/it]                                                     {'loss': 0.5587, 'grad_norm': 0.40222999453544617, 'learning_rate': 3.538561244329229e-05, 'epoch': 0.82}
 82%|████████▏ | 2540/3086 [1:25:22<18:59,  2.09s/it] 82%|████████▏ | 2541/3086 [1:25:24<17:33,  1.93s/it] 82%|████████▏ | 2542/3086 [1:25:26<18:11,  2.01s/it] 82%|████████▏ | 2543/3086 [1:25:28<17:11,  1.90s/it] 82%|████████▏ | 2544/3086 [1:25:30<18:02,  2.00s/it] 82%|████████▏ | 2545/3086 [1:25:31<17:00,  1.89s/it] 83%|████████▎ | 2546/3086 [1:25:34<17:18,  1.92s/it] 83%|████████▎ | 2547/3086 [1:25:36<17:49,  1.98s/it] 83%|████████▎ | 2548/3086 [1:25:38<19:21,  2.16s/it] 83%|████████▎ | 2549/3086 [1:25:40<18:30,  2.07s/it] 83%|████████▎ | 2550/3086 [1:25:42<17:27,  1.95s/it]                                                     {'loss': 0.5683, 'grad_norm': 0.5117132067680359, 'learning_rate': 3.473752430330525e-05, 'epoch': 0.83}
 83%|████████▎ | 2550/3086 [1:25:42<17:27,  1.95s/it] 83%|████████▎ | 2551/3086 [1:25:44<17:03,  1.91s/it] 83%|████████▎ | 2552/3086 [1:25:45<16:23,  1.84s/it] 83%|████████▎ | 2553/3086 [1:25:47<16:12,  1.82s/it] 83%|████████▎ | 2554/3086 [1:25:50<18:15,  2.06s/it] 83%|████████▎ | 2555/3086 [1:25:52<18:43,  2.12s/it] 83%|████████▎ | 2556/3086 [1:25:54<19:03,  2.16s/it] 83%|████████▎ | 2557/3086 [1:25:56<18:18,  2.08s/it] 83%|████████▎ | 2558/3086 [1:25:58<18:29,  2.10s/it] 83%|████████▎ | 2559/3086 [1:26:00<17:44,  2.02s/it] 83%|████████▎ | 2560/3086 [1:26:02<17:27,  1.99s/it]                                                     {'loss': 0.5421, 'grad_norm': 0.4024420380592346, 'learning_rate': 3.408943616331821e-05, 'epoch': 0.83}
 83%|████████▎ | 2560/3086 [1:26:02<17:27,  1.99s/it] 83%|████████▎ | 2561/3086 [1:26:04<17:07,  1.96s/it] 83%|████████▎ | 2562/3086 [1:26:06<16:38,  1.91s/it] 83%|████████▎ | 2563/3086 [1:26:07<16:35,  1.90s/it] 83%|████████▎ | 2564/3086 [1:26:09<16:01,  1.84s/it] 83%|████████▎ | 2565/3086 [1:26:11<16:27,  1.90s/it] 83%|████████▎ | 2566/3086 [1:26:13<15:36,  1.80s/it] 83%|████████▎ | 2567/3086 [1:26:15<17:19,  2.00s/it] 83%|████████▎ | 2568/3086 [1:26:17<17:22,  2.01s/it] 83%|████████▎ | 2569/3086 [1:26:19<16:53,  1.96s/it] 83%|████████▎ | 2570/3086 [1:26:21<16:06,  1.87s/it]                                                     {'loss': 0.551, 'grad_norm': 0.46247583627700806, 'learning_rate': 3.3441348023331176e-05, 'epoch': 0.83}
 83%|████████▎ | 2570/3086 [1:26:21<16:06,  1.87s/it] 83%|████████▎ | 2571/3086 [1:26:23<16:24,  1.91s/it] 83%|████████▎ | 2572/3086 [1:26:25<15:59,  1.87s/it] 83%|████████▎ | 2573/3086 [1:26:26<15:54,  1.86s/it] 83%|████████▎ | 2574/3086 [1:26:28<15:40,  1.84s/it] 83%|████████▎ | 2575/3086 [1:26:31<17:12,  2.02s/it] 83%|████████▎ | 2576/3086 [1:26:33<18:17,  2.15s/it] 84%|████████▎ | 2577/3086 [1:26:35<17:48,  2.10s/it] 84%|████████▎ | 2578/3086 [1:26:37<18:21,  2.17s/it] 84%|████████▎ | 2579/3086 [1:26:39<17:23,  2.06s/it] 84%|████████▎ | 2580/3086 [1:26:41<15:57,  1.89s/it]                                                     {'loss': 0.5687, 'grad_norm': 0.41438984870910645, 'learning_rate': 3.2793259883344136e-05, 'epoch': 0.84}
 84%|████████▎ | 2580/3086 [1:26:41<15:57,  1.89s/it] 84%|████████▎ | 2581/3086 [1:26:42<15:02,  1.79s/it] 84%|████████▎ | 2582/3086 [1:26:44<15:20,  1.83s/it] 84%|████████▎ | 2583/3086 [1:26:46<16:14,  1.94s/it] 84%|████████▎ | 2584/3086 [1:26:48<16:35,  1.98s/it] 84%|████████▍ | 2585/3086 [1:26:51<17:51,  2.14s/it] 84%|████████▍ | 2586/3086 [1:26:53<18:00,  2.16s/it] 84%|████████▍ | 2587/3086 [1:26:55<17:25,  2.10s/it] 84%|████████▍ | 2588/3086 [1:26:57<15:45,  1.90s/it] 84%|████████▍ | 2589/3086 [1:26:59<16:58,  2.05s/it] 84%|████████▍ | 2590/3086 [1:27:01<16:17,  1.97s/it]                                                     {'loss': 0.5571, 'grad_norm': 0.4210025668144226, 'learning_rate': 3.21451717433571e-05, 'epoch': 0.84}
 84%|████████▍ | 2590/3086 [1:27:01<16:17,  1.97s/it] 84%|████████▍ | 2591/3086 [1:27:03<15:58,  1.94s/it] 84%|████████▍ | 2592/3086 [1:27:04<15:29,  1.88s/it] 84%|████████▍ | 2593/3086 [1:27:06<14:41,  1.79s/it] 84%|████████▍ | 2594/3086 [1:27:08<15:42,  1.92s/it] 84%|████████▍ | 2595/3086 [1:27:11<17:05,  2.09s/it] 84%|████████▍ | 2596/3086 [1:27:13<16:58,  2.08s/it] 84%|████████▍ | 2597/3086 [1:27:15<17:30,  2.15s/it] 84%|████████▍ | 2598/3086 [1:27:17<16:17,  2.00s/it] 84%|████████▍ | 2599/3086 [1:27:18<15:40,  1.93s/it] 84%|████████▍ | 2600/3086 [1:27:20<15:00,  1.85s/it]                                                     {'loss': 0.5461, 'grad_norm': 0.4464643895626068, 'learning_rate': 3.1497083603370056e-05, 'epoch': 0.84}
 84%|████████▍ | 2600/3086 [1:27:20<15:00,  1.85s/it] 84%|████████▍ | 2601/3086 [1:27:22<15:52,  1.96s/it] 84%|████████▍ | 2602/3086 [1:27:24<15:40,  1.94s/it] 84%|████████▍ | 2603/3086 [1:27:26<14:36,  1.82s/it] 84%|████████▍ | 2604/3086 [1:27:28<15:17,  1.90s/it] 84%|████████▍ | 2605/3086 [1:27:30<16:17,  2.03s/it] 84%|████████▍ | 2606/3086 [1:27:32<16:18,  2.04s/it] 84%|████████▍ | 2607/3086 [1:27:35<17:04,  2.14s/it] 85%|████████▍ | 2608/3086 [1:27:37<17:01,  2.14s/it] 85%|████████▍ | 2609/3086 [1:27:39<16:15,  2.05s/it] 85%|████████▍ | 2610/3086 [1:27:40<14:57,  1.88s/it]                                                     {'loss': 0.5515, 'grad_norm': 0.462676465511322, 'learning_rate': 3.084899546338302e-05, 'epoch': 0.85}
 85%|████████▍ | 2610/3086 [1:27:40<14:57,  1.88s/it] 85%|████████▍ | 2611/3086 [1:27:43<16:31,  2.09s/it] 85%|████████▍ | 2612/3086 [1:27:45<16:57,  2.15s/it] 85%|████████▍ | 2613/3086 [1:27:47<15:59,  2.03s/it] 85%|████████▍ | 2614/3086 [1:27:48<15:29,  1.97s/it] 85%|████████▍ | 2615/3086 [1:27:50<15:09,  1.93s/it] 85%|████████▍ | 2616/3086 [1:27:52<15:00,  1.92s/it] 85%|████████▍ | 2617/3086 [1:27:54<14:45,  1.89s/it] 85%|████████▍ | 2618/3086 [1:27:56<15:05,  1.93s/it] 85%|████████▍ | 2619/3086 [1:27:58<15:01,  1.93s/it] 85%|████████▍ | 2620/3086 [1:28:00<15:44,  2.03s/it]                                                     {'loss': 0.5441, 'grad_norm': 0.5056302547454834, 'learning_rate': 3.0200907323395983e-05, 'epoch': 0.85}
 85%|████████▍ | 2620/3086 [1:28:00<15:44,  2.03s/it] 85%|████████▍ | 2621/3086 [1:28:02<14:46,  1.91s/it] 85%|████████▍ | 2622/3086 [1:28:04<14:11,  1.83s/it] 85%|████████▍ | 2623/3086 [1:28:06<15:07,  1.96s/it] 85%|████████▌ | 2624/3086 [1:28:08<14:33,  1.89s/it] 85%|████████▌ | 2625/3086 [1:28:10<15:52,  2.07s/it] 85%|████████▌ | 2626/3086 [1:28:12<15:31,  2.03s/it] 85%|████████▌ | 2627/3086 [1:28:14<16:05,  2.10s/it] 85%|████████▌ | 2628/3086 [1:28:16<14:54,  1.95s/it] 85%|████████▌ | 2629/3086 [1:28:18<16:26,  2.16s/it] 85%|████████▌ | 2630/3086 [1:28:20<15:30,  2.04s/it]                                                     {'loss': 0.556, 'grad_norm': 0.41907206177711487, 'learning_rate': 2.9552819183408947e-05, 'epoch': 0.85}
 85%|████████▌ | 2630/3086 [1:28:20<15:30,  2.04s/it] 85%|████████▌ | 2631/3086 [1:28:22<14:27,  1.91s/it] 85%|████████▌ | 2632/3086 [1:28:24<14:45,  1.95s/it] 85%|████████▌ | 2633/3086 [1:28:26<14:02,  1.86s/it] 85%|████████▌ | 2634/3086 [1:28:28<14:57,  1.99s/it] 85%|████████▌ | 2635/3086 [1:28:30<15:31,  2.07s/it] 85%|████████▌ | 2636/3086 [1:28:32<15:12,  2.03s/it] 85%|████████▌ | 2637/3086 [1:28:34<14:53,  1.99s/it] 85%|████████▌ | 2638/3086 [1:28:36<14:22,  1.92s/it] 86%|████████▌ | 2639/3086 [1:28:38<14:12,  1.91s/it] 86%|████████▌ | 2640/3086 [1:28:40<14:51,  2.00s/it]                                                     {'loss': 0.5645, 'grad_norm': 0.4161687195301056, 'learning_rate': 2.8904731043421907e-05, 'epoch': 0.86}
 86%|████████▌ | 2640/3086 [1:28:40<14:51,  2.00s/it] 86%|████████▌ | 2641/3086 [1:28:42<14:34,  1.97s/it] 86%|████████▌ | 2642/3086 [1:28:44<15:38,  2.11s/it] 86%|████████▌ | 2643/3086 [1:28:46<14:13,  1.93s/it] 86%|████████▌ | 2644/3086 [1:28:48<14:12,  1.93s/it] 86%|████████▌ | 2645/3086 [1:28:49<13:53,  1.89s/it] 86%|████████▌ | 2646/3086 [1:28:51<13:38,  1.86s/it] 86%|████████▌ | 2647/3086 [1:28:53<13:24,  1.83s/it] 86%|████████▌ | 2648/3086 [1:28:55<13:37,  1.87s/it] 86%|████████▌ | 2649/3086 [1:28:57<13:40,  1.88s/it] 86%|████████▌ | 2650/3086 [1:28:59<15:23,  2.12s/it]                                                     {'loss': 0.553, 'grad_norm': 0.5530335903167725, 'learning_rate': 2.825664290343487e-05, 'epoch': 0.86}
 86%|████████▌ | 2650/3086 [1:28:59<15:23,  2.12s/it] 86%|████████▌ | 2651/3086 [1:29:02<15:23,  2.12s/it] 86%|████████▌ | 2652/3086 [1:29:03<14:39,  2.03s/it] 86%|████████▌ | 2653/3086 [1:29:05<14:44,  2.04s/it] 86%|████████▌ | 2654/3086 [1:29:07<14:29,  2.01s/it] 86%|████████▌ | 2655/3086 [1:29:10<15:58,  2.22s/it] 86%|████████▌ | 2656/3086 [1:29:12<15:19,  2.14s/it] 86%|████████▌ | 2657/3086 [1:29:14<15:28,  2.17s/it] 86%|████████▌ | 2658/3086 [1:29:17<16:06,  2.26s/it] 86%|████████▌ | 2659/3086 [1:29:19<16:45,  2.35s/it] 86%|████████▌ | 2660/3086 [1:29:21<15:34,  2.19s/it]                                                     {'loss': 0.5569, 'grad_norm': 0.3977941572666168, 'learning_rate': 2.760855476344783e-05, 'epoch': 0.86}
 86%|████████▌ | 2660/3086 [1:29:21<15:34,  2.19s/it] 86%|████████▌ | 2661/3086 [1:29:23<14:36,  2.06s/it] 86%|████████▋ | 2662/3086 [1:29:25<15:12,  2.15s/it] 86%|████████▋ | 2663/3086 [1:29:27<15:02,  2.13s/it] 86%|████████▋ | 2664/3086 [1:29:29<14:09,  2.01s/it] 86%|████████▋ | 2665/3086 [1:29:31<13:14,  1.89s/it] 86%|████████▋ | 2666/3086 [1:29:32<12:52,  1.84s/it] 86%|████████▋ | 2667/3086 [1:29:35<13:32,  1.94s/it] 86%|████████▋ | 2668/3086 [1:29:37<13:34,  1.95s/it] 86%|████████▋ | 2669/3086 [1:29:38<13:30,  1.94s/it] 87%|████████▋ | 2670/3086 [1:29:41<14:02,  2.03s/it]                                                     {'loss': 0.5563, 'grad_norm': 0.43940991163253784, 'learning_rate': 2.696046662346079e-05, 'epoch': 0.87}
 87%|████████▋ | 2670/3086 [1:29:41<14:02,  2.03s/it] 87%|████████▋ | 2671/3086 [1:29:43<15:01,  2.17s/it] 87%|████████▋ | 2672/3086 [1:29:45<14:16,  2.07s/it] 87%|████████▋ | 2673/3086 [1:29:47<14:01,  2.04s/it] 87%|████████▋ | 2674/3086 [1:29:49<13:58,  2.03s/it] 87%|████████▋ | 2675/3086 [1:29:52<14:54,  2.18s/it] 87%|████████▋ | 2676/3086 [1:29:53<14:01,  2.05s/it] 87%|████████▋ | 2677/3086 [1:29:55<13:27,  1.97s/it] 87%|████████▋ | 2678/3086 [1:29:57<14:13,  2.09s/it] 87%|████████▋ | 2679/3086 [1:29:59<13:20,  1.97s/it] 87%|████████▋ | 2680/3086 [1:30:01<13:27,  1.99s/it]                                                     {'loss': 0.559, 'grad_norm': 0.44651973247528076, 'learning_rate': 2.6312378483473754e-05, 'epoch': 0.87}
 87%|████████▋ | 2680/3086 [1:30:01<13:27,  1.99s/it] 87%|████████▋ | 2681/3086 [1:30:04<14:50,  2.20s/it] 87%|████████▋ | 2682/3086 [1:30:06<14:00,  2.08s/it] 87%|████████▋ | 2683/3086 [1:30:08<15:27,  2.30s/it] 87%|████████▋ | 2684/3086 [1:30:10<14:32,  2.17s/it] 87%|████████▋ | 2685/3086 [1:30:12<13:24,  2.01s/it] 87%|████████▋ | 2686/3086 [1:30:14<13:06,  1.97s/it] 87%|████████▋ | 2687/3086 [1:30:16<13:17,  2.00s/it] 87%|████████▋ | 2688/3086 [1:30:18<13:19,  2.01s/it] 87%|████████▋ | 2689/3086 [1:30:19<12:24,  1.88s/it] 87%|████████▋ | 2690/3086 [1:30:21<12:33,  1.90s/it]                                                     {'loss': 0.5507, 'grad_norm': 0.4157458543777466, 'learning_rate': 2.5664290343486714e-05, 'epoch': 0.87}
 87%|████████▋ | 2690/3086 [1:30:21<12:33,  1.90s/it] 87%|████████▋ | 2691/3086 [1:30:23<12:01,  1.83s/it] 87%|████████▋ | 2692/3086 [1:30:25<12:45,  1.94s/it] 87%|████████▋ | 2693/3086 [1:30:27<13:06,  2.00s/it] 87%|████████▋ | 2694/3086 [1:30:30<13:40,  2.09s/it] 87%|████████▋ | 2695/3086 [1:30:32<13:23,  2.05s/it] 87%|████████▋ | 2696/3086 [1:30:34<12:52,  1.98s/it] 87%|████████▋ | 2697/3086 [1:30:35<12:39,  1.95s/it] 87%|████████▋ | 2698/3086 [1:30:37<12:33,  1.94s/it] 87%|████████▋ | 2699/3086 [1:30:39<12:25,  1.93s/it] 87%|████████▋ | 2700/3086 [1:30:41<12:07,  1.89s/it]                                                     {'loss': 0.5626, 'grad_norm': 0.40977010130882263, 'learning_rate': 2.5016202203499677e-05, 'epoch': 0.87}
 87%|████████▋ | 2700/3086 [1:30:41<12:07,  1.89s/it] 88%|████████▊ | 2701/3086 [1:30:43<12:10,  1.90s/it] 88%|████████▊ | 2702/3086 [1:30:45<12:05,  1.89s/it] 88%|████████▊ | 2703/3086 [1:30:46<11:39,  1.83s/it] 88%|████████▊ | 2704/3086 [1:30:48<11:51,  1.86s/it] 88%|████████▊ | 2705/3086 [1:30:50<12:11,  1.92s/it] 88%|████████▊ | 2706/3086 [1:30:52<12:12,  1.93s/it] 88%|████████▊ | 2707/3086 [1:30:55<12:59,  2.06s/it] 88%|████████▊ | 2708/3086 [1:30:56<12:03,  1.91s/it] 88%|████████▊ | 2709/3086 [1:30:58<11:48,  1.88s/it] 88%|████████▊ | 2710/3086 [1:31:00<11:14,  1.79s/it]                                                     {'loss': 0.56, 'grad_norm': 0.4435724914073944, 'learning_rate': 2.436811406351264e-05, 'epoch': 0.88}
 88%|████████▊ | 2710/3086 [1:31:00<11:14,  1.79s/it] 88%|████████▊ | 2711/3086 [1:31:02<11:39,  1.87s/it] 88%|████████▊ | 2712/3086 [1:31:04<11:29,  1.84s/it] 88%|████████▊ | 2713/3086 [1:31:06<11:46,  1.90s/it] 88%|████████▊ | 2714/3086 [1:31:07<11:38,  1.88s/it] 88%|████████▊ | 2715/3086 [1:31:09<11:02,  1.79s/it] 88%|████████▊ | 2716/3086 [1:31:11<10:59,  1.78s/it] 88%|████████▊ | 2717/3086 [1:31:13<11:40,  1.90s/it] 88%|████████▊ | 2718/3086 [1:31:15<11:09,  1.82s/it] 88%|████████▊ | 2719/3086 [1:31:17<11:27,  1.87s/it] 88%|████████▊ | 2720/3086 [1:31:18<11:19,  1.86s/it]                                                     {'loss': 0.5559, 'grad_norm': 0.36800599098205566, 'learning_rate': 2.37200259235256e-05, 'epoch': 0.88}
 88%|████████▊ | 2720/3086 [1:31:18<11:19,  1.86s/it] 88%|████████▊ | 2721/3086 [1:31:20<11:20,  1.86s/it] 88%|████████▊ | 2722/3086 [1:31:22<10:58,  1.81s/it] 88%|████████▊ | 2723/3086 [1:31:24<11:08,  1.84s/it] 88%|████████▊ | 2724/3086 [1:31:26<11:06,  1.84s/it] 88%|████████▊ | 2725/3086 [1:31:28<11:22,  1.89s/it] 88%|████████▊ | 2726/3086 [1:31:29<10:56,  1.82s/it] 88%|████████▊ | 2727/3086 [1:31:32<11:25,  1.91s/it] 88%|████████▊ | 2728/3086 [1:31:33<10:50,  1.82s/it] 88%|████████▊ | 2729/3086 [1:31:35<11:00,  1.85s/it] 88%|████████▊ | 2730/3086 [1:31:37<11:38,  1.96s/it]                                                     {'loss': 0.5589, 'grad_norm': 0.43427062034606934, 'learning_rate': 2.3071937783538564e-05, 'epoch': 0.88}
 88%|████████▊ | 2730/3086 [1:31:37<11:38,  1.96s/it] 88%|████████▊ | 2731/3086 [1:31:39<11:30,  1.95s/it] 89%|████████▊ | 2732/3086 [1:31:41<12:07,  2.06s/it] 89%|████████▊ | 2733/3086 [1:31:43<11:48,  2.01s/it] 89%|████████▊ | 2734/3086 [1:31:45<11:14,  1.92s/it] 89%|████████▊ | 2735/3086 [1:31:47<11:46,  2.01s/it] 89%|████████▊ | 2736/3086 [1:31:50<12:57,  2.22s/it] 89%|████████▊ | 2737/3086 [1:31:52<12:02,  2.07s/it] 89%|████████▊ | 2738/3086 [1:31:54<13:03,  2.25s/it] 89%|████████▉ | 2739/3086 [1:31:57<13:05,  2.26s/it] 89%|████████▉ | 2740/3086 [1:31:59<12:47,  2.22s/it]                                                     {'loss': 0.5542, 'grad_norm': 0.3993292450904846, 'learning_rate': 2.242384964355152e-05, 'epoch': 0.89}
 89%|████████▉ | 2740/3086 [1:31:59<12:47,  2.22s/it] 89%|████████▉ | 2741/3086 [1:32:01<12:52,  2.24s/it] 89%|████████▉ | 2742/3086 [1:32:03<11:46,  2.05s/it] 89%|████████▉ | 2743/3086 [1:32:05<12:06,  2.12s/it] 89%|████████▉ | 2744/3086 [1:32:07<12:03,  2.11s/it] 89%|████████▉ | 2745/3086 [1:32:09<10:56,  1.92s/it] 89%|████████▉ | 2746/3086 [1:32:10<10:49,  1.91s/it] 89%|████████▉ | 2747/3086 [1:32:13<11:09,  1.97s/it] 89%|████████▉ | 2748/3086 [1:32:14<10:38,  1.89s/it] 89%|████████▉ | 2749/3086 [1:32:17<11:10,  1.99s/it] 89%|████████▉ | 2750/3086 [1:32:18<11:02,  1.97s/it]                                                     {'loss': 0.5554, 'grad_norm': 0.4373204708099365, 'learning_rate': 2.1775761503564485e-05, 'epoch': 0.89}
 89%|████████▉ | 2750/3086 [1:32:18<11:02,  1.97s/it] 89%|████████▉ | 2751/3086 [1:32:20<10:39,  1.91s/it] 89%|████████▉ | 2752/3086 [1:32:22<10:40,  1.92s/it] 89%|████████▉ | 2753/3086 [1:32:24<10:18,  1.86s/it] 89%|████████▉ | 2754/3086 [1:32:26<10:36,  1.92s/it] 89%|████████▉ | 2755/3086 [1:32:28<11:01,  2.00s/it] 89%|████████▉ | 2756/3086 [1:32:30<11:27,  2.08s/it] 89%|████████▉ | 2757/3086 [1:32:33<11:36,  2.12s/it] 89%|████████▉ | 2758/3086 [1:32:35<11:28,  2.10s/it] 89%|████████▉ | 2759/3086 [1:32:37<11:11,  2.05s/it] 89%|████████▉ | 2760/3086 [1:32:39<11:07,  2.05s/it]                                                     {'loss': 0.555, 'grad_norm': 0.4407823383808136, 'learning_rate': 2.1127673363577448e-05, 'epoch': 0.89}
 89%|████████▉ | 2760/3086 [1:32:39<11:07,  2.05s/it] 89%|████████▉ | 2761/3086 [1:32:41<10:55,  2.02s/it] 90%|████████▉ | 2762/3086 [1:32:42<10:34,  1.96s/it] 90%|████████▉ | 2763/3086 [1:32:44<10:10,  1.89s/it] 90%|████████▉ | 2764/3086 [1:32:46<09:45,  1.82s/it] 90%|████████▉ | 2765/3086 [1:32:48<09:54,  1.85s/it] 90%|████████▉ | 2766/3086 [1:32:51<11:25,  2.14s/it] 90%|████████▉ | 2767/3086 [1:32:53<11:24,  2.15s/it] 90%|████████▉ | 2768/3086 [1:32:55<11:33,  2.18s/it] 90%|████████▉ | 2769/3086 [1:32:57<10:48,  2.05s/it] 90%|████████▉ | 2770/3086 [1:32:58<10:11,  1.94s/it]                                                     {'loss': 0.5578, 'grad_norm': 0.46606141328811646, 'learning_rate': 2.0479585223590408e-05, 'epoch': 0.9}
 90%|████████▉ | 2770/3086 [1:32:58<10:11,  1.94s/it] 90%|████████▉ | 2771/3086 [1:33:00<10:23,  1.98s/it] 90%|████████▉ | 2772/3086 [1:33:02<10:06,  1.93s/it] 90%|████████▉ | 2773/3086 [1:33:04<09:31,  1.83s/it] 90%|████████▉ | 2774/3086 [1:33:06<09:31,  1.83s/it] 90%|████████▉ | 2775/3086 [1:33:08<09:54,  1.91s/it] 90%|████████▉ | 2776/3086 [1:33:09<09:25,  1.82s/it] 90%|████████▉ | 2777/3086 [1:33:12<10:12,  1.98s/it] 90%|█████████ | 2778/3086 [1:33:14<10:13,  1.99s/it] 90%|█████████ | 2779/3086 [1:33:16<10:13,  2.00s/it] 90%|█████████ | 2780/3086 [1:33:18<10:09,  1.99s/it]                                                     {'loss': 0.5545, 'grad_norm': 0.500843346118927, 'learning_rate': 1.983149708360337e-05, 'epoch': 0.9}
 90%|█████████ | 2780/3086 [1:33:18<10:09,  1.99s/it] 90%|█████████ | 2781/3086 [1:33:19<09:26,  1.86s/it] 90%|█████████ | 2782/3086 [1:33:21<09:09,  1.81s/it] 90%|█████████ | 2783/3086 [1:33:23<09:21,  1.85s/it] 90%|█████████ | 2784/3086 [1:33:25<09:26,  1.87s/it] 90%|█████████ | 2785/3086 [1:33:27<09:12,  1.84s/it] 90%|█████████ | 2786/3086 [1:33:29<09:15,  1.85s/it] 90%|█████████ | 2787/3086 [1:33:31<09:28,  1.90s/it] 90%|█████████ | 2788/3086 [1:33:32<09:18,  1.87s/it] 90%|█████████ | 2789/3086 [1:33:34<09:37,  1.94s/it] 90%|█████████ | 2790/3086 [1:33:37<09:46,  1.98s/it]                                                     {'loss': 0.5546, 'grad_norm': 0.4723987877368927, 'learning_rate': 1.918340894361633e-05, 'epoch': 0.9}
 90%|█████████ | 2790/3086 [1:33:37<09:46,  1.98s/it] 90%|█████████ | 2791/3086 [1:33:39<09:53,  2.01s/it] 90%|█████████ | 2792/3086 [1:33:40<09:29,  1.94s/it] 91%|█████████ | 2793/3086 [1:33:42<09:37,  1.97s/it] 91%|█████████ | 2794/3086 [1:33:44<09:11,  1.89s/it] 91%|█████████ | 2795/3086 [1:33:46<09:24,  1.94s/it] 91%|█████████ | 2796/3086 [1:33:48<09:20,  1.93s/it] 91%|█████████ | 2797/3086 [1:33:50<09:51,  2.05s/it] 91%|█████████ | 2798/3086 [1:33:53<10:10,  2.12s/it] 91%|█████████ | 2799/3086 [1:33:55<09:51,  2.06s/it] 91%|█████████ | 2800/3086 [1:33:56<09:28,  1.99s/it]                                                     {'loss': 0.5417, 'grad_norm': 0.45276427268981934, 'learning_rate': 1.8535320803629295e-05, 'epoch': 0.91}
 91%|█████████ | 2800/3086 [1:33:56<09:28,  1.99s/it] 91%|█████████ | 2801/3086 [1:33:58<08:53,  1.87s/it] 91%|█████████ | 2802/3086 [1:34:00<08:36,  1.82s/it] 91%|█████████ | 2803/3086 [1:34:01<08:20,  1.77s/it] 91%|█████████ | 2804/3086 [1:34:03<08:29,  1.81s/it] 91%|█████████ | 2805/3086 [1:34:06<09:15,  1.98s/it] 91%|█████████ | 2806/3086 [1:34:08<09:30,  2.04s/it] 91%|█████████ | 2807/3086 [1:34:10<09:58,  2.14s/it] 91%|█████████ | 2808/3086 [1:34:12<09:58,  2.15s/it] 91%|█████████ | 2809/3086 [1:34:14<09:33,  2.07s/it] 91%|█████████ | 2810/3086 [1:34:16<09:10,  2.00s/it]                                                     {'loss': 0.5647, 'grad_norm': 0.3988613188266754, 'learning_rate': 1.7887232663642255e-05, 'epoch': 0.91}
 91%|█████████ | 2810/3086 [1:34:16<09:10,  2.00s/it] 91%|█████████ | 2811/3086 [1:34:18<08:39,  1.89s/it] 91%|█████████ | 2812/3086 [1:34:20<08:38,  1.89s/it] 91%|█████████ | 2813/3086 [1:34:22<09:03,  1.99s/it] 91%|█████████ | 2814/3086 [1:34:24<08:37,  1.90s/it] 91%|█████████ | 2815/3086 [1:34:25<08:39,  1.92s/it] 91%|█████████▏| 2816/3086 [1:34:27<08:34,  1.91s/it] 91%|█████████▏| 2817/3086 [1:34:29<08:46,  1.96s/it] 91%|█████████▏| 2818/3086 [1:34:32<09:06,  2.04s/it] 91%|█████████▏| 2819/3086 [1:34:34<09:07,  2.05s/it] 91%|█████████▏| 2820/3086 [1:34:36<08:55,  2.01s/it]                                                     {'loss': 0.5567, 'grad_norm': 0.40102019906044006, 'learning_rate': 1.723914452365522e-05, 'epoch': 0.91}
 91%|█████████▏| 2820/3086 [1:34:36<08:55,  2.01s/it] 91%|█████████▏| 2821/3086 [1:34:38<08:52,  2.01s/it] 91%|█████████▏| 2822/3086 [1:34:40<08:50,  2.01s/it] 91%|█████████▏| 2823/3086 [1:34:42<09:01,  2.06s/it] 92%|█████████▏| 2824/3086 [1:34:44<08:52,  2.03s/it] 92%|█████████▏| 2825/3086 [1:34:46<08:41,  2.00s/it] 92%|█████████▏| 2826/3086 [1:34:47<08:09,  1.88s/it] 92%|█████████▏| 2827/3086 [1:34:49<08:27,  1.96s/it] 92%|█████████▏| 2828/3086 [1:34:51<08:01,  1.86s/it] 92%|█████████▏| 2829/3086 [1:34:53<07:56,  1.85s/it] 92%|█████████▏| 2830/3086 [1:34:55<07:34,  1.78s/it]                                                     {'loss': 0.5642, 'grad_norm': 0.4268732964992523, 'learning_rate': 1.659105638366818e-05, 'epoch': 0.92}
 92%|█████████▏| 2830/3086 [1:34:55<07:34,  1.78s/it] 92%|█████████▏| 2831/3086 [1:34:57<08:27,  1.99s/it] 92%|█████████▏| 2832/3086 [1:34:59<08:12,  1.94s/it] 92%|█████████▏| 2833/3086 [1:35:01<08:15,  1.96s/it] 92%|█████████▏| 2834/3086 [1:35:03<08:44,  2.08s/it] 92%|█████████▏| 2835/3086 [1:35:05<08:18,  1.99s/it] 92%|█████████▏| 2836/3086 [1:35:07<08:43,  2.10s/it] 92%|█████████▏| 2837/3086 [1:35:09<08:00,  1.93s/it] 92%|█████████▏| 2838/3086 [1:35:11<08:30,  2.06s/it] 92%|█████████▏| 2839/3086 [1:35:13<08:31,  2.07s/it] 92%|█████████▏| 2840/3086 [1:35:15<08:28,  2.07s/it]                                                     {'loss': 0.554, 'grad_norm': 0.4313948452472687, 'learning_rate': 1.5942968243681142e-05, 'epoch': 0.92}
 92%|█████████▏| 2840/3086 [1:35:15<08:28,  2.07s/it] 92%|█████████▏| 2841/3086 [1:35:17<08:20,  2.04s/it] 92%|█████████▏| 2842/3086 [1:35:19<08:11,  2.01s/it] 92%|█████████▏| 2843/3086 [1:35:21<08:15,  2.04s/it] 92%|█████████▏| 2844/3086 [1:35:24<08:40,  2.15s/it] 92%|█████████▏| 2845/3086 [1:35:26<08:28,  2.11s/it] 92%|█████████▏| 2846/3086 [1:35:28<08:34,  2.15s/it] 92%|█████████▏| 2847/3086 [1:35:30<08:01,  2.02s/it] 92%|█████████▏| 2848/3086 [1:35:32<07:48,  1.97s/it] 92%|█████████▏| 2849/3086 [1:35:33<07:29,  1.90s/it] 92%|█████████▏| 2850/3086 [1:35:35<07:04,  1.80s/it]                                                     {'loss': 0.5598, 'grad_norm': 0.4500274956226349, 'learning_rate': 1.5294880103694102e-05, 'epoch': 0.92}
 92%|█████████▏| 2850/3086 [1:35:35<07:04,  1.80s/it] 92%|█████████▏| 2851/3086 [1:35:37<07:04,  1.81s/it] 92%|█████████▏| 2852/3086 [1:35:39<07:24,  1.90s/it] 92%|█████████▏| 2853/3086 [1:35:41<07:38,  1.97s/it] 92%|█████████▏| 2854/3086 [1:35:44<08:22,  2.17s/it] 93%|█████████▎| 2855/3086 [1:35:45<07:49,  2.03s/it] 93%|█████████▎| 2856/3086 [1:35:48<08:30,  2.22s/it] 93%|█████████▎| 2857/3086 [1:35:50<08:09,  2.14s/it] 93%|█████████▎| 2858/3086 [1:35:52<07:42,  2.03s/it] 93%|█████████▎| 2859/3086 [1:35:53<07:05,  1.87s/it] 93%|█████████▎| 2860/3086 [1:35:55<07:23,  1.96s/it]                                                     {'loss': 0.5677, 'grad_norm': 0.39184099435806274, 'learning_rate': 1.4646791963707066e-05, 'epoch': 0.93}
 93%|█████████▎| 2860/3086 [1:35:55<07:23,  1.96s/it] 93%|█████████▎| 2861/3086 [1:35:57<07:12,  1.92s/it] 93%|█████████▎| 2862/3086 [1:35:59<07:25,  1.99s/it] 93%|█████████▎| 2863/3086 [1:36:02<07:32,  2.03s/it] 93%|█████████▎| 2864/3086 [1:36:04<07:41,  2.08s/it] 93%|█████████▎| 2865/3086 [1:36:06<07:24,  2.01s/it] 93%|█████████▎| 2866/3086 [1:36:07<07:10,  1.96s/it] 93%|█████████▎| 2867/3086 [1:36:09<07:07,  1.95s/it] 93%|█████████▎| 2868/3086 [1:36:11<06:56,  1.91s/it] 93%|█████████▎| 2869/3086 [1:36:14<07:29,  2.07s/it] 93%|█████████▎| 2870/3086 [1:36:16<07:56,  2.21s/it]                                                     {'loss': 0.5557, 'grad_norm': 0.5061575770378113, 'learning_rate': 1.3998703823720027e-05, 'epoch': 0.93}
 93%|█████████▎| 2870/3086 [1:36:16<07:56,  2.21s/it] 93%|█████████▎| 2871/3086 [1:36:18<07:45,  2.17s/it] 93%|█████████▎| 2872/3086 [1:36:21<07:53,  2.21s/it] 93%|█████████▎| 2873/3086 [1:36:23<07:41,  2.17s/it] 93%|█████████▎| 2874/3086 [1:36:25<07:43,  2.19s/it] 93%|█████████▎| 2875/3086 [1:36:27<07:25,  2.11s/it] 93%|█████████▎| 2876/3086 [1:36:29<07:03,  2.01s/it] 93%|█████████▎| 2877/3086 [1:36:31<06:59,  2.01s/it] 93%|█████████▎| 2878/3086 [1:36:33<07:03,  2.03s/it] 93%|█████████▎| 2879/3086 [1:36:35<06:50,  1.98s/it] 93%|█████████▎| 2880/3086 [1:36:36<06:34,  1.92s/it]                                                     {'loss': 0.5478, 'grad_norm': 0.382413774728775, 'learning_rate': 1.335061568373299e-05, 'epoch': 0.93}
 93%|█████████▎| 2880/3086 [1:36:36<06:34,  1.92s/it] 93%|█████████▎| 2881/3086 [1:36:38<06:37,  1.94s/it] 93%|█████████▎| 2882/3086 [1:36:40<06:38,  1.95s/it] 93%|█████████▎| 2883/3086 [1:36:42<06:03,  1.79s/it] 93%|█████████▎| 2884/3086 [1:36:44<06:43,  2.00s/it] 93%|█████████▎| 2885/3086 [1:36:46<06:45,  2.02s/it] 94%|█████████▎| 2886/3086 [1:36:48<06:49,  2.05s/it] 94%|█████████▎| 2887/3086 [1:36:51<06:55,  2.09s/it] 94%|█████████▎| 2888/3086 [1:36:52<06:43,  2.04s/it] 94%|█████████▎| 2889/3086 [1:36:55<06:52,  2.09s/it] 94%|█████████▎| 2890/3086 [1:36:57<06:48,  2.09s/it]                                                     {'loss': 0.5611, 'grad_norm': 0.44270703196525574, 'learning_rate': 1.2702527543745951e-05, 'epoch': 0.94}
 94%|█████████▎| 2890/3086 [1:36:57<06:48,  2.09s/it] 94%|█████████▎| 2891/3086 [1:36:59<06:39,  2.05s/it] 94%|█████████▎| 2892/3086 [1:37:01<06:43,  2.08s/it] 94%|█████████▎| 2893/3086 [1:37:03<06:32,  2.04s/it] 94%|█████████▍| 2894/3086 [1:37:05<06:32,  2.04s/it] 94%|█████████▍| 2895/3086 [1:37:07<06:49,  2.14s/it] 94%|█████████▍| 2896/3086 [1:37:10<07:09,  2.26s/it] 94%|█████████▍| 2897/3086 [1:37:11<06:34,  2.09s/it] 94%|█████████▍| 2898/3086 [1:37:14<06:49,  2.18s/it] 94%|█████████▍| 2899/3086 [1:37:16<07:03,  2.27s/it] 94%|█████████▍| 2900/3086 [1:37:18<06:29,  2.09s/it]                                                     {'loss': 0.5488, 'grad_norm': 0.4254551827907562, 'learning_rate': 1.2054439403758911e-05, 'epoch': 0.94}
 94%|█████████▍| 2900/3086 [1:37:18<06:29,  2.09s/it] 94%|█████████▍| 2901/3086 [1:37:20<06:13,  2.02s/it] 94%|█████████▍| 2902/3086 [1:37:22<06:15,  2.04s/it] 94%|█████████▍| 2903/3086 [1:37:24<06:16,  2.06s/it] 94%|█████████▍| 2904/3086 [1:37:25<05:41,  1.87s/it] 94%|█████████▍| 2905/3086 [1:37:27<05:47,  1.92s/it] 94%|█████████▍| 2906/3086 [1:37:30<06:00,  2.00s/it] 94%|█████████▍| 2907/3086 [1:37:31<05:39,  1.90s/it] 94%|█████████▍| 2908/3086 [1:37:34<06:01,  2.03s/it] 94%|█████████▍| 2909/3086 [1:37:36<05:53,  2.00s/it] 94%|█████████▍| 2910/3086 [1:37:38<06:12,  2.11s/it]                                                     {'loss': 0.5501, 'grad_norm': 0.37712591886520386, 'learning_rate': 1.1406351263771873e-05, 'epoch': 0.94}
 94%|█████████▍| 2910/3086 [1:37:38<06:12,  2.11s/it] 94%|█████████▍| 2911/3086 [1:37:40<05:46,  1.98s/it] 94%|█████████▍| 2912/3086 [1:37:42<06:04,  2.09s/it] 94%|█████████▍| 2913/3086 [1:37:44<05:37,  1.95s/it] 94%|█████████▍| 2914/3086 [1:37:46<05:53,  2.06s/it] 94%|█████████▍| 2915/3086 [1:37:48<05:45,  2.02s/it] 94%|█████████▍| 2916/3086 [1:37:50<05:59,  2.11s/it] 95%|█████████▍| 2917/3086 [1:37:52<05:39,  2.01s/it] 95%|█████████▍| 2918/3086 [1:37:54<05:42,  2.04s/it] 95%|█████████▍| 2919/3086 [1:37:56<05:25,  1.95s/it] 95%|█████████▍| 2920/3086 [1:37:58<05:15,  1.90s/it]                                                     {'loss': 0.5628, 'grad_norm': 0.4051651358604431, 'learning_rate': 1.0758263123784836e-05, 'epoch': 0.95}
 95%|█████████▍| 2920/3086 [1:37:58<05:15,  1.90s/it] 95%|█████████▍| 2921/3086 [1:38:00<05:18,  1.93s/it] 95%|█████████▍| 2922/3086 [1:38:01<05:09,  1.89s/it] 95%|█████████▍| 2923/3086 [1:38:04<05:35,  2.06s/it] 95%|█████████▍| 2924/3086 [1:38:06<05:27,  2.02s/it] 95%|█████████▍| 2925/3086 [1:38:08<05:29,  2.05s/it] 95%|█████████▍| 2926/3086 [1:38:10<05:15,  1.97s/it] 95%|█████████▍| 2927/3086 [1:38:12<05:25,  2.05s/it] 95%|█████████▍| 2928/3086 [1:38:14<05:11,  1.97s/it] 95%|█████████▍| 2929/3086 [1:38:16<05:08,  1.97s/it] 95%|█████████▍| 2930/3086 [1:38:18<05:16,  2.03s/it]                                                     {'loss': 0.5618, 'grad_norm': 0.39289265871047974, 'learning_rate': 1.0110174983797798e-05, 'epoch': 0.95}
 95%|█████████▍| 2930/3086 [1:38:18<05:16,  2.03s/it] 95%|█████████▍| 2931/3086 [1:38:20<05:14,  2.03s/it] 95%|█████████▌| 2932/3086 [1:38:22<05:16,  2.06s/it] 95%|█████████▌| 2933/3086 [1:38:24<04:53,  1.92s/it] 95%|█████████▌| 2934/3086 [1:38:25<04:44,  1.87s/it] 95%|█████████▌| 2935/3086 [1:38:27<04:49,  1.92s/it] 95%|█████████▌| 2936/3086 [1:38:29<04:40,  1.87s/it] 95%|█████████▌| 2937/3086 [1:38:31<04:34,  1.84s/it] 95%|█████████▌| 2938/3086 [1:38:33<04:33,  1.85s/it] 95%|█████████▌| 2939/3086 [1:38:35<04:47,  1.95s/it] 95%|█████████▌| 2940/3086 [1:38:37<04:45,  1.95s/it]                                                     {'loss': 0.5639, 'grad_norm': 0.3081526756286621, 'learning_rate': 9.46208684381076e-06, 'epoch': 0.95}
 95%|█████████▌| 2940/3086 [1:38:37<04:45,  1.95s/it] 95%|█████████▌| 2941/3086 [1:38:39<04:42,  1.95s/it] 95%|█████████▌| 2942/3086 [1:38:41<05:06,  2.13s/it] 95%|█████████▌| 2943/3086 [1:38:43<04:47,  2.01s/it] 95%|█████████▌| 2944/3086 [1:38:45<04:30,  1.91s/it] 95%|█████████▌| 2945/3086 [1:38:47<04:36,  1.96s/it] 95%|█████████▌| 2946/3086 [1:38:49<04:21,  1.87s/it] 95%|█████████▌| 2947/3086 [1:38:51<04:34,  1.97s/it] 96%|█████████▌| 2948/3086 [1:38:53<04:33,  1.99s/it] 96%|█████████▌| 2949/3086 [1:38:54<04:20,  1.90s/it] 96%|█████████▌| 2950/3086 [1:38:56<04:19,  1.91s/it]                                                     {'loss': 0.5587, 'grad_norm': 0.4433385133743286, 'learning_rate': 8.81399870382372e-06, 'epoch': 0.96}
 96%|█████████▌| 2950/3086 [1:38:56<04:19,  1.91s/it] 96%|█████████▌| 2951/3086 [1:38:59<04:25,  1.97s/it] 96%|█████████▌| 2952/3086 [1:39:00<04:22,  1.96s/it] 96%|█████████▌| 2953/3086 [1:39:02<04:12,  1.90s/it] 96%|█████████▌| 2954/3086 [1:39:04<04:22,  1.99s/it] 96%|█████████▌| 2955/3086 [1:39:07<04:34,  2.10s/it] 96%|█████████▌| 2956/3086 [1:39:09<04:25,  2.04s/it] 96%|█████████▌| 2957/3086 [1:39:11<04:21,  2.03s/it] 96%|█████████▌| 2958/3086 [1:39:13<04:14,  1.99s/it] 96%|█████████▌| 2959/3086 [1:39:15<04:23,  2.08s/it] 96%|█████████▌| 2960/3086 [1:39:17<04:20,  2.07s/it]                                                     {'loss': 0.5492, 'grad_norm': 0.368308424949646, 'learning_rate': 8.165910563836682e-06, 'epoch': 0.96}
 96%|█████████▌| 2960/3086 [1:39:17<04:20,  2.07s/it] 96%|█████████▌| 2961/3086 [1:39:19<04:09,  2.00s/it] 96%|█████████▌| 2962/3086 [1:39:21<04:08,  2.00s/it] 96%|█████████▌| 2963/3086 [1:39:23<04:25,  2.16s/it] 96%|█████████▌| 2964/3086 [1:39:25<04:14,  2.09s/it] 96%|█████████▌| 2965/3086 [1:39:27<04:00,  1.99s/it] 96%|█████████▌| 2966/3086 [1:39:29<03:49,  1.91s/it] 96%|█████████▌| 2967/3086 [1:39:30<03:35,  1.81s/it] 96%|█████████▌| 2968/3086 [1:39:32<03:38,  1.85s/it] 96%|█████████▌| 2969/3086 [1:39:34<03:28,  1.78s/it] 96%|█████████▌| 2970/3086 [1:39:36<03:44,  1.93s/it]                                                     {'loss': 0.553, 'grad_norm': 0.486081600189209, 'learning_rate': 7.5178224238496435e-06, 'epoch': 0.96}
 96%|█████████▌| 2970/3086 [1:39:36<03:44,  1.93s/it] 96%|█████████▋| 2971/3086 [1:39:38<03:47,  1.98s/it] 96%|█████████▋| 2972/3086 [1:39:40<03:51,  2.03s/it] 96%|█████████▋| 2973/3086 [1:39:42<03:39,  1.94s/it] 96%|█████████▋| 2974/3086 [1:39:44<03:52,  2.08s/it] 96%|█████████▋| 2975/3086 [1:39:46<03:43,  2.01s/it] 96%|█████████▋| 2976/3086 [1:39:48<03:40,  2.00s/it] 96%|█████████▋| 2977/3086 [1:39:51<03:48,  2.10s/it] 97%|█████████▋| 2978/3086 [1:39:53<03:46,  2.10s/it] 97%|█████████▋| 2979/3086 [1:39:55<03:53,  2.18s/it] 97%|█████████▋| 2980/3086 [1:39:57<03:36,  2.04s/it]                                                     {'loss': 0.5489, 'grad_norm': 0.3590598404407501, 'learning_rate': 6.869734283862605e-06, 'epoch': 0.97}
 97%|█████████▋| 2980/3086 [1:39:57<03:36,  2.04s/it] 97%|█████████▋| 2981/3086 [1:39:59<03:23,  1.94s/it] 97%|█████████▋| 2982/3086 [1:40:01<03:35,  2.07s/it] 97%|█████████▋| 2983/3086 [1:40:03<03:36,  2.10s/it] 97%|█████████▋| 2984/3086 [1:40:05<03:30,  2.06s/it] 97%|█████████▋| 2985/3086 [1:40:07<03:20,  1.99s/it] 97%|█████████▋| 2986/3086 [1:40:10<04:08,  2.48s/it] 97%|█████████▋| 2987/3086 [1:40:12<03:40,  2.22s/it] 97%|█████████▋| 2988/3086 [1:40:14<03:26,  2.11s/it] 97%|█████████▋| 2989/3086 [1:40:16<03:32,  2.19s/it] 97%|█████████▋| 2990/3086 [1:40:18<03:20,  2.08s/it]                                                     {'loss': 0.5556, 'grad_norm': 0.40261489152908325, 'learning_rate': 6.221646143875568e-06, 'epoch': 0.97}
 97%|█████████▋| 2990/3086 [1:40:18<03:20,  2.08s/it] 97%|█████████▋| 2991/3086 [1:40:20<03:05,  1.95s/it] 97%|█████████▋| 2992/3086 [1:40:22<03:00,  1.92s/it] 97%|█████████▋| 2993/3086 [1:40:24<03:06,  2.01s/it] 97%|█████████▋| 2994/3086 [1:40:26<03:18,  2.16s/it] 97%|█████████▋| 2995/3086 [1:40:29<03:22,  2.22s/it] 97%|█████████▋| 2996/3086 [1:40:31<03:17,  2.19s/it] 97%|█████████▋| 2997/3086 [1:40:33<03:11,  2.15s/it] 97%|█████████▋| 2998/3086 [1:40:35<03:06,  2.11s/it] 97%|█████████▋| 2999/3086 [1:40:37<02:58,  2.05s/it] 97%|█████████▋| 3000/3086 [1:40:39<02:56,  2.05s/it]                                                     {'loss': 0.5487, 'grad_norm': 0.4412515461444855, 'learning_rate': 5.57355800388853e-06, 'epoch': 0.97}
 97%|█████████▋| 3000/3086 [1:40:39<02:56,  2.05s/it] 97%|█████████▋| 3001/3086 [1:40:41<02:52,  2.02s/it] 97%|█████████▋| 3002/3086 [1:40:43<03:01,  2.17s/it] 97%|█████████▋| 3003/3086 [1:40:46<02:59,  2.16s/it] 97%|█████████▋| 3004/3086 [1:40:48<02:55,  2.14s/it] 97%|█████████▋| 3005/3086 [1:40:50<02:49,  2.10s/it] 97%|█████████▋| 3006/3086 [1:40:51<02:36,  1.96s/it] 97%|█████████▋| 3007/3086 [1:40:53<02:27,  1.87s/it] 97%|█████████▋| 3008/3086 [1:40:56<02:45,  2.12s/it] 98%|█████████▊| 3009/3086 [1:40:58<02:40,  2.08s/it] 98%|█████████▊| 3010/3086 [1:41:00<02:50,  2.25s/it]                                                     {'loss': 0.5427, 'grad_norm': 0.3236030638217926, 'learning_rate': 4.9254698639014905e-06, 'epoch': 0.98}
 98%|█████████▊| 3010/3086 [1:41:00<02:50,  2.25s/it] 98%|█████████▊| 3011/3086 [1:41:02<02:41,  2.15s/it] 98%|█████████▊| 3012/3086 [1:41:04<02:35,  2.10s/it] 98%|█████████▊| 3013/3086 [1:41:06<02:35,  2.13s/it] 98%|█████████▊| 3014/3086 [1:41:08<02:28,  2.06s/it] 98%|█████████▊| 3015/3086 [1:41:10<02:29,  2.10s/it] 98%|█████████▊| 3016/3086 [1:41:12<02:19,  2.00s/it] 98%|█████████▊| 3017/3086 [1:41:14<02:07,  1.85s/it] 98%|█████████▊| 3018/3086 [1:41:16<02:06,  1.86s/it] 98%|█████████▊| 3019/3086 [1:41:18<02:06,  1.89s/it] 98%|█████████▊| 3020/3086 [1:41:20<02:10,  1.97s/it]                                                     {'loss': 0.5523, 'grad_norm': 0.4594671130180359, 'learning_rate': 4.277381723914452e-06, 'epoch': 0.98}
 98%|█████████▊| 3020/3086 [1:41:20<02:10,  1.97s/it] 98%|█████████▊| 3021/3086 [1:41:22<02:06,  1.94s/it] 98%|█████████▊| 3022/3086 [1:41:23<02:03,  1.94s/it] 98%|█████████▊| 3023/3086 [1:41:25<01:59,  1.89s/it] 98%|█████████▊| 3024/3086 [1:41:27<02:00,  1.94s/it] 98%|█████████▊| 3025/3086 [1:41:29<02:00,  1.98s/it] 98%|█████████▊| 3026/3086 [1:41:31<01:53,  1.89s/it] 98%|█████████▊| 3027/3086 [1:41:33<01:58,  2.02s/it] 98%|█████████▊| 3028/3086 [1:41:35<01:54,  1.97s/it] 98%|█████████▊| 3029/3086 [1:41:37<01:52,  1.97s/it] 98%|█████████▊| 3030/3086 [1:41:39<01:49,  1.95s/it]                                                     {'loss': 0.5563, 'grad_norm': 0.42036864161491394, 'learning_rate': 3.6292935839274145e-06, 'epoch': 0.98}
 98%|█████████▊| 3030/3086 [1:41:39<01:49,  1.95s/it] 98%|█████████▊| 3031/3086 [1:41:42<02:05,  2.29s/it] 98%|█████████▊| 3032/3086 [1:41:45<02:08,  2.38s/it] 98%|█████████▊| 3033/3086 [1:41:47<02:00,  2.27s/it] 98%|█████████▊| 3034/3086 [1:41:49<01:52,  2.16s/it] 98%|█████████▊| 3035/3086 [1:41:51<01:46,  2.08s/it] 98%|█████████▊| 3036/3086 [1:41:52<01:39,  1.99s/it] 98%|█████████▊| 3037/3086 [1:41:54<01:37,  2.00s/it] 98%|█████████▊| 3038/3086 [1:41:56<01:32,  1.93s/it] 98%|█████████▊| 3039/3086 [1:41:58<01:31,  1.94s/it] 99%|█████████▊| 3040/3086 [1:42:00<01:29,  1.94s/it]                                                     {'loss': 0.5656, 'grad_norm': 0.43055260181427, 'learning_rate': 2.981205443940376e-06, 'epoch': 0.99}
 99%|█████████▊| 3040/3086 [1:42:00<01:29,  1.94s/it] 99%|█████████▊| 3041/3086 [1:42:02<01:23,  1.85s/it] 99%|█████████▊| 3042/3086 [1:42:04<01:22,  1.87s/it] 99%|█████████▊| 3043/3086 [1:42:05<01:16,  1.78s/it] 99%|█████████▊| 3044/3086 [1:42:08<01:26,  2.07s/it] 99%|█████████▊| 3045/3086 [1:42:10<01:19,  1.95s/it] 99%|█████████▊| 3046/3086 [1:42:12<01:25,  2.14s/it] 99%|█████████▊| 3047/3086 [1:42:14<01:21,  2.09s/it] 99%|█████████▉| 3048/3086 [1:42:16<01:12,  1.91s/it] 99%|█████████▉| 3049/3086 [1:42:17<01:07,  1.82s/it] 99%|█████████▉| 3050/3086 [1:42:19<01:07,  1.88s/it]                                                     {'loss': 0.5594, 'grad_norm': 0.3967965543270111, 'learning_rate': 2.3331173039533376e-06, 'epoch': 0.99}
 99%|█████████▉| 3050/3086 [1:42:19<01:07,  1.88s/it] 99%|█████████▉| 3051/3086 [1:42:21<01:04,  1.86s/it] 99%|█████████▉| 3052/3086 [1:42:23<01:02,  1.85s/it] 99%|█████████▉| 3053/3086 [1:42:25<01:01,  1.88s/it] 99%|█████████▉| 3054/3086 [1:42:26<00:55,  1.73s/it] 99%|█████████▉| 3055/3086 [1:42:28<00:56,  1.81s/it] 99%|█████████▉| 3056/3086 [1:42:30<00:56,  1.87s/it] 99%|█████████▉| 3057/3086 [1:42:33<00:59,  2.04s/it] 99%|█████████▉| 3058/3086 [1:42:34<00:55,  1.97s/it] 99%|█████████▉| 3059/3086 [1:42:36<00:49,  1.82s/it] 99%|█████████▉| 3060/3086 [1:42:38<00:49,  1.92s/it]                                                     {'loss': 0.5596, 'grad_norm': 0.39832428097724915, 'learning_rate': 1.6850291639662994e-06, 'epoch': 0.99}
 99%|█████████▉| 3060/3086 [1:42:38<00:49,  1.92s/it] 99%|█████████▉| 3061/3086 [1:42:40<00:45,  1.84s/it] 99%|█████████▉| 3062/3086 [1:42:42<00:45,  1.91s/it] 99%|█████████▉| 3063/3086 [1:42:43<00:40,  1.78s/it] 99%|█████████▉| 3064/3086 [1:42:45<00:38,  1.74s/it] 99%|█████████▉| 3065/3086 [1:42:47<00:40,  1.94s/it] 99%|█████████▉| 3066/3086 [1:42:50<00:40,  2.05s/it] 99%|█████████▉| 3067/3086 [1:42:51<00:36,  1.93s/it] 99%|█████████▉| 3068/3086 [1:42:53<00:35,  1.96s/it] 99%|█████████▉| 3069/3086 [1:42:55<00:31,  1.83s/it] 99%|█████████▉| 3070/3086 [1:42:57<00:30,  1.90s/it]                                                     {'loss': 0.547, 'grad_norm': 0.42715463042259216, 'learning_rate': 1.0369410239792612e-06, 'epoch': 0.99}
 99%|█████████▉| 3070/3086 [1:42:57<00:30,  1.90s/it]100%|█████████▉| 3071/3086 [1:42:59<00:31,  2.08s/it]100%|█████████▉| 3072/3086 [1:43:02<00:29,  2.10s/it]100%|█████████▉| 3073/3086 [1:43:04<00:27,  2.09s/it]100%|█████████▉| 3074/3086 [1:43:06<00:26,  2.17s/it]100%|█████████▉| 3075/3086 [1:43:08<00:23,  2.16s/it]100%|█████████▉| 3076/3086 [1:43:10<00:20,  2.07s/it]100%|█████████▉| 3077/3086 [1:43:12<00:18,  2.07s/it]100%|█████████▉| 3078/3086 [1:43:15<00:17,  2.24s/it]100%|█████████▉| 3079/3086 [1:43:17<00:14,  2.14s/it]100%|█████████▉| 3080/3086 [1:43:18<00:11,  2.00s/it]                                                     {'loss': 0.5564, 'grad_norm': 0.3845086097717285, 'learning_rate': 3.88852883992223e-07, 'epoch': 1.0}
100%|█████████▉| 3080/3086 [1:43:18<00:11,  2.00s/it]100%|█████████▉| 3081/3086 [1:43:20<00:09,  1.98s/it]100%|█████████▉| 3082/3086 [1:43:23<00:08,  2.11s/it]100%|█████████▉| 3083/3086 [1:43:25<00:06,  2.13s/it]100%|█████████▉| 3084/3086 [1:43:27<00:04,  2.06s/it]100%|█████████▉| 3085/3086 [1:43:29<00:02,  2.09s/it]100%|██████████| 3086/3086 [1:43:30<00:00,  1.95s/it][INFO|trainer.py:3503] 2024-11-12 05:21:23,465 >> Saving model checkpoint to /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/xlmr-qwe2.5-math7b-metamath/checkpoint-3086
[INFO|configuration_utils.py:472] 2024-11-12 05:21:23,471 >> Configuration saved in /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/xlmr-qwe2.5-math7b-metamath/checkpoint-3086/config.json
[INFO|tokenization_utils_base.py:2684] 2024-11-12 05:21:23,594 >> tokenizer config file saved in /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/xlmr-qwe2.5-math7b-metamath/checkpoint-3086/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2024-11-12 05:21:23,597 >> Special tokens file saved in /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/xlmr-qwe2.5-math7b-metamath/checkpoint-3086/special_tokens_map.json
[INFO|trainer.py:2394] 2024-11-12 05:21:24,390 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                     {'train_runtime': 6220.0628, 'train_samples_per_second': 63.504, 'train_steps_per_second': 0.496, 'train_loss': 0.5904236524350753, 'epoch': 1.0}
100%|██████████| 3086/3086 [1:43:40<00:00,  1.95s/it]100%|██████████| 3086/3086 [1:43:40<00:00,  2.02s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:3503] 2024-11-12 05:21:31,656 >> Saving model checkpoint to /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/xlmr-qwe2.5-math7b-metamath
[INFO|configuration_utils.py:472] 2024-11-12 05:21:31,660 >> Configuration saved in /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/xlmr-qwe2.5-math7b-metamath/config.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|tokenization_utils_base.py:2684] 2024-11-12 05:21:31,775 >> tokenizer config file saved in /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/xlmr-qwe2.5-math7b-metamath/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2024-11-12 05:21:31,777 >> Special tokens file saved in /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/xlmr-qwe2.5-math7b-metamath/special_tokens_map.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
***** train metrics *****
  epoch                    =        1.0
  total_flos               =        0GF
  train_loss               =     0.5904
  train_runtime            = 1:43:40.06
  train_samples            =     395000
  train_samples_per_second =     63.504
  train_steps_per_second   =      0.496
[INFO|modelcard.py:449] 2024-11-12 05:21:32,611 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'dataset': {'name': '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json', 'type': '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json'}}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[2024-11-12 05:21:34,814] [INFO] [launch.py:351:main] Process 250 exits successfully.
[2024-11-12 05:21:35,815] [INFO] [launch.py:351:main] Process 247 exits successfully.
[2024-11-12 05:21:35,815] [INFO] [launch.py:351:main] Process 248 exits successfully.
[2024-11-12 05:21:38,819] [INFO] [launch.py:351:main] Process 246 exits successfully.
[2024-11-12 05:21:38,819] [INFO] [launch.py:351:main] Process 252 exits successfully.
[2024-11-12 05:21:38,819] [INFO] [launch.py:351:main] Process 245 exits successfully.
[2024-11-12 05:21:38,819] [INFO] [launch.py:351:main] Process 251 exits successfully.
[2024-11-12 05:21:38,819] [INFO] [launch.py:351:main] Process 249 exits successfully.
