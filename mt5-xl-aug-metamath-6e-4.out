[2024-11-14 01:29:05,684] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
[2024-11-14 01:29:07,382] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-11-14 01:29:07,383] [INFO] [runner.py:568:main] cmd = /root/work/huangxin/envs/hs/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=9901 --enable_each_rank_log=None /root/work/huangxin/nanda/ImplicitTransBridge-master/train.py --deepspeed /root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json --dataset_name /root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json --preprocessing_num_workers 64 --llm_path /root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct --slm_path_a /root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt --slm_path_b /root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt --stage 0 --output_dir /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5xl-aug-qwe2.5-math7b-metamath --do_train --max_seq_length 1024 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --learning_rate 6e-4 --num_train_epochs 1 --save_only_model --logging_steps 10 --save_steps 2000 --seed 42 --overwrite_output_dir --bf16
[2024-11-14 01:29:08,772] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[2024-11-14 01:29:10,173] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-11-14 01:29:10,173] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-11-14 01:29:10,173] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-11-14 01:29:10,173] [INFO] [launch.py:164:main] dist_world_size=8
[2024-11-14 01:29:10,173] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-11-14 01:29:10,185] [INFO] [launch.py:256:main] process 245 spawned with command: ['/root/work/huangxin/envs/hs/bin/python', '-u', '/root/work/huangxin/nanda/ImplicitTransBridge-master/train.py', '--local_rank=0', '--deepspeed', '/root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json', '--dataset_name', '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json', '--preprocessing_num_workers', '64', '--llm_path', '/root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct', '--slm_path_a', '/root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt', '--slm_path_b', '/root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt', '--stage', '0', '--output_dir', '/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5xl-aug-qwe2.5-math7b-metamath', '--do_train', '--max_seq_length', '1024', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '6e-4', '--num_train_epochs', '1', '--save_only_model', '--logging_steps', '10', '--save_steps', '2000', '--seed', '42', '--overwrite_output_dir', '--bf16']
[2024-11-14 01:29:10,195] [INFO] [launch.py:256:main] process 246 spawned with command: ['/root/work/huangxin/envs/hs/bin/python', '-u', '/root/work/huangxin/nanda/ImplicitTransBridge-master/train.py', '--local_rank=1', '--deepspeed', '/root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json', '--dataset_name', '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json', '--preprocessing_num_workers', '64', '--llm_path', '/root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct', '--slm_path_a', '/root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt', '--slm_path_b', '/root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt', '--stage', '0', '--output_dir', '/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5xl-aug-qwe2.5-math7b-metamath', '--do_train', '--max_seq_length', '1024', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '6e-4', '--num_train_epochs', '1', '--save_only_model', '--logging_steps', '10', '--save_steps', '2000', '--seed', '42', '--overwrite_output_dir', '--bf16']
[2024-11-14 01:29:10,206] [INFO] [launch.py:256:main] process 247 spawned with command: ['/root/work/huangxin/envs/hs/bin/python', '-u', '/root/work/huangxin/nanda/ImplicitTransBridge-master/train.py', '--local_rank=2', '--deepspeed', '/root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json', '--dataset_name', '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json', '--preprocessing_num_workers', '64', '--llm_path', '/root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct', '--slm_path_a', '/root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt', '--slm_path_b', '/root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt', '--stage', '0', '--output_dir', '/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5xl-aug-qwe2.5-math7b-metamath', '--do_train', '--max_seq_length', '1024', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '6e-4', '--num_train_epochs', '1', '--save_only_model', '--logging_steps', '10', '--save_steps', '2000', '--seed', '42', '--overwrite_output_dir', '--bf16']
[2024-11-14 01:29:10,217] [INFO] [launch.py:256:main] process 248 spawned with command: ['/root/work/huangxin/envs/hs/bin/python', '-u', '/root/work/huangxin/nanda/ImplicitTransBridge-master/train.py', '--local_rank=3', '--deepspeed', '/root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json', '--dataset_name', '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json', '--preprocessing_num_workers', '64', '--llm_path', '/root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct', '--slm_path_a', '/root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt', '--slm_path_b', '/root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt', '--stage', '0', '--output_dir', '/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5xl-aug-qwe2.5-math7b-metamath', '--do_train', '--max_seq_length', '1024', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '6e-4', '--num_train_epochs', '1', '--save_only_model', '--logging_steps', '10', '--save_steps', '2000', '--seed', '42', '--overwrite_output_dir', '--bf16']
[2024-11-14 01:29:10,227] [INFO] [launch.py:256:main] process 249 spawned with command: ['/root/work/huangxin/envs/hs/bin/python', '-u', '/root/work/huangxin/nanda/ImplicitTransBridge-master/train.py', '--local_rank=4', '--deepspeed', '/root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json', '--dataset_name', '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json', '--preprocessing_num_workers', '64', '--llm_path', '/root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct', '--slm_path_a', '/root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt', '--slm_path_b', '/root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt', '--stage', '0', '--output_dir', '/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5xl-aug-qwe2.5-math7b-metamath', '--do_train', '--max_seq_length', '1024', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '6e-4', '--num_train_epochs', '1', '--save_only_model', '--logging_steps', '10', '--save_steps', '2000', '--seed', '42', '--overwrite_output_dir', '--bf16']
[2024-11-14 01:29:10,237] [INFO] [launch.py:256:main] process 250 spawned with command: ['/root/work/huangxin/envs/hs/bin/python', '-u', '/root/work/huangxin/nanda/ImplicitTransBridge-master/train.py', '--local_rank=5', '--deepspeed', '/root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json', '--dataset_name', '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json', '--preprocessing_num_workers', '64', '--llm_path', '/root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct', '--slm_path_a', '/root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt', '--slm_path_b', '/root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt', '--stage', '0', '--output_dir', '/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5xl-aug-qwe2.5-math7b-metamath', '--do_train', '--max_seq_length', '1024', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '6e-4', '--num_train_epochs', '1', '--save_only_model', '--logging_steps', '10', '--save_steps', '2000', '--seed', '42', '--overwrite_output_dir', '--bf16']
[2024-11-14 01:29:10,245] [INFO] [launch.py:256:main] process 251 spawned with command: ['/root/work/huangxin/envs/hs/bin/python', '-u', '/root/work/huangxin/nanda/ImplicitTransBridge-master/train.py', '--local_rank=6', '--deepspeed', '/root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json', '--dataset_name', '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json', '--preprocessing_num_workers', '64', '--llm_path', '/root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct', '--slm_path_a', '/root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt', '--slm_path_b', '/root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt', '--stage', '0', '--output_dir', '/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5xl-aug-qwe2.5-math7b-metamath', '--do_train', '--max_seq_length', '1024', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '6e-4', '--num_train_epochs', '1', '--save_only_model', '--logging_steps', '10', '--save_steps', '2000', '--seed', '42', '--overwrite_output_dir', '--bf16']
[2024-11-14 01:29:10,253] [INFO] [launch.py:256:main] process 252 spawned with command: ['/root/work/huangxin/envs/hs/bin/python', '-u', '/root/work/huangxin/nanda/ImplicitTransBridge-master/train.py', '--local_rank=7', '--deepspeed', '/root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json', '--dataset_name', '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json', '--preprocessing_num_workers', '64', '--llm_path', '/root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct', '--slm_path_a', '/root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt', '--slm_path_b', '/root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt', '--stage', '0', '--output_dir', '/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5xl-aug-qwe2.5-math7b-metamath', '--do_train', '--max_seq_length', '1024', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '6e-4', '--num_train_epochs', '1', '--save_only_model', '--logging_steps', '10', '--save_steps', '2000', '--seed', '42', '--overwrite_output_dir', '--bf16']
[2024-11-14 01:29:14,731] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-14 01:29:14,736] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-14 01:29:14,737] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-14 01:29:14,737] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-14 01:29:14,740] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-14 01:29:14,740] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-14 01:29:14,741] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-14 01:29:14,744] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/root/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[2024-11-14 01:29:15,369] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-14 01:29:15,373] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-14 01:29:15,396] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-14 01:29:15,396] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-11-14 01:29:15,397] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-14 01:29:15,398] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-14 01:29:15,401] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-14 01:29:15,427] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-14 01:29:15,431] [INFO] [comm.py:637:init_distributed] cdb=None
11/14/2024 01:29:15 - WARNING - __main__ - Process rank: 7, device: cuda:7, n_gpu: 1, distributed training: True, 16-bits training: False
11/14/2024 01:29:15 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, 16-bits training: False
[WARNING|logging.py:328] 2024-11-14 01:29:16,238 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[WARNING|logging.py:328] 2024-11-14 01:29:16,244 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
/root/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
11/14/2024 01:29:16 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
11/14/2024 01:29:16 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/root/work/huangxin/nanda/ImplicitTransBridge-master/ds_configs/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=no,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0006,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5xl-aug-qwe2.5-math7b-metamath/runs/Nov14_01-29-14_dt-3f438f9c1ce34127b03dfa1fd050d2d6-master-8a9da368afa1-0,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5xl-aug-qwe2.5-math7b-metamath,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5xl-aug-qwe2.5-math7b-metamath,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=2000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|tokenization_utils_base.py:2267] 2024-11-14 01:29:16,706 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2267] 2024-11-14 01:29:16,706 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2267] 2024-11-14 01:29:16,706 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2267] 2024-11-14 01:29:16,706 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2267] 2024-11-14 01:29:16,706 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2267] 2024-11-14 01:29:16,706 >> loading file tokenizer_config.json
11/14/2024 01:29:16 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, 16-bits training: False
11/14/2024 01:29:16 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
11/14/2024 01:29:16 - WARNING - __main__ - Process rank: 5, device: cuda:5, n_gpu: 1, distributed training: True, 16-bits training: False
11/14/2024 01:29:16 - WARNING - __main__ - Process rank: 6, device: cuda:6, n_gpu: 1, distributed training: True, 16-bits training: False
11/14/2024 01:29:16 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1, distributed training: True, 16-bits training: False
[INFO|tokenization_utils_base.py:2513] 2024-11-14 01:29:16,882 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:731] 2024-11-14 01:29:16,883 >> loading configuration file /root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt/config.json
[INFO|configuration_utils.py:800] 2024-11-14 01:29:16,886 >> Model config MT5Config {
  "_name_or_path": "/root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt",
  "architectures": [
    "MT5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 5120,
  "d_kv": 64,
  "d_model": 2048,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "mt5",
  "num_decoder_layers": 24,
  "num_heads": 32,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "tokenizer_class": "T5Tokenizer",
  "torch_dtype": "float32",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 250112
}

[INFO|tokenization_utils_base.py:2267] 2024-11-14 01:29:16,887 >> loading file spiece.model
[INFO|tokenization_utils_base.py:2267] 2024-11-14 01:29:16,887 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2267] 2024-11-14 01:29:16,887 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2267] 2024-11-14 01:29:16,887 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2267] 2024-11-14 01:29:16,887 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:731] 2024-11-14 01:29:16,887 >> loading configuration file /root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt/config.json
[INFO|configuration_utils.py:800] 2024-11-14 01:29:16,888 >> Model config MT5Config {
  "_name_or_path": "/root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt",
  "architectures": [
    "MT5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 5120,
  "d_kv": 64,
  "d_model": 2048,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "mt5",
  "num_decoder_layers": 24,
  "num_heads": 32,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "tokenizer_class": "T5Tokenizer",
  "torch_dtype": "float32",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 250112
}

[WARNING|logging.py:328] 2024-11-14 01:29:17,237 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
/root/.local/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
[WARNING|logging.py:328] 2024-11-14 01:29:17,371 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[WARNING|logging.py:328] 2024-11-14 01:29:17,383 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[WARNING|logging.py:328] 2024-11-14 01:29:17,394 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[WARNING|logging.py:328] 2024-11-14 01:29:17,397 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[WARNING|logging.py:328] 2024-11-14 01:29:17,404 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
/root/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][INFO|configuration_utils.py:731] 2024-11-14 01:29:18,304 >> loading configuration file /root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt/config.json
[INFO|configuration_utils.py:800] 2024-11-14 01:29:18,306 >> Model config MT5Config {
  "_name_or_path": "/root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt",
  "architectures": [
    "MT5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 5120,
  "d_kv": 64,
  "d_model": 2048,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "mt5",
  "num_decoder_layers": 24,
  "num_heads": 32,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "tokenizer_class": "T5Tokenizer",
  "torch_dtype": "float32",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 250112
}

/root/.local/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/root/.local/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
[INFO|configuration_utils.py:731] 2024-11-14 01:29:18,942 >> loading configuration file /root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct/config.json
[INFO|configuration_utils.py:800] 2024-11-14 01:29:18,943 >> Model config Qwen2Config {
  "_name_or_path": "/root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 4096,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_theta": 10000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|modeling_utils.py:3675] 2024-11-14 01:29:18,968 >> loading weights file /root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct/model.safetensors.index.json
[INFO|configuration_utils.py:1038] 2024-11-14 01:29:18,988 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.80s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:17,  5.98s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:17,  5.93s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:16,  5.37s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:18,  6.04s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:07<00:21,  7.22s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:07<00:21,  7.17s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:16,  5.37s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.81s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:14<00:15,  7.54s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:14,  7.04s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:12<00:13,  6.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:14,  7.06s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:14<00:15,  7.52s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.77s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:20<00:07,  7.14s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:20<00:07,  7.27s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:21<00:07,  7.47s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:21<00:07,  7.45s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:20<00:07,  7.25s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:22<00:07,  7.72s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:21<00:07,  7.46s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:22<00:07,  7.71s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  6.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  6.97s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  6.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  6.76s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:26<00:00,  6.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:26<00:00,  6.74s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  6.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  6.77s/it]
[INFO|modeling_utils.py:4507] 2024-11-14 01:29:46,156 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4515] 2024-11-14 01:29:46,156 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:991] 2024-11-14 01:29:46,158 >> loading configuration file /root/work/huangxin/nanda/models/Qwen/Qwen2.5-Math-7B-Instruct/generation_config.json
[INFO|configuration_utils.py:1038] 2024-11-14 01:29:46,158 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643
}

[INFO|configuration_utils.py:731] 2024-11-14 01:29:46,160 >> loading configuration file /root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt/config.json
[INFO|configuration_utils.py:800] 2024-11-14 01:29:46,161 >> Model config MT5Config {
  "_name_or_path": "/root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt",
  "architectures": [
    "MT5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 5120,
  "d_kv": 64,
  "d_model": 2048,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "mt5",
  "num_decoder_layers": 24,
  "num_heads": 32,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "tokenizer_class": "T5Tokenizer",
  "torch_dtype": "float32",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 250112
}

Loading checkpoint shards: 100%|██████████| 4/4 [00:26<00:00,  6.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:26<00:00,  6.52s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:26<00:00,  6.54s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:26<00:00,  6.61s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:26<00:00,  6.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:26<00:00,  6.61s/it]
[INFO|modeling_utils.py:3675] 2024-11-14 01:29:46,206 >> loading weights file /root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt/model.safetensors.index.json
Loading checkpoint shards: 100%|██████████| 4/4 [00:28<00:00,  6.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:28<00:00,  7.07s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.30s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.31s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.34s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.71s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.28s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.36s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.38s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.14s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.17s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.18s/it]
[INFO|modeling_utils.py:4497] 2024-11-14 01:29:55,555 >> Some weights of the model checkpoint at /root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt were not used when initializing MT5Model: ['lm_head.weight']
- This IS expected if you are initializing MT5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MT5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:4515] 2024-11-14 01:29:55,555 >> All the weights of MT5Model were initialized from the model checkpoint at /root/work/huangxin/nanda/models/google/mt5-xl-lm-adapt.
If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5Model for predictions without further training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.19s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.18s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.14s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.19s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.38s/it]
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - Small LM A model size: 3230.390272 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - Small LM A model size: 3230.390272 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - Small LM A model size: 3230.390272 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - Small LM A model size: 3230.390272 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - Small LM A model size: 3230.390272 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - Small LM A model size: 3230.390272 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - Small LM A model size: 3230.390272 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - Small LM A model size: 3230.390272 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - Small LM A model size: 3230.390272 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - Small LM A model size: 3230.390272 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - Small LM A model size: 3230.390272 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - Small LM A model size: 3230.390272 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - Small LM A model size: 3230.390272 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - Small LM A model size: 3230.390272 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - Small LM A model size: 3230.390272 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - Small LM A model size: 3230.390272 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - mapping a layer size: 23.079936 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - mapping a layer size: 23.079936 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - mapping a layer size: 23.079936 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - mapping a layer size: 23.079936 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - mapping a layer size: 23.079936 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - mapping a layer size: 23.079936 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - mapping a layer size: 23.079936 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - mapping a layer size: 23.079936 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - mapping a layer size: 23.079936 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - mapping a layer size: 23.079936 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - mapping a layer size: 23.079936 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - mapping a layer size: 23.079936 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - mapping a layer size: 23.079936 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - mapping a layer size: 23.079936 M
Using custom data configuration default-5110b371feb00e53
11/14/2024 01:29:55 - INFO - datasets.builder - Using custom data configuration default-5110b371feb00e53
Loading Dataset Infos from /root/.local/lib/python3.9/site-packages/datasets/packaged_modules/json
11/14/2024 01:29:55 - INFO - datasets.info - Loading Dataset Infos from /root/.local/lib/python3.9/site-packages/datasets/packaged_modules/json
Generating train split: 0 examples [00:00, ? examples/s]11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - mapping a layer size: 23.079936 M
11/14/2024 01:29:55 - INFO - src.model_utils.modeling_itb - mapping a layer size: 23.079936 M
Generating train split: 395000 examples [00:09, 39902.49 examples/s]Generating train split: 395000 examples [00:09, 39900.59 examples/s]
Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5110b371feb00e53/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
11/14/2024 01:30:05 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5110b371feb00e53/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5110b371feb00e53/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
11/14/2024 01:30:05 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5110b371feb00e53/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
Map:   0%|          | 0/395000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-5110b371feb00e53/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-c93d03b502b72c78.arrow
11/14/2024 01:30:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-5110b371feb00e53/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-c93d03b502b72c78.arrow
Map:   0%|          | 1000/395000 [00:00<00:57, 6823.82 examples/s]Map:   1%|          | 2000/395000 [00:00<00:51, 7697.71 examples/s]Map:   1%|          | 3000/395000 [00:00<00:48, 8086.30 examples/s]Map:   1%|          | 4000/395000 [00:00<00:46, 8330.69 examples/s]Map:   1%|▏         | 5000/395000 [00:00<00:45, 8543.94 examples/s]Map:   2%|▏         | 6000/395000 [00:00<00:44, 8723.49 examples/s]Map:   2%|▏         | 7000/395000 [00:00<00:44, 8665.15 examples/s]Map:   2%|▏         | 8000/395000 [00:00<00:43, 8797.19 examples/s]Map:   2%|▏         | 9000/395000 [00:01<00:43, 8809.46 examples/s]Map:   3%|▎         | 10000/395000 [00:01<00:43, 8842.22 examples/s]Map:   3%|▎         | 11000/395000 [00:01<00:43, 8828.18 examples/s]Map:   3%|▎         | 12000/395000 [00:01<00:43, 8889.90 examples/s]Map:   3%|▎         | 13000/395000 [00:01<00:42, 8975.98 examples/s]Map:   4%|▎         | 14000/395000 [00:01<00:42, 9036.03 examples/s]Map:   4%|▍         | 15000/395000 [00:01<00:42, 9019.01 examples/s]Map:   4%|▍         | 16000/395000 [00:01<00:42, 8975.07 examples/s]Map:   4%|▍         | 17000/395000 [00:01<00:41, 9043.03 examples/s]Map:   5%|▍         | 18000/395000 [00:02<00:41, 9074.05 examples/s]Map:   5%|▍         | 19000/395000 [00:02<00:41, 9104.61 examples/s]Map:   5%|▌         | 20000/395000 [00:02<00:40, 9165.39 examples/s]Map:   5%|▌         | 21000/395000 [00:02<00:40, 9194.35 examples/s]Map:   6%|▌         | 22000/395000 [00:02<01:00, 6170.38 examples/s]Map:   6%|▌         | 23000/395000 [00:02<00:54, 6787.81 examples/s]Map:   6%|▌         | 24000/395000 [00:02<00:50, 7388.09 examples/s]Map:   6%|▋         | 25000/395000 [00:02<00:47, 7824.32 examples/s]Map:   7%|▋         | 26000/395000 [00:03<00:45, 8182.02 examples/s]Map:   7%|▋         | 27000/395000 [00:03<00:43, 8523.04 examples/s]Map:   7%|▋         | 28000/395000 [00:03<00:41, 8763.14 examples/s]Map:   7%|▋         | 29000/395000 [00:03<00:40, 8969.59 examples/s]Map:   8%|▊         | 30000/395000 [00:03<00:40, 9074.92 examples/s]Map:   8%|▊         | 31000/395000 [00:03<00:40, 9033.61 examples/s]Map:   8%|▊         | 32000/395000 [00:03<00:39, 9094.65 examples/s]Map:   8%|▊         | 33000/395000 [00:03<00:39, 9200.35 examples/s]Map:   9%|▊         | 34000/395000 [00:03<00:39, 9223.20 examples/s]Map:   9%|▉         | 35000/395000 [00:04<00:38, 9302.87 examples/s]Map:   9%|▉         | 36000/395000 [00:04<00:38, 9327.86 examples/s]Map:   9%|▉         | 37000/395000 [00:04<00:38, 9311.95 examples/s]Map:  10%|▉         | 38000/395000 [00:04<00:38, 9368.56 examples/s]Map:  10%|▉         | 39000/395000 [00:04<00:38, 9333.41 examples/s]Map:  10%|█         | 40000/395000 [00:04<00:38, 9329.53 examples/s]Map:  10%|█         | 41000/395000 [00:04<00:37, 9320.46 examples/s]Map:  11%|█         | 42000/395000 [00:04<00:56, 6278.46 examples/s]Map:  11%|█         | 43000/395000 [00:05<00:50, 6972.07 examples/s]Map:  11%|█         | 44000/395000 [00:05<00:46, 7599.77 examples/s]Map:  11%|█▏        | 45000/395000 [00:05<00:43, 8089.64 examples/s]Map:  12%|█▏        | 46000/395000 [00:05<00:43, 8078.49 examples/s]Map:  12%|█▏        | 47000/395000 [00:05<00:42, 8271.66 examples/s]Map:  12%|█▏        | 48000/395000 [00:05<00:41, 8307.98 examples/s]Map:  12%|█▏        | 49000/395000 [00:05<00:40, 8589.96 examples/s]Map:  13%|█▎        | 50000/395000 [00:05<00:39, 8821.68 examples/s]Map:  13%|█▎        | 51000/395000 [00:05<00:38, 8963.33 examples/s]Map:  13%|█▎        | 52000/395000 [00:06<00:37, 9116.24 examples/s]Map:  13%|█▎        | 53000/395000 [00:06<00:36, 9246.64 examples/s]Map:  14%|█▎        | 54000/395000 [00:06<00:36, 9333.53 examples/s]Map:  14%|█▍        | 55000/395000 [00:06<00:36, 9303.15 examples/s]Map:  14%|█▍        | 56000/395000 [00:06<00:36, 9345.08 examples/s]Map:  14%|█▍        | 57000/395000 [00:06<00:36, 9254.23 examples/s]Map:  15%|█▍        | 58000/395000 [00:06<00:36, 9298.28 examples/s]Map:  15%|█▍        | 59000/395000 [00:06<00:36, 9301.03 examples/s]Map:  15%|█▌        | 60000/395000 [00:06<00:35, 9343.40 examples/s]Map:  15%|█▌        | 61000/395000 [00:07<00:35, 9388.95 examples/s]Map:  16%|█▌        | 62000/395000 [00:07<00:53, 6279.13 examples/s]Map:  16%|█▌        | 63000/395000 [00:07<00:47, 6980.55 examples/s]Map:  16%|█▌        | 64000/395000 [00:07<00:43, 7543.74 examples/s]Map:  16%|█▋        | 65000/395000 [00:07<00:41, 7977.38 examples/s]Map:  17%|█▋        | 66000/395000 [00:07<00:39, 8323.21 examples/s]Map:  17%|█▋        | 67000/395000 [00:07<00:38, 8453.90 examples/s]Map:  17%|█▋        | 68000/395000 [00:07<00:37, 8656.45 examples/s]Map:  17%|█▋        | 69000/395000 [00:08<00:36, 8895.58 examples/s]Map:  18%|█▊        | 70000/395000 [00:08<00:35, 9121.19 examples/s]Map:  18%|█▊        | 71000/395000 [00:08<00:35, 9245.01 examples/s]Map:  18%|█▊        | 72000/395000 [00:08<00:34, 9357.89 examples/s]Map:  18%|█▊        | 73000/395000 [00:08<00:34, 9324.45 examples/s]Map:  19%|█▊        | 74000/395000 [00:08<00:33, 9444.82 examples/s]Map:  19%|█▉        | 75000/395000 [00:08<00:34, 9376.33 examples/s]Map:  19%|█▉        | 76000/395000 [00:08<00:33, 9433.05 examples/s]Map:  19%|█▉        | 77000/395000 [00:08<00:33, 9492.87 examples/s]Map:  20%|█▉        | 78000/395000 [00:09<00:33, 9536.03 examples/s]Map:  20%|██        | 79000/395000 [00:09<00:33, 9526.15 examples/s]Map:  20%|██        | 80000/395000 [00:09<00:33, 9535.78 examples/s]Map:  21%|██        | 81000/395000 [00:09<00:53, 5817.10 examples/s]Map:  21%|██        | 82000/395000 [00:09<00:47, 6590.42 examples/s]Map:  21%|██        | 83000/395000 [00:09<00:42, 7270.10 examples/s]Map:  21%|██▏       | 84000/395000 [00:09<00:39, 7834.86 examples/s]Map:  22%|██▏       | 85000/395000 [00:09<00:37, 8273.61 examples/s]Map:  22%|██▏       | 86000/395000 [00:10<00:36, 8526.12 examples/s]Map:  22%|██▏       | 87000/395000 [00:10<00:35, 8779.42 examples/s]Map:  22%|██▏       | 88000/395000 [00:10<00:34, 8810.53 examples/s]Map:  23%|██▎       | 89000/395000 [00:10<00:34, 8999.91 examples/s]Map:  23%|██▎       | 90000/395000 [00:10<00:33, 9159.11 examples/s]Map:  23%|██▎       | 91000/395000 [00:10<00:32, 9269.87 examples/s]Map:  23%|██▎       | 92000/395000 [00:10<00:32, 9338.11 examples/s]Map:  24%|██▎       | 93000/395000 [00:10<00:31, 9453.89 examples/s]Map:  24%|██▍       | 94000/395000 [00:10<00:31, 9441.73 examples/s]Map:  24%|██▍       | 95000/395000 [00:11<00:31, 9476.16 examples/s]Map:  24%|██▍       | 96000/395000 [00:11<00:31, 9453.21 examples/s]Map:  25%|██▍       | 97000/395000 [00:11<00:32, 9291.67 examples/s]Map:  25%|██▍       | 98000/395000 [00:11<00:32, 9128.08 examples/s]Map:  25%|██▌       | 99000/395000 [00:11<00:32, 9018.00 examples/s]Map:  25%|██▌       | 100000/395000 [00:11<00:32, 9110.20 examples/s]Map:  26%|██▌       | 101000/395000 [00:11<00:47, 6155.52 examples/s]Map:  26%|██▌       | 102000/395000 [00:12<00:43, 6798.95 examples/s]Map:  26%|██▌       | 103000/395000 [00:12<00:39, 7404.36 examples/s]Map:  26%|██▋       | 104000/395000 [00:12<00:37, 7847.02 examples/s]Map:  27%|██▋       | 106000/395000 [00:12<00:34, 8440.12 examples/s]Map:  27%|██▋       | 107000/395000 [00:12<00:33, 8534.06 examples/s]Map:  27%|██▋       | 108000/395000 [00:12<00:32, 8752.72 examples/s]Map:  28%|██▊       | 109000/395000 [00:12<00:31, 8972.99 examples/s]Map:  28%|██▊       | 110000/395000 [00:12<00:31, 9172.37 examples/s]Map:  28%|██▊       | 111000/395000 [00:12<00:30, 9298.99 examples/s]Map:  28%|██▊       | 112000/395000 [00:13<00:30, 9397.15 examples/s]Map:  29%|██▊       | 113000/395000 [00:13<00:29, 9469.99 examples/s]Map:  29%|██▉       | 114000/395000 [00:13<00:30, 9343.33 examples/s]Map:  29%|██▉       | 115000/395000 [00:13<00:29, 9368.53 examples/s]Map:  29%|██▉       | 116000/395000 [00:13<00:29, 9397.16 examples/s]Map:  30%|██▉       | 117000/395000 [00:13<00:29, 9522.98 examples/s]Map:  30%|██▉       | 118000/395000 [00:13<00:28, 9586.28 examples/s]Map:  30%|███       | 119000/395000 [00:13<00:28, 9597.88 examples/s]Map:  30%|███       | 120000/395000 [00:14<00:41, 6614.55 examples/s]Map:  31%|███       | 121000/395000 [00:14<00:37, 7262.70 examples/s]Map:  31%|███       | 122000/395000 [00:14<00:35, 7777.20 examples/s]Map:  31%|███       | 123000/395000 [00:14<00:32, 8242.74 examples/s]Map:  31%|███▏      | 124000/395000 [00:14<00:31, 8640.89 examples/s]Map:  32%|███▏      | 125000/395000 [00:14<00:30, 8925.86 examples/s]Map:  32%|███▏      | 126000/395000 [00:14<00:29, 9187.54 examples/s]Map:  32%|███▏      | 127000/395000 [00:14<00:28, 9313.16 examples/s]Map:  32%|███▏      | 128000/395000 [00:14<00:28, 9428.13 examples/s]Map:  33%|███▎      | 129000/395000 [00:14<00:27, 9500.27 examples/s]Map:  33%|███▎      | 130000/395000 [00:15<00:27, 9532.38 examples/s]Map:  33%|███▎      | 131000/395000 [00:15<00:27, 9606.09 examples/s]Map:  33%|███▎      | 132000/395000 [00:15<00:27, 9650.11 examples/s]Map:  34%|███▎      | 133000/395000 [00:15<00:27, 9609.29 examples/s]Map:  34%|███▍      | 134000/395000 [00:15<00:27, 9574.47 examples/s]Map:  34%|███▍      | 135000/395000 [00:15<00:26, 9637.89 examples/s]Map:  34%|███▍      | 136000/395000 [00:15<00:26, 9658.65 examples/s]Map:  35%|███▍      | 137000/395000 [00:15<00:26, 9620.86 examples/s]Map:  35%|███▍      | 138000/395000 [00:15<00:26, 9720.20 examples/s]Map:  35%|███▌      | 139000/395000 [00:16<00:26, 9646.55 examples/s]Map:  35%|███▌      | 140000/395000 [00:16<00:35, 7134.11 examples/s]Map:  36%|███▌      | 141000/395000 [00:16<00:32, 7789.62 examples/s]Map:  36%|███▌      | 142000/395000 [00:16<00:30, 8286.54 examples/s]Map:  36%|███▌      | 143000/395000 [00:16<00:29, 8676.72 examples/s]Map:  36%|███▋      | 144000/395000 [00:16<00:27, 8969.94 examples/s]Map:  37%|███▋      | 145000/395000 [00:16<00:27, 9122.76 examples/s]Map:  37%|███▋      | 146000/395000 [00:16<00:26, 9293.98 examples/s]Map:  37%|███▋      | 147000/395000 [00:16<00:26, 9352.19 examples/s]Map:  37%|███▋      | 148000/395000 [00:17<00:26, 9461.92 examples/s]Map:  38%|███▊      | 149000/395000 [00:17<00:25, 9511.66 examples/s]Map:  38%|███▊      | 150000/395000 [00:17<00:25, 9522.83 examples/s]Map:  38%|███▊      | 151000/395000 [00:17<00:26, 9359.62 examples/s]Map:  38%|███▊      | 152000/395000 [00:17<00:25, 9506.66 examples/s]Map:  39%|███▊      | 153000/395000 [00:17<00:25, 9531.62 examples/s]Map:  39%|███▉      | 154000/395000 [00:17<00:25, 9556.42 examples/s]Map:  39%|███▉      | 155000/395000 [00:17<00:24, 9608.76 examples/s]Map:  39%|███▉      | 156000/395000 [00:17<00:24, 9673.72 examples/s]Map:  40%|███▉      | 157000/395000 [00:18<00:24, 9559.63 examples/s]Map:  40%|████      | 158000/395000 [00:18<00:24, 9565.65 examples/s]Map:  40%|████      | 159000/395000 [00:18<00:33, 7055.55 examples/s]Map:  41%|████      | 160000/395000 [00:18<00:30, 7646.17 examples/s]Map:  41%|████      | 161000/395000 [00:18<00:28, 8121.07 examples/s]Map:  41%|████      | 162000/395000 [00:18<00:27, 8546.01 examples/s]Map:  41%|████▏     | 163000/395000 [00:18<00:26, 8779.62 examples/s]Map:  42%|████▏     | 164000/395000 [00:18<00:25, 9089.26 examples/s]Map:  42%|████▏     | 165000/395000 [00:18<00:24, 9215.90 examples/s]Map:  42%|████▏     | 166000/395000 [00:19<00:24, 9297.52 examples/s]Map:  42%|████▏     | 167000/395000 [00:19<00:24, 9376.34 examples/s]Map:  43%|████▎     | 168000/395000 [00:19<00:23, 9518.95 examples/s]Map:  43%|████▎     | 169000/395000 [00:19<00:23, 9558.59 examples/s]Map:  43%|████▎     | 170000/395000 [00:19<00:23, 9587.25 examples/s]Map:  43%|████▎     | 171000/395000 [00:19<00:23, 9511.43 examples/s]Map:  44%|████▎     | 172000/395000 [00:19<00:23, 9518.97 examples/s]Map:  44%|████▍     | 173000/395000 [00:19<00:23, 9585.90 examples/s]Map:  44%|████▍     | 174000/395000 [00:19<00:23, 9599.85 examples/s]Map:  44%|████▍     | 175000/395000 [00:20<00:23, 9556.56 examples/s]Map:  45%|████▍     | 176000/395000 [00:20<00:22, 9656.06 examples/s]Map:  45%|████▌     | 178000/395000 [00:20<00:22, 9650.44 examples/s]Map:  45%|████▌     | 179000/395000 [00:20<00:29, 7370.96 examples/s]Map:  46%|████▌     | 180000/395000 [00:20<00:27, 7780.17 examples/s]Map:  46%|████▌     | 181000/395000 [00:20<00:26, 8191.89 examples/s]Map:  46%|████▌     | 182000/395000 [00:20<00:24, 8541.73 examples/s]Map:  46%|████▋     | 183000/395000 [00:20<00:24, 8700.41 examples/s]Map:  47%|████▋     | 184000/395000 [00:21<00:23, 8872.69 examples/s]Map:  47%|████▋     | 185000/395000 [00:21<00:23, 8950.60 examples/s]Map:  47%|████▋     | 186000/395000 [00:21<00:22, 9106.92 examples/s]Map:  47%|████▋     | 187000/395000 [00:21<00:23, 9012.55 examples/s]Map:  48%|████▊     | 188000/395000 [00:21<00:22, 9171.19 examples/s]Map:  48%|████▊     | 189000/395000 [00:21<00:22, 9297.35 examples/s]Map:  48%|████▊     | 190000/395000 [00:21<00:22, 9180.77 examples/s]Map:  48%|████▊     | 191000/395000 [00:21<00:21, 9355.37 examples/s]Map:  49%|████▉     | 193000/395000 [00:22<00:21, 9586.19 examples/s]Map:  49%|████▉     | 194000/395000 [00:22<00:20, 9607.71 examples/s]Map:  49%|████▉     | 195000/395000 [00:22<00:20, 9675.73 examples/s]Map:  50%|████▉     | 196000/395000 [00:22<00:20, 9638.58 examples/s]Map:  50%|████▉     | 197000/395000 [00:22<00:20, 9504.59 examples/s]Map:  50%|█████     | 198000/395000 [00:22<00:29, 6728.55 examples/s]Map:  50%|█████     | 199000/395000 [00:22<00:26, 7430.01 examples/s]Map:  51%|█████     | 200000/395000 [00:22<00:24, 8002.45 examples/s]Map:  51%|█████     | 201000/395000 [00:23<00:22, 8494.20 examples/s]Map:  51%|█████     | 202000/395000 [00:23<00:21, 8850.58 examples/s]Map:  51%|█████▏    | 203000/395000 [00:23<00:21, 9106.89 examples/s]Map:  52%|█████▏    | 204000/395000 [00:23<00:20, 9274.02 examples/s]Map:  52%|█████▏    | 205000/395000 [00:23<00:20, 9422.49 examples/s]Map:  52%|█████▏    | 206000/395000 [00:23<00:19, 9537.58 examples/s]Map:  52%|█████▏    | 207000/395000 [00:23<00:19, 9591.16 examples/s]Map:  53%|█████▎    | 208000/395000 [00:23<00:19, 9548.38 examples/s]Map:  53%|█████▎    | 209000/395000 [00:23<00:19, 9673.08 examples/s]Map:  53%|█████▎    | 210000/395000 [00:23<00:19, 9673.89 examples/s]Map:  53%|█████▎    | 211000/395000 [00:24<00:18, 9696.25 examples/s]Map:  54%|█████▍    | 213000/395000 [00:24<00:18, 9798.70 examples/s]Map:  54%|█████▍    | 214000/395000 [00:24<00:18, 9836.33 examples/s]Map:  54%|█████▍    | 215000/395000 [00:24<00:18, 9846.34 examples/s]Map:  55%|█████▍    | 216000/395000 [00:24<00:18, 9779.49 examples/s]Map:  55%|█████▍    | 217000/395000 [00:24<00:18, 9687.60 examples/s]Map:  55%|█████▌    | 218000/395000 [00:24<00:24, 7296.43 examples/s]Map:  55%|█████▌    | 219000/395000 [00:24<00:22, 7878.39 examples/s]Map:  56%|█████▌    | 220000/395000 [00:25<00:20, 8336.21 examples/s]Map:  56%|█████▌    | 221000/395000 [00:25<00:19, 8744.45 examples/s]Map:  56%|█████▌    | 222000/395000 [00:25<00:19, 9045.07 examples/s]Map:  56%|█████▋    | 223000/395000 [00:25<00:18, 9273.47 examples/s]Map:  57%|█████▋    | 224000/395000 [00:25<00:18, 9458.97 examples/s]Map:  57%|█████▋    | 225000/395000 [00:25<00:17, 9555.78 examples/s]Map:  57%|█████▋    | 226000/395000 [00:25<00:17, 9664.94 examples/s]Map:  57%|█████▋    | 227000/395000 [00:25<00:17, 9708.99 examples/s]Map:  58%|█████▊    | 229000/395000 [00:26<00:16, 9801.85 examples/s]Map:  58%|█████▊    | 230000/395000 [00:26<00:16, 9831.14 examples/s]Map:  58%|█████▊    | 231000/395000 [00:26<00:16, 9839.28 examples/s]Map:  59%|█████▊    | 232000/395000 [00:26<00:16, 9828.30 examples/s]Map:  59%|█████▉    | 234000/395000 [00:26<00:16, 9843.83 examples/s]Map:  59%|█████▉    | 235000/395000 [00:26<00:16, 9779.15 examples/s]Map:  60%|█████▉    | 236000/395000 [00:26<00:16, 9719.92 examples/s]Map:  60%|██████    | 237000/395000 [00:26<00:21, 7364.05 examples/s]Map:  60%|██████    | 238000/395000 [00:27<00:19, 7886.82 examples/s]Map:  61%|██████    | 239000/395000 [00:27<00:18, 8318.48 examples/s]Map:  61%|██████    | 240000/395000 [00:27<00:17, 8639.63 examples/s]Map:  61%|██████    | 241000/395000 [00:27<00:17, 8932.35 examples/s]Map:  61%|██████▏   | 242000/395000 [00:27<00:16, 9085.30 examples/s]Map:  62%|██████▏   | 243000/395000 [00:27<00:16, 9295.59 examples/s]Map:  62%|██████▏   | 244000/395000 [00:27<00:16, 9428.95 examples/s]Map:  62%|██████▏   | 245000/395000 [00:27<00:15, 9545.45 examples/s]Map:  62%|██████▏   | 246000/395000 [00:27<00:15, 9600.90 examples/s]Map:  63%|██████▎   | 247000/395000 [00:27<00:15, 9630.45 examples/s]Map:  63%|██████▎   | 248000/395000 [00:28<00:15, 9648.62 examples/s]Map:  63%|██████▎   | 250000/395000 [00:28<00:14, 9768.28 examples/s]Map:  64%|██████▎   | 251000/395000 [00:28<00:14, 9788.00 examples/s]Map:  64%|██████▍   | 252000/395000 [00:28<00:14, 9818.05 examples/s]Map:  64%|██████▍   | 253000/395000 [00:28<00:14, 9855.27 examples/s]Map:  64%|██████▍   | 254000/395000 [00:28<00:14, 9836.48 examples/s]Map:  65%|██████▍   | 255000/395000 [00:28<00:14, 9658.28 examples/s]Map:  65%|██████▍   | 256000/395000 [00:28<00:14, 9643.31 examples/s]Map:  65%|██████▌   | 257000/395000 [00:29<00:19, 7198.51 examples/s]Map:  65%|██████▌   | 258000/395000 [00:29<00:17, 7814.60 examples/s]Map:  66%|██████▌   | 259000/395000 [00:29<00:16, 8308.53 examples/s]Map:  66%|██████▌   | 260000/395000 [00:29<00:15, 8703.85 examples/s]Map:  66%|██████▌   | 261000/395000 [00:29<00:14, 8993.90 examples/s]Map:  66%|██████▋   | 262000/395000 [00:29<00:14, 9221.49 examples/s]Map:  67%|██████▋   | 263000/395000 [00:29<00:14, 9395.93 examples/s]Map:  67%|██████▋   | 264000/395000 [00:29<00:13, 9538.65 examples/s]Map:  67%|██████▋   | 265000/395000 [00:29<00:13, 9568.95 examples/s]Map:  67%|██████▋   | 266000/395000 [00:30<00:13, 9590.86 examples/s]Map:  68%|██████▊   | 268000/395000 [00:30<00:13, 9705.65 examples/s]Map:  68%|██████▊   | 269000/395000 [00:30<00:12, 9740.69 examples/s]Map:  68%|██████▊   | 270000/395000 [00:30<00:12, 9688.82 examples/s]Map:  69%|██████▊   | 271000/395000 [00:30<00:12, 9741.25 examples/s]Map:  69%|██████▉   | 272000/395000 [00:30<00:12, 9745.48 examples/s]Map:  69%|██████▉   | 273000/395000 [00:30<00:12, 9796.09 examples/s]Map:  69%|██████▉   | 274000/395000 [00:30<00:12, 9820.60 examples/s]Map:  70%|██████▉   | 275000/395000 [00:30<00:12, 9783.83 examples/s]Map:  70%|██████▉   | 276000/395000 [00:31<00:16, 7225.00 examples/s]Map:  70%|███████   | 277000/395000 [00:31<00:15, 7796.03 examples/s]Map:  70%|███████   | 278000/395000 [00:31<00:14, 8245.06 examples/s]Map:  71%|███████   | 279000/395000 [00:31<00:13, 8624.95 examples/s]Map:  71%|███████   | 280000/395000 [00:31<00:12, 8981.36 examples/s]Map:  71%|███████   | 281000/395000 [00:31<00:12, 9222.08 examples/s]Map:  71%|███████▏  | 282000/395000 [00:31<00:12, 9391.24 examples/s]Map:  72%|███████▏  | 283000/395000 [00:31<00:11, 9468.76 examples/s]Map:  72%|███████▏  | 285000/395000 [00:32<00:11, 9697.84 examples/s]Map:  72%|███████▏  | 286000/395000 [00:32<00:11, 9769.14 examples/s]Map:  73%|███████▎  | 287000/395000 [00:32<00:11, 9760.02 examples/s]Map:  73%|███████▎  | 288000/395000 [00:32<00:11, 9726.07 examples/s]Map:  73%|███████▎  | 289000/395000 [00:32<00:10, 9660.12 examples/s]Map:  73%|███████▎  | 290000/395000 [00:32<00:10, 9612.70 examples/s]Map:  74%|███████▎  | 291000/395000 [00:32<00:10, 9666.59 examples/s]Map:  74%|███████▍  | 292000/395000 [00:32<00:10, 9630.54 examples/s]Map:  74%|███████▍  | 293000/395000 [00:32<00:10, 9712.34 examples/s]Map:  74%|███████▍  | 294000/395000 [00:33<00:10, 9786.80 examples/s]Map:  75%|███████▍  | 295000/395000 [00:33<00:10, 9741.83 examples/s]Map:  75%|███████▍  | 296000/395000 [00:33<00:14, 6941.48 examples/s]Map:  75%|███████▌  | 297000/395000 [00:33<00:12, 7577.87 examples/s]Map:  75%|███████▌  | 298000/395000 [00:33<00:12, 8029.02 examples/s]Map:  76%|███████▌  | 300000/395000 [00:33<00:10, 8812.48 examples/s]Map:  76%|███████▌  | 301000/395000 [00:33<00:10, 9056.04 examples/s]Map:  76%|███████▋  | 302000/395000 [00:34<00:10, 9269.71 examples/s]Map:  77%|███████▋  | 303000/395000 [00:34<00:09, 9449.39 examples/s]Map:  77%|███████▋  | 304000/395000 [00:34<00:09, 9581.56 examples/s]Map:  77%|███████▋  | 305000/395000 [00:34<00:09, 9647.72 examples/s]Map:  78%|███████▊  | 307000/395000 [00:34<00:09, 9772.68 examples/s]Map:  78%|███████▊  | 308000/395000 [00:34<00:08, 9769.20 examples/s]Map:  78%|███████▊  | 309000/395000 [00:34<00:08, 9776.39 examples/s]Map:  78%|███████▊  | 310000/395000 [00:34<00:08, 9803.28 examples/s]Map:  79%|███████▊  | 311000/395000 [00:34<00:08, 9852.90 examples/s]Map:  79%|███████▉  | 312000/395000 [00:35<00:08, 9824.51 examples/s]Map:  79%|███████▉  | 313000/395000 [00:35<00:08, 9800.09 examples/s]Map:  79%|███████▉  | 314000/395000 [00:35<00:08, 9808.21 examples/s]Map:  80%|███████▉  | 315000/395000 [00:35<00:11, 7257.36 examples/s]Map:  80%|████████  | 316000/395000 [00:35<00:10, 7883.02 examples/s]Map:  80%|████████  | 317000/395000 [00:35<00:09, 8359.79 examples/s]Map:  81%|████████  | 319000/395000 [00:35<00:08, 8985.68 examples/s]Map:  81%|████████  | 320000/395000 [00:35<00:08, 9212.07 examples/s]Map:  81%|████████▏ | 321000/395000 [00:36<00:07, 9346.51 examples/s]Map:  82%|████████▏ | 323000/395000 [00:36<00:07, 9619.47 examples/s]Map:  82%|████████▏ | 325000/395000 [00:36<00:07, 9790.14 examples/s]Map:  83%|████████▎ | 326000/395000 [00:36<00:07, 9783.23 examples/s]Map:  83%|████████▎ | 327000/395000 [00:36<00:06, 9772.23 examples/s]Map:  83%|████████▎ | 328000/395000 [00:36<00:06, 9738.61 examples/s]Map:  83%|████████▎ | 329000/395000 [00:36<00:06, 9741.66 examples/s]Map:  84%|████████▍ | 331000/395000 [00:37<00:06, 9815.34 examples/s]Map:  84%|████████▍ | 332000/395000 [00:37<00:06, 9782.81 examples/s]Map:  84%|████████▍ | 333000/395000 [00:37<00:06, 9787.81 examples/s]Map:  85%|████████▍ | 334000/395000 [00:37<00:06, 9814.90 examples/s]Map:  85%|████████▍ | 335000/395000 [00:37<00:08, 7320.01 examples/s]Map:  85%|████████▌ | 336000/395000 [00:37<00:07, 7866.88 examples/s]Map:  85%|████████▌ | 337000/395000 [00:37<00:06, 8332.89 examples/s]Map:  86%|████████▌ | 339000/395000 [00:37<00:06, 9053.26 examples/s]Map:  86%|████████▌ | 340000/395000 [00:38<00:05, 9167.65 examples/s]Map:  87%|████████▋ | 342000/395000 [00:38<00:05, 9469.04 examples/s]Map:  87%|████████▋ | 343000/395000 [00:38<00:05, 9550.81 examples/s]Map:  87%|████████▋ | 344000/395000 [00:38<00:05, 9587.47 examples/s]Map:  87%|████████▋ | 345000/395000 [00:38<00:05, 9644.86 examples/s]Map:  88%|████████▊ | 346000/395000 [00:38<00:05, 9693.20 examples/s]Map:  88%|████████▊ | 347000/395000 [00:38<00:04, 9681.84 examples/s]Map:  88%|████████▊ | 348000/395000 [00:38<00:04, 9742.71 examples/s]Map:  88%|████████▊ | 349000/395000 [00:39<00:04, 9700.65 examples/s]Map:  89%|████████▊ | 350000/395000 [00:39<00:04, 9779.67 examples/s]Map:  89%|████████▉ | 351000/395000 [00:39<00:04, 9801.63 examples/s]Map:  89%|████████▉ | 352000/395000 [00:39<00:04, 9803.96 examples/s]Map:  90%|████████▉ | 354000/395000 [00:39<00:05, 7704.38 examples/s]Map:  90%|████████▉ | 355000/395000 [00:39<00:04, 8122.56 examples/s]Map:  90%|█████████ | 356000/395000 [00:39<00:04, 8481.60 examples/s]Map:  90%|█████████ | 357000/395000 [00:39<00:04, 8810.98 examples/s]Map:  91%|█████████ | 358000/395000 [00:40<00:04, 9013.19 examples/s]Map:  91%|█████████ | 360000/395000 [00:40<00:03, 9233.05 examples/s]Map:  91%|█████████▏| 361000/395000 [00:40<00:03, 9338.72 examples/s]Map:  92%|█████████▏| 362000/395000 [00:40<00:03, 9203.54 examples/s]Map:  92%|█████████▏| 363000/395000 [00:40<00:03, 9387.73 examples/s]Map:  92%|█████████▏| 364000/395000 [00:40<00:03, 9530.56 examples/s]Map:  93%|█████████▎| 366000/395000 [00:40<00:02, 9781.03 examples/s]Map:  93%|█████████▎| 367000/395000 [00:41<00:02, 9393.74 examples/s]Map:  93%|█████████▎| 369000/395000 [00:41<00:02, 9576.09 examples/s]Map:  94%|█████████▎| 370000/395000 [00:41<00:02, 9603.19 examples/s]Map:  94%|█████████▍| 371000/395000 [00:41<00:02, 9280.33 examples/s]Map:  94%|█████████▍| 372000/395000 [00:41<00:02, 9323.52 examples/s]Map:  94%|█████████▍| 373000/395000 [00:41<00:02, 9461.48 examples/s]Map:  95%|█████████▍| 374000/395000 [00:41<00:02, 7199.83 examples/s]Map:  95%|█████████▍| 375000/395000 [00:41<00:02, 7590.77 examples/s]Map:  95%|█████████▌| 376000/395000 [00:42<00:02, 8093.15 examples/s]Map:  95%|█████████▌| 377000/395000 [00:42<00:02, 8496.43 examples/s]Map:  96%|█████████▌| 378000/395000 [00:42<00:01, 8879.46 examples/s]Map:  96%|█████████▌| 379000/395000 [00:42<00:01, 8765.83 examples/s]Map:  96%|█████████▌| 380000/395000 [00:42<00:01, 8493.67 examples/s]Map:  96%|█████████▋| 381000/395000 [00:42<00:01, 8885.06 examples/s]Map:  97%|█████████▋| 382000/395000 [00:42<00:01, 8756.16 examples/s]Map:  97%|█████████▋| 383000/395000 [00:42<00:01, 9089.99 examples/s]Map:  97%|█████████▋| 384000/395000 [00:42<00:01, 9322.16 examples/s]Map:  97%|█████████▋| 385000/395000 [00:43<00:01, 9384.08 examples/s]Map:  98%|█████████▊| 386000/395000 [00:43<00:00, 9431.36 examples/s]Map:  98%|█████████▊| 387000/395000 [00:43<00:00, 9529.54 examples/s]Map:  98%|█████████▊| 388000/395000 [00:43<00:00, 9567.76 examples/s]Map:  98%|█████████▊| 389000/395000 [00:43<00:00, 9626.49 examples/s]Map:  99%|█████████▊| 390000/395000 [00:43<00:00, 9710.96 examples/s]Map:  99%|█████████▉| 392000/395000 [00:43<00:00, 9792.84 examples/s]Map:  99%|█████████▉| 393000/395000 [00:43<00:00, 7608.21 examples/s]Map: 100%|█████████▉| 394000/395000 [00:44<00:00, 8053.30 examples/s]Map: 100%|██████████| 395000/395000 [00:44<00:00, 8481.03 examples/s]Map: 100%|██████████| 395000/395000 [00:44<00:00, 8935.46 examples/s]
11/14/2024 01:30:52 - INFO - __main__ - Dataset({
    features: ['query', 'response', 'type', 'original_question', 'prompt', 'prefix'],
    num_rows: 395000
})
11/14/2024 01:30:52 - INFO - __main__ - ImplicitTransBridge(
  (llm): Qwen2ForCausalLM(
    (model): Qwen2Model(
      (embed_tokens): Embedding(152064, 3584)
      (layers): ModuleList(
        (0-27): 28 x Qwen2DecoderLayer(
          (self_attn): Qwen2SdpaAttention(
            (q_proj): Linear(in_features=3584, out_features=3584, bias=True)
            (k_proj): Linear(in_features=3584, out_features=512, bias=True)
            (v_proj): Linear(in_features=3584, out_features=512, bias=True)
            (o_proj): Linear(in_features=3584, out_features=3584, bias=False)
            (rotary_emb): Qwen2RotaryEmbedding()
          )
          (mlp): Qwen2MLP(
            (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)
            (up_proj): Linear(in_features=3584, out_features=18944, bias=False)
            (down_proj): Linear(in_features=18944, out_features=3584, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
          (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)
        )
      )
      (norm): Qwen2RMSNorm((3584,), eps=1e-06)
    )
    (lm_head): Linear(in_features=3584, out_features=152064, bias=False)
  )
  (llm_embedding_layer): Embedding(152064, 3584)
  (mt_model): MT5Model(
    (shared): Embedding(250112, 2048)
    (encoder): MT5Stack(
      (embed_tokens): Embedding(250112, 2048)
      (block): ModuleList(
        (0): MT5Block(
          (layer): ModuleList(
            (0): MT5LayerSelfAttention(
              (SelfAttention): MT5Attention(
                (q): Linear(in_features=2048, out_features=2048, bias=False)
                (k): Linear(in_features=2048, out_features=2048, bias=False)
                (v): Linear(in_features=2048, out_features=2048, bias=False)
                (o): Linear(in_features=2048, out_features=2048, bias=False)
                (relative_attention_bias): Embedding(32, 32)
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): MT5LayerFF(
              (DenseReluDense): MT5DenseGatedActDense(
                (wi_0): Linear(in_features=2048, out_features=5120, bias=False)
                (wi_1): Linear(in_features=2048, out_features=5120, bias=False)
                (wo): Linear(in_features=5120, out_features=2048, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (act): NewGELUActivation()
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1-23): 23 x MT5Block(
          (layer): ModuleList(
            (0): MT5LayerSelfAttention(
              (SelfAttention): MT5Attention(
                (q): Linear(in_features=2048, out_features=2048, bias=False)
                (k): Linear(in_features=2048, out_features=2048, bias=False)
                (v): Linear(in_features=2048, out_features=2048, bias=False)
                (o): Linear(in_features=2048, out_features=2048, bias=False)
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): MT5LayerFF(
              (DenseReluDense): MT5DenseGatedActDense(
                (wi_0): Linear(in_features=2048, out_features=5120, bias=False)
                (wi_1): Linear(in_features=2048, out_features=5120, bias=False)
                (wo): Linear(in_features=5120, out_features=2048, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (act): NewGELUActivation()
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (final_layer_norm): MT5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (decoder): MT5Stack(
      (embed_tokens): Embedding(250112, 2048)
      (block): ModuleList(
        (0): MT5Block(
          (layer): ModuleList(
            (0): MT5LayerSelfAttention(
              (SelfAttention): MT5Attention(
                (q): Linear(in_features=2048, out_features=2048, bias=False)
                (k): Linear(in_features=2048, out_features=2048, bias=False)
                (v): Linear(in_features=2048, out_features=2048, bias=False)
                (o): Linear(in_features=2048, out_features=2048, bias=False)
                (relative_attention_bias): Embedding(32, 32)
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): MT5LayerCrossAttention(
              (EncDecAttention): MT5Attention(
                (q): Linear(in_features=2048, out_features=2048, bias=False)
                (k): Linear(in_features=2048, out_features=2048, bias=False)
                (v): Linear(in_features=2048, out_features=2048, bias=False)
                (o): Linear(in_features=2048, out_features=2048, bias=False)
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (2): MT5LayerFF(
              (DenseReluDense): MT5DenseGatedActDense(
                (wi_0): Linear(in_features=2048, out_features=5120, bias=False)
                (wi_1): Linear(in_features=2048, out_features=5120, bias=False)
                (wo): Linear(in_features=5120, out_features=2048, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (act): NewGELUActivation()
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1-23): 23 x MT5Block(
          (layer): ModuleList(
            (0): MT5LayerSelfAttention(
              (SelfAttention): MT5Attention(
                (q): Linear(in_features=2048, out_features=2048, bias=False)
                (k): Linear(in_features=2048, out_features=2048, bias=False)
                (v): Linear(in_features=2048, out_features=2048, bias=False)
                (o): Linear(in_features=2048, out_features=2048, bias=False)
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (1): MT5LayerCrossAttention(
              (EncDecAttention): MT5Attention(
                (q): Linear(in_features=2048, out_features=2048, bias=False)
                (k): Linear(in_features=2048, out_features=2048, bias=False)
                (v): Linear(in_features=2048, out_features=2048, bias=False)
                (o): Linear(in_features=2048, out_features=2048, bias=False)
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (2): MT5LayerFF(
              (DenseReluDense): MT5DenseGatedActDense(
                (wi_0): Linear(in_features=2048, out_features=5120, bias=False)
                (wi_1): Linear(in_features=2048, out_features=5120, bias=False)
                (wo): Linear(in_features=5120, out_features=2048, bias=False)
                (dropout): Dropout(p=0.1, inplace=False)
                (act): NewGELUActivation()
              )
              (layer_norm): MT5LayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (final_layer_norm): MT5LayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (slm_a): MT5Stack(
    (embed_tokens): Embedding(250112, 2048)
    (block): ModuleList(
      (0): MT5Block(
        (layer): ModuleList(
          (0): MT5LayerSelfAttention(
            (SelfAttention): MT5Attention(
              (q): Linear(in_features=2048, out_features=2048, bias=False)
              (k): Linear(in_features=2048, out_features=2048, bias=False)
              (v): Linear(in_features=2048, out_features=2048, bias=False)
              (o): Linear(in_features=2048, out_features=2048, bias=False)
              (relative_attention_bias): Embedding(32, 32)
            )
            (layer_norm): MT5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): MT5LayerFF(
            (DenseReluDense): MT5DenseGatedActDense(
              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)
              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)
              (wo): Linear(in_features=5120, out_features=2048, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): NewGELUActivation()
            )
            (layer_norm): MT5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1-23): 23 x MT5Block(
        (layer): ModuleList(
          (0): MT5LayerSelfAttention(
            (SelfAttention): MT5Attention(
              (q): Linear(in_features=2048, out_features=2048, bias=False)
              (k): Linear(in_features=2048, out_features=2048, bias=False)
              (v): Linear(in_features=2048, out_features=2048, bias=False)
              (o): Linear(in_features=2048, out_features=2048, bias=False)
            )
            (layer_norm): MT5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): MT5LayerFF(
            (DenseReluDense): MT5DenseGatedActDense(
              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)
              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)
              (wo): Linear(in_features=5120, out_features=2048, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): NewGELUActivation()
            )
            (layer_norm): MT5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (final_layer_norm): MT5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (mapping_a): Mapping(
    (mlp): MLP(
      (linear1): Linear(in_features=2048, out_features=4096, bias=True)
      (linear2): Linear(in_features=4096, out_features=3584, bias=True)
      (relu): ReLU()
    )
  )
)
11/14/2024 01:30:52 - INFO - __main__ - trainable params: 23079936 || all params: 10869086720 || trainable%: 0.2123
11/14/2024 01:30:52 - WARNING - accelerate.utils.other - Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:648] 2024-11-14 01:30:52,923 >> Using auto half precision backend
[2024-11-14 01:30:53,343] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.4, git-hash=unknown, git-branch=unknown
[2024-11-14 01:31:43,333] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-14 01:31:43,336] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-14 01:31:43,336] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-14 01:31:43,339] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-11-14 01:31:43,339] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-11-14 01:31:43,339] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-14 01:31:43,339] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 500000000
[2024-11-14 01:31:43,339] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 500000000
[2024-11-14 01:31:43,339] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: False
[2024-11-14 01:31:43,339] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False
[WARNING|logging.py:313] 2024-11-14 01:31:43,593 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-14 01:31:43,594 >> You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-14 01:31:43,597 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-14 01:31:43,597 >> You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-14 01:31:43,597 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-14 01:31:43,598 >> You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-14 01:31:43,599 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-14 01:31:43,599 >> You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-14 01:31:43,604 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-14 01:31:43,604 >> You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-14 01:31:43,608 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-14 01:31:43,609 >> You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-14 01:31:43,620 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-14 01:31:43,621 >> You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[2024-11-14 01:31:43,730] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2024-11-14 01:31:43,731] [INFO] [utils.py:782:see_memory_usage] MA 20.35 GB         Max_MA 20.35 GB         CA 21.03 GB         Max_CA 21 GB 
[2024-11-14 01:31:43,731] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 116.16 GB, percent = 5.8%
[2024-11-14 01:31:43,858] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2024-11-14 01:31:43,858] [INFO] [utils.py:782:see_memory_usage] MA 20.35 GB         Max_MA 20.37 GB         CA 21.04 GB         Max_CA 21 GB 
[2024-11-14 01:31:43,858] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 116.16 GB, percent = 5.8%
[2024-11-14 01:31:43,858] [INFO] [stage_1_and_2.py:543:__init__] optimizer state initialized
[2024-11-14 01:31:43,996] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2024-11-14 01:31:43,996] [INFO] [utils.py:782:see_memory_usage] MA 20.35 GB         Max_MA 20.35 GB         CA 21.04 GB         Max_CA 21 GB 
[2024-11-14 01:31:43,996] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 116.24 GB, percent = 5.8%
[2024-11-14 01:31:43,997] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2024-11-14 01:31:43,997] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-14 01:31:43,997] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-14 01:31:43,997] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0006, 0.0006], mom=[(0.9, 0.999), (0.9, 0.999)]
[2024-11-14 01:31:43,999] [INFO] [config.py:997:print] DeepSpeedEngine configuration:
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   amp_enabled .................. False
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   amp_params ................... False
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   bfloat16_enabled ............. True
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   bfloat16_immediate_grad_update  False
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   checkpoint_parallel_write_pipeline  False
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   checkpoint_tag_validation_enabled  True
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   checkpoint_tag_validation_fail  False
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f153c36d100>
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   communication_data_type ...... None
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   curriculum_enabled_legacy .... False
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   curriculum_params_legacy ..... False
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   data_efficiency_enabled ...... False
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   dataloader_drop_last ......... False
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   disable_allgather ............ False
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   dump_state ................... False
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   dynamic_loss_scale_args ...... None
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   eigenvalue_enabled ........... False
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   eigenvalue_layer_num ......... 0
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   eigenvalue_max_iter .......... 100
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   eigenvalue_stability ......... 1e-06
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   eigenvalue_tol ............... 0.01
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   eigenvalue_verbose ........... False
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   elasticity_enabled ........... False
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   fp16_auto_cast ............... None
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   fp16_enabled ................. False
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   fp16_master_weights_and_gradients  False
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   global_rank .................. 0
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   grad_accum_dtype ............. None
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   gradient_accumulation_steps .. 4
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   gradient_clipping ............ 1.0
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   gradient_predivide_factor .... 1.0
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   graph_harvesting ............. False
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   initial_dynamic_scale ........ 1
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   load_universal_checkpoint .... False
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   loss_scale ................... 1.0
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   memory_breakdown ............. False
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   mics_hierarchial_params_gather  False
[2024-11-14 01:31:44,000] [INFO] [config.py:1001:print]   mics_shard_size .............. -1
[2024-11-14 01:31:44,001] [INFO] [config.py:1001:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-14 01:31:44,001] [INFO] [config.py:1001:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-14 01:31:44,001] [INFO] [config.py:1001:print]   optimizer_legacy_fusion ...... False
[2024-11-14 01:31:44,001] [INFO] [config.py:1001:print]   optimizer_name ............... None
[2024-11-14 01:31:44,001] [INFO] [config.py:1001:print]   optimizer_params ............. None
[2024-11-14 01:31:44,001] [INFO] [config.py:1001:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-14 01:31:44,001] [INFO] [config.py:1001:print]   pld_enabled .................. False
[2024-11-14 01:31:44,001] [INFO] [config.py:1001:print]   pld_params ................... False
[2024-11-14 01:31:44,001] [INFO] [config.py:1001:print]   prescale_gradients ........... False
[2024-11-14 01:31:44,001] [INFO] [config.py:1001:print]   scheduler_name ............... None
[2024-11-14 01:31:44,001] [INFO] [config.py:1001:print]   scheduler_params ............. None
[2024-11-14 01:31:44,001] [INFO] [config.py:1001:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-14 01:31:44,001] [INFO] [config.py:1001:print]   sparse_attention ............. None
[2024-11-14 01:31:44,001] [INFO] [config.py:1001:print]   sparse_gradients_enabled ..... False
[2024-11-14 01:31:44,001] [INFO] [config.py:1001:print]   steps_per_print .............. inf
[2024-11-14 01:31:44,001] [INFO] [config.py:1001:print]   timers_config ................ enabled=True synchronized=True
[2024-11-14 01:31:44,001] [INFO] [config.py:1001:print]   train_batch_size ............. 128
[2024-11-14 01:31:44,001] [INFO] [config.py:1001:print]   train_micro_batch_size_per_gpu  4
[2024-11-14 01:31:44,001] [INFO] [config.py:1001:print]   use_data_before_expert_parallel_  False
[2024-11-14 01:31:44,001] [INFO] [config.py:1001:print]   use_node_local_storage ....... False
[2024-11-14 01:31:44,001] [INFO] [config.py:1001:print]   wall_clock_breakdown ......... False
[2024-11-14 01:31:44,001] [INFO] [config.py:1001:print]   weight_quantization_config ... None
[2024-11-14 01:31:44,001] [INFO] [config.py:1001:print]   world_size ................... 8
[2024-11-14 01:31:44,001] [INFO] [config.py:1001:print]   zero_allow_untested_optimizer  True
[2024-11-14 01:31:44,001] [INFO] [config.py:1001:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-14 01:31:44,001] [INFO] [config.py:1001:print]   zero_enabled ................. True
[2024-11-14 01:31:44,001] [INFO] [config.py:1001:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-14 01:31:44,001] [INFO] [config.py:1001:print]   zero_optimization_stage ...... 2
[2024-11-14 01:31:44,001] [INFO] [config.py:987:print_user_config]   json = {
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 4, 
    "gradient_accumulation_steps": 4, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }
}
[INFO|trainer.py:2134] 2024-11-14 01:31:44,001 >> ***** Running training *****
[INFO|trainer.py:2135] 2024-11-14 01:31:44,001 >>   Num examples = 395,000
[INFO|trainer.py:2136] 2024-11-14 01:31:44,001 >>   Num Epochs = 1
[INFO|trainer.py:2137] 2024-11-14 01:31:44,001 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2140] 2024-11-14 01:31:44,001 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2141] 2024-11-14 01:31:44,001 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2142] 2024-11-14 01:31:44,001 >>   Total optimization steps = 3,086
[INFO|trainer.py:2143] 2024-11-14 01:31:44,004 >>   Number of trainable parameters = 23,079,936
  0%|          | 0/3086 [00:00<?, ?it/s][WARNING|logging.py:313] 2024-11-14 01:31:44,023 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:313] 2024-11-14 01:31:44,026 >> You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|modeling_utils.py:1264] 2024-11-14 01:31:44,735 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed
[WARNING|modeling_utils.py:1264] 2024-11-14 01:31:44,735 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed
[WARNING|modeling_utils.py:1264] 2024-11-14 01:31:44,735 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed
[WARNING|modeling_utils.py:1264] 2024-11-14 01:31:44,735 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed
[WARNING|modeling_utils.py:1264] 2024-11-14 01:31:44,736 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed
[WARNING|modeling_utils.py:1264] 2024-11-14 01:31:44,736 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed
[WARNING|modeling_utils.py:1264] 2024-11-14 01:31:44,736 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed
[WARNING|modeling_utils.py:1264] 2024-11-14 01:31:44,736 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed
  0%|          | 1/3086 [00:02<2:00:36,  2.35s/it]  0%|          | 2/3086 [00:04<1:46:50,  2.08s/it]  0%|          | 3/3086 [00:06<1:45:01,  2.04s/it]  0%|          | 4/3086 [00:08<1:54:45,  2.23s/it]  0%|          | 5/3086 [00:10<1:54:24,  2.23s/it]  0%|          | 6/3086 [00:12<1:42:48,  2.00s/it]  0%|          | 7/3086 [00:14<1:48:56,  2.12s/it]  0%|          | 8/3086 [00:17<1:51:38,  2.18s/it]  0%|          | 9/3086 [00:18<1:43:39,  2.02s/it]  0%|          | 10/3086 [00:20<1:42:55,  2.01s/it]                                                   {'loss': 1.2211, 'grad_norm': 0.4843805432319641, 'learning_rate': 0.0005980557355800388, 'epoch': 0.0}
  0%|          | 10/3086 [00:20<1:42:55,  2.01s/it]  0%|          | 11/3086 [00:23<1:45:47,  2.06s/it]  0%|          | 12/3086 [00:25<1:46:37,  2.08s/it]  0%|          | 13/3086 [00:27<1:53:29,  2.22s/it]  0%|          | 14/3086 [00:30<2:02:35,  2.39s/it]  0%|          | 15/3086 [00:32<1:51:58,  2.19s/it]  1%|          | 16/3086 [00:34<1:50:25,  2.16s/it]  1%|          | 17/3086 [00:35<1:41:49,  1.99s/it]  1%|          | 18/3086 [00:38<1:48:54,  2.13s/it]  1%|          | 19/3086 [00:40<1:41:28,  1.99s/it]  1%|          | 20/3086 [00:42<1:43:59,  2.04s/it]                                                   {'loss': 1.1343, 'grad_norm': 0.3544251322746277, 'learning_rate': 0.0005961114711600777, 'epoch': 0.01}
  1%|          | 20/3086 [00:42<1:43:59,  2.04s/it]  1%|          | 21/3086 [00:43<1:39:33,  1.95s/it]  1%|          | 22/3086 [00:45<1:37:39,  1.91s/it]  1%|          | 23/3086 [00:47<1:39:17,  1.94s/it]  1%|          | 24/3086 [00:49<1:34:46,  1.86s/it]  1%|          | 25/3086 [00:51<1:31:51,  1.80s/it]  1%|          | 26/3086 [00:52<1:25:25,  1.68s/it]  1%|          | 27/3086 [00:54<1:28:12,  1.73s/it]  1%|          | 28/3086 [00:55<1:25:33,  1.68s/it]  1%|          | 29/3086 [00:57<1:29:48,  1.76s/it]  1%|          | 30/3086 [01:00<1:36:57,  1.90s/it]                                                   {'loss': 1.0322, 'grad_norm': 0.4496588110923767, 'learning_rate': 0.0005941672067401166, 'epoch': 0.01}
  1%|          | 30/3086 [01:00<1:36:57,  1.90s/it]  1%|          | 31/3086 [01:02<1:39:02,  1.95s/it]  1%|          | 32/3086 [01:03<1:35:05,  1.87s/it]  1%|          | 33/3086 [01:05<1:33:35,  1.84s/it]  1%|          | 34/3086 [01:07<1:32:15,  1.81s/it]  1%|          | 35/3086 [01:09<1:30:23,  1.78s/it]  1%|          | 36/3086 [01:11<1:39:57,  1.97s/it]  1%|          | 37/3086 [01:13<1:34:52,  1.87s/it]  1%|          | 38/3086 [01:15<1:40:09,  1.97s/it]  1%|▏         | 39/3086 [01:17<1:38:01,  1.93s/it]  1%|▏         | 40/3086 [01:18<1:35:29,  1.88s/it]                                                   {'loss': 0.8995, 'grad_norm': 0.2662450075149536, 'learning_rate': 0.0005922229423201555, 'epoch': 0.01}
  1%|▏         | 40/3086 [01:18<1:35:29,  1.88s/it]  1%|▏         | 41/3086 [01:21<1:50:56,  2.19s/it]  1%|▏         | 42/3086 [01:23<1:47:08,  2.11s/it]  1%|▏         | 43/3086 [01:25<1:44:58,  2.07s/it]  1%|▏         | 44/3086 [01:27<1:40:35,  1.98s/it]  1%|▏         | 45/3086 [01:30<1:51:31,  2.20s/it]  1%|▏         | 46/3086 [01:31<1:44:25,  2.06s/it]  2%|▏         | 47/3086 [01:33<1:37:16,  1.92s/it]  2%|▏         | 48/3086 [01:35<1:33:37,  1.85s/it]  2%|▏         | 49/3086 [01:36<1:32:35,  1.83s/it]  2%|▏         | 50/3086 [01:39<1:37:24,  1.92s/it]                                                   {'loss': 0.7765, 'grad_norm': 0.1431298404932022, 'learning_rate': 0.0005902786779001944, 'epoch': 0.02}
  2%|▏         | 50/3086 [01:39<1:37:24,  1.92s/it]  2%|▏         | 51/3086 [01:41<1:41:48,  2.01s/it]  2%|▏         | 52/3086 [01:43<1:47:37,  2.13s/it]  2%|▏         | 53/3086 [01:46<1:56:45,  2.31s/it]  2%|▏         | 54/3086 [01:48<1:53:53,  2.25s/it]  2%|▏         | 55/3086 [01:50<1:43:42,  2.05s/it]  2%|▏         | 56/3086 [01:53<2:06:25,  2.50s/it]  2%|▏         | 57/3086 [01:55<1:59:43,  2.37s/it]  2%|▏         | 58/3086 [01:57<1:52:30,  2.23s/it]  2%|▏         | 59/3086 [02:00<1:59:09,  2.36s/it]  2%|▏         | 60/3086 [02:02<1:57:35,  2.33s/it]                                                   {'loss': 0.7051, 'grad_norm': 0.06738586723804474, 'learning_rate': 0.0005883344134802333, 'epoch': 0.02}
  2%|▏         | 60/3086 [02:02<1:57:35,  2.33s/it]  2%|▏         | 61/3086 [02:04<1:51:24,  2.21s/it]  2%|▏         | 62/3086 [02:06<1:41:59,  2.02s/it]  2%|▏         | 63/3086 [02:08<1:50:44,  2.20s/it]  2%|▏         | 64/3086 [02:10<1:46:07,  2.11s/it]  2%|▏         | 65/3086 [02:12<1:41:18,  2.01s/it]  2%|▏         | 66/3086 [02:14<1:44:24,  2.07s/it]  2%|▏         | 67/3086 [02:16<1:38:07,  1.95s/it]  2%|▏         | 68/3086 [02:18<1:40:12,  1.99s/it]  2%|▏         | 69/3086 [02:20<1:45:28,  2.10s/it]  2%|▏         | 70/3086 [02:22<1:46:15,  2.11s/it]                                                   {'loss': 0.7142, 'grad_norm': 0.08817701786756516, 'learning_rate': 0.0005863901490602722, 'epoch': 0.02}
  2%|▏         | 70/3086 [02:22<1:46:15,  2.11s/it]  2%|▏         | 71/3086 [02:24<1:42:42,  2.04s/it]  2%|▏         | 72/3086 [02:26<1:39:30,  1.98s/it]  2%|▏         | 73/3086 [02:28<1:35:40,  1.91s/it]  2%|▏         | 74/3086 [02:30<1:39:14,  1.98s/it]  2%|▏         | 75/3086 [02:33<1:48:03,  2.15s/it]  2%|▏         | 76/3086 [02:34<1:43:14,  2.06s/it]  2%|▏         | 77/3086 [02:36<1:40:33,  2.01s/it]  3%|▎         | 78/3086 [02:38<1:36:43,  1.93s/it]  3%|▎         | 79/3086 [02:40<1:44:09,  2.08s/it]  3%|▎         | 80/3086 [02:43<1:45:58,  2.12s/it]                                                   {'loss': 0.6751, 'grad_norm': 0.10920436680316925, 'learning_rate': 0.000584445884640311, 'epoch': 0.03}
  3%|▎         | 80/3086 [02:43<1:45:58,  2.12s/it]  3%|▎         | 81/3086 [02:45<1:47:55,  2.15s/it]  3%|▎         | 82/3086 [02:47<1:41:06,  2.02s/it]  3%|▎         | 83/3086 [02:49<1:45:58,  2.12s/it]  3%|▎         | 84/3086 [02:51<1:40:46,  2.01s/it]  3%|▎         | 85/3086 [02:53<1:43:49,  2.08s/it]  3%|▎         | 86/3086 [02:55<1:37:35,  1.95s/it]  3%|▎         | 87/3086 [02:56<1:35:00,  1.90s/it]  3%|▎         | 88/3086 [02:58<1:32:55,  1.86s/it]  3%|▎         | 89/3086 [03:00<1:30:24,  1.81s/it]  3%|▎         | 90/3086 [03:02<1:39:09,  1.99s/it]                                                   {'loss': 0.6805, 'grad_norm': 0.07065323740243912, 'learning_rate': 0.0005825016202203499, 'epoch': 0.03}
  3%|▎         | 90/3086 [03:02<1:39:09,  1.99s/it]  3%|▎         | 91/3086 [03:04<1:38:18,  1.97s/it]  3%|▎         | 92/3086 [03:06<1:37:25,  1.95s/it]  3%|▎         | 93/3086 [03:09<1:45:54,  2.12s/it]  3%|▎         | 94/3086 [03:11<1:46:13,  2.13s/it]  3%|▎         | 95/3086 [03:13<1:45:33,  2.12s/it]  3%|▎         | 96/3086 [03:15<1:41:15,  2.03s/it]  3%|▎         | 97/3086 [03:17<1:42:00,  2.05s/it]  3%|▎         | 98/3086 [03:19<1:42:38,  2.06s/it]  3%|▎         | 99/3086 [03:22<1:55:37,  2.32s/it]  3%|▎         | 100/3086 [03:24<1:50:00,  2.21s/it]                                                    {'loss': 0.6517, 'grad_norm': 0.07422814518213272, 'learning_rate': 0.0005805573558003888, 'epoch': 0.03}
  3%|▎         | 100/3086 [03:24<1:50:00,  2.21s/it]  3%|▎         | 101/3086 [03:26<1:44:04,  2.09s/it]  3%|▎         | 102/3086 [03:28<1:42:29,  2.06s/it]  3%|▎         | 103/3086 [03:29<1:38:04,  1.97s/it]  3%|▎         | 104/3086 [03:32<1:42:34,  2.06s/it]  3%|▎         | 105/3086 [03:34<1:44:33,  2.10s/it]  3%|▎         | 106/3086 [03:36<1:51:46,  2.25s/it]  3%|▎         | 107/3086 [03:38<1:46:07,  2.14s/it]  3%|▎         | 108/3086 [03:40<1:42:43,  2.07s/it]  4%|▎         | 109/3086 [03:42<1:41:00,  2.04s/it]  4%|▎         | 110/3086 [03:44<1:41:55,  2.05s/it]                                                    {'loss': 0.6514, 'grad_norm': 0.07894335687160492, 'learning_rate': 0.0005786130913804277, 'epoch': 0.04}
  4%|▎         | 110/3086 [03:44<1:41:55,  2.05s/it]  4%|▎         | 111/3086 [03:46<1:37:28,  1.97s/it]  4%|▎         | 112/3086 [03:48<1:37:17,  1.96s/it]  4%|▎         | 113/3086 [03:50<1:33:56,  1.90s/it]  4%|▎         | 114/3086 [03:51<1:26:55,  1.75s/it]  4%|▎         | 115/3086 [03:53<1:27:44,  1.77s/it]  4%|▍         | 116/3086 [03:55<1:28:45,  1.79s/it]  4%|▍         | 117/3086 [03:57<1:30:59,  1.84s/it]  4%|▍         | 118/3086 [03:59<1:32:54,  1.88s/it]  4%|▍         | 119/3086 [04:01<1:36:54,  1.96s/it]  4%|▍         | 120/3086 [04:02<1:32:59,  1.88s/it]                                                    {'loss': 0.6485, 'grad_norm': 0.09596147388219833, 'learning_rate': 0.0005766688269604666, 'epoch': 0.04}
  4%|▍         | 120/3086 [04:03<1:32:59,  1.88s/it]  4%|▍         | 121/3086 [04:05<1:35:57,  1.94s/it]  4%|▍         | 122/3086 [04:07<1:41:44,  2.06s/it]  4%|▍         | 123/3086 [04:09<1:44:01,  2.11s/it]  4%|▍         | 124/3086 [04:12<1:50:20,  2.24s/it]  4%|▍         | 125/3086 [04:14<1:58:08,  2.39s/it]  4%|▍         | 126/3086 [04:17<1:58:02,  2.39s/it]  4%|▍         | 127/3086 [04:19<1:53:38,  2.30s/it]  4%|▍         | 128/3086 [04:21<1:49:18,  2.22s/it]  4%|▍         | 129/3086 [04:23<1:50:48,  2.25s/it]  4%|▍         | 130/3086 [04:25<1:45:45,  2.15s/it]                                                    {'loss': 0.6531, 'grad_norm': 0.07420293241739273, 'learning_rate': 0.0005747245625405055, 'epoch': 0.04}
  4%|▍         | 130/3086 [04:25<1:45:45,  2.15s/it]  4%|▍         | 131/3086 [04:27<1:47:04,  2.17s/it]  4%|▍         | 132/3086 [04:29<1:42:41,  2.09s/it]  4%|▍         | 133/3086 [04:31<1:39:02,  2.01s/it]  4%|▍         | 134/3086 [04:33<1:39:39,  2.03s/it]  4%|▍         | 135/3086 [04:35<1:34:02,  1.91s/it]  4%|▍         | 136/3086 [04:36<1:29:05,  1.81s/it]  4%|▍         | 137/3086 [04:39<1:34:09,  1.92s/it]  4%|▍         | 138/3086 [04:40<1:32:37,  1.89s/it]  5%|▍         | 139/3086 [04:42<1:35:34,  1.95s/it]  5%|▍         | 140/3086 [04:44<1:32:23,  1.88s/it]                                                    {'loss': 0.6461, 'grad_norm': 0.07611651718616486, 'learning_rate': 0.0005727802981205444, 'epoch': 0.05}
  5%|▍         | 140/3086 [04:44<1:32:23,  1.88s/it]  5%|▍         | 141/3086 [04:46<1:36:43,  1.97s/it]  5%|▍         | 142/3086 [04:49<1:40:09,  2.04s/it]  5%|▍         | 143/3086 [04:51<1:39:01,  2.02s/it]  5%|▍         | 144/3086 [04:52<1:34:00,  1.92s/it]  5%|▍         | 145/3086 [04:54<1:35:07,  1.94s/it]  5%|▍         | 146/3086 [04:56<1:35:43,  1.95s/it]  5%|▍         | 147/3086 [04:58<1:32:17,  1.88s/it]  5%|▍         | 148/3086 [05:00<1:39:33,  2.03s/it]  5%|▍         | 149/3086 [05:02<1:41:00,  2.06s/it]  5%|▍         | 150/3086 [05:04<1:40:16,  2.05s/it]                                                    {'loss': 0.6325, 'grad_norm': 0.07660933583974838, 'learning_rate': 0.0005708360337005833, 'epoch': 0.05}
  5%|▍         | 150/3086 [05:04<1:40:16,  2.05s/it]  5%|▍         | 151/3086 [05:06<1:35:38,  1.96s/it]  5%|▍         | 152/3086 [05:08<1:38:57,  2.02s/it]  5%|▍         | 153/3086 [05:10<1:37:47,  2.00s/it]  5%|▍         | 154/3086 [05:12<1:36:14,  1.97s/it]  5%|▌         | 155/3086 [05:14<1:34:24,  1.93s/it]  5%|▌         | 156/3086 [05:16<1:38:04,  2.01s/it]  5%|▌         | 157/3086 [05:18<1:37:44,  2.00s/it]  5%|▌         | 158/3086 [05:20<1:39:52,  2.05s/it]  5%|▌         | 159/3086 [05:22<1:37:28,  2.00s/it]  5%|▌         | 160/3086 [05:25<1:44:17,  2.14s/it]                                                    {'loss': 0.6179, 'grad_norm': 0.06809059530496597, 'learning_rate': 0.0005688917692806221, 'epoch': 0.05}
  5%|▌         | 160/3086 [05:25<1:44:17,  2.14s/it]  5%|▌         | 161/3086 [05:28<1:56:55,  2.40s/it]  5%|▌         | 162/3086 [05:30<1:50:51,  2.27s/it]  5%|▌         | 163/3086 [05:32<1:45:45,  2.17s/it]  5%|▌         | 164/3086 [05:34<1:45:12,  2.16s/it]  5%|▌         | 165/3086 [05:36<1:52:29,  2.31s/it]  5%|▌         | 166/3086 [05:39<1:55:05,  2.36s/it]  5%|▌         | 167/3086 [05:41<1:50:01,  2.26s/it]  5%|▌         | 168/3086 [05:43<1:45:45,  2.17s/it]  5%|▌         | 169/3086 [05:45<1:37:59,  2.02s/it]  6%|▌         | 170/3086 [05:46<1:33:54,  1.93s/it]                                                    {'loss': 0.622, 'grad_norm': 0.11822641640901566, 'learning_rate': 0.000566947504860661, 'epoch': 0.06}
  6%|▌         | 170/3086 [05:46<1:33:54,  1.93s/it]  6%|▌         | 171/3086 [05:49<1:46:58,  2.20s/it]  6%|▌         | 172/3086 [05:51<1:38:34,  2.03s/it]  6%|▌         | 173/3086 [05:53<1:36:12,  1.98s/it]  6%|▌         | 174/3086 [05:55<1:43:00,  2.12s/it]  6%|▌         | 175/3086 [05:57<1:43:13,  2.13s/it]  6%|▌         | 176/3086 [05:59<1:34:48,  1.95s/it]  6%|▌         | 177/3086 [06:01<1:38:30,  2.03s/it]  6%|▌         | 178/3086 [06:03<1:38:55,  2.04s/it]  6%|▌         | 179/3086 [06:05<1:33:27,  1.93s/it]  6%|▌         | 180/3086 [06:06<1:27:24,  1.80s/it]                                                    {'loss': 0.6211, 'grad_norm': 0.062218666076660156, 'learning_rate': 0.0005650032404406999, 'epoch': 0.06}
  6%|▌         | 180/3086 [06:06<1:27:24,  1.80s/it]  6%|▌         | 181/3086 [06:08<1:26:57,  1.80s/it]  6%|▌         | 182/3086 [06:10<1:31:33,  1.89s/it]  6%|▌         | 183/3086 [06:12<1:31:00,  1.88s/it]  6%|▌         | 184/3086 [06:14<1:30:47,  1.88s/it]  6%|▌         | 185/3086 [06:16<1:29:33,  1.85s/it]  6%|▌         | 186/3086 [06:18<1:31:24,  1.89s/it]  6%|▌         | 187/3086 [06:19<1:28:48,  1.84s/it]  6%|▌         | 188/3086 [06:22<1:36:33,  2.00s/it]  6%|▌         | 189/3086 [06:24<1:42:21,  2.12s/it]  6%|▌         | 190/3086 [06:26<1:33:35,  1.94s/it]                                                    {'loss': 0.6108, 'grad_norm': 0.10570404678583145, 'learning_rate': 0.0005630589760207388, 'epoch': 0.06}
  6%|▌         | 190/3086 [06:26<1:33:35,  1.94s/it]  6%|▌         | 191/3086 [06:28<1:38:24,  2.04s/it]  6%|▌         | 192/3086 [06:30<1:33:25,  1.94s/it]  6%|▋         | 193/3086 [06:32<1:35:34,  1.98s/it]  6%|▋         | 194/3086 [06:34<1:36:51,  2.01s/it]  6%|▋         | 195/3086 [06:36<1:34:16,  1.96s/it]  6%|▋         | 196/3086 [06:38<1:43:59,  2.16s/it]  6%|▋         | 197/3086 [06:40<1:39:05,  2.06s/it]  6%|▋         | 198/3086 [06:43<1:48:40,  2.26s/it]  6%|▋         | 199/3086 [06:45<1:43:20,  2.15s/it]  6%|▋         | 200/3086 [06:47<1:42:54,  2.14s/it]                                                    {'loss': 0.5997, 'grad_norm': 0.06483344733715057, 'learning_rate': 0.0005611147116007777, 'epoch': 0.06}
  6%|▋         | 200/3086 [06:47<1:42:54,  2.14s/it]  7%|▋         | 201/3086 [06:49<1:40:24,  2.09s/it]  7%|▋         | 202/3086 [06:51<1:46:52,  2.22s/it]  7%|▋         | 203/3086 [06:53<1:41:12,  2.11s/it]  7%|▋         | 204/3086 [06:55<1:33:44,  1.95s/it]  7%|▋         | 205/3086 [06:57<1:44:28,  2.18s/it]  7%|▋         | 206/3086 [06:59<1:39:36,  2.08s/it]  7%|▋         | 207/3086 [07:01<1:37:21,  2.03s/it]  7%|▋         | 208/3086 [07:04<1:41:39,  2.12s/it]  7%|▋         | 209/3086 [07:05<1:36:26,  2.01s/it]  7%|▋         | 210/3086 [07:07<1:35:18,  1.99s/it]                                                    {'loss': 0.6021, 'grad_norm': 0.06799495965242386, 'learning_rate': 0.0005591704471808166, 'epoch': 0.07}
  7%|▋         | 210/3086 [07:07<1:35:18,  1.99s/it]  7%|▋         | 211/3086 [07:10<1:40:57,  2.11s/it]  7%|▋         | 212/3086 [07:12<1:44:44,  2.19s/it]  7%|▋         | 213/3086 [07:14<1:43:41,  2.17s/it]  7%|▋         | 214/3086 [07:16<1:37:35,  2.04s/it]  7%|▋         | 215/3086 [07:19<1:47:31,  2.25s/it]  7%|▋         | 216/3086 [07:21<1:43:33,  2.16s/it]  7%|▋         | 217/3086 [07:23<1:43:59,  2.17s/it]  7%|▋         | 218/3086 [07:25<1:39:49,  2.09s/it]  7%|▋         | 219/3086 [07:27<1:42:26,  2.14s/it]  7%|▋         | 220/3086 [07:29<1:35:34,  2.00s/it]                                                    {'loss': 0.6024, 'grad_norm': 0.06607820838689804, 'learning_rate': 0.0005572261827608555, 'epoch': 0.07}
  7%|▋         | 220/3086 [07:29<1:35:34,  2.00s/it]  7%|▋         | 221/3086 [07:31<1:43:09,  2.16s/it]  7%|▋         | 222/3086 [07:33<1:38:04,  2.05s/it]  7%|▋         | 223/3086 [07:35<1:40:39,  2.11s/it]  7%|▋         | 224/3086 [07:37<1:39:06,  2.08s/it]  7%|▋         | 225/3086 [07:40<1:44:32,  2.19s/it]  7%|▋         | 226/3086 [07:42<1:48:23,  2.27s/it]  7%|▋         | 227/3086 [07:45<1:57:28,  2.47s/it]  7%|▋         | 228/3086 [07:48<1:58:50,  2.49s/it]  7%|▋         | 229/3086 [07:49<1:50:27,  2.32s/it]  7%|▋         | 230/3086 [07:51<1:38:36,  2.07s/it]                                                    {'loss': 0.5888, 'grad_norm': 0.08276420086622238, 'learning_rate': 0.0005552819183408942, 'epoch': 0.07}
  7%|▋         | 230/3086 [07:51<1:38:36,  2.07s/it]  7%|▋         | 231/3086 [07:53<1:35:09,  2.00s/it]  8%|▊         | 232/3086 [07:55<1:41:54,  2.14s/it]  8%|▊         | 233/3086 [07:57<1:36:36,  2.03s/it]  8%|▊         | 234/3086 [07:59<1:41:31,  2.14s/it]  8%|▊         | 235/3086 [08:01<1:36:37,  2.03s/it]  8%|▊         | 236/3086 [08:03<1:35:46,  2.02s/it]  8%|▊         | 237/3086 [08:05<1:32:35,  1.95s/it]  8%|▊         | 238/3086 [08:07<1:33:09,  1.96s/it]  8%|▊         | 239/3086 [08:09<1:32:23,  1.95s/it]  8%|▊         | 240/3086 [08:11<1:30:30,  1.91s/it]                                                    {'loss': 0.5893, 'grad_norm': 0.05724336951971054, 'learning_rate': 0.0005533376539209332, 'epoch': 0.08}
  8%|▊         | 240/3086 [08:11<1:30:30,  1.91s/it]  8%|▊         | 241/3086 [08:13<1:29:06,  1.88s/it]  8%|▊         | 242/3086 [08:14<1:29:10,  1.88s/it]  8%|▊         | 243/3086 [08:16<1:31:46,  1.94s/it]  8%|▊         | 244/3086 [08:19<1:37:57,  2.07s/it]  8%|▊         | 245/3086 [08:21<1:35:36,  2.02s/it]  8%|▊         | 246/3086 [08:23<1:34:35,  2.00s/it]  8%|▊         | 247/3086 [08:25<1:36:40,  2.04s/it]  8%|▊         | 248/3086 [08:27<1:34:17,  1.99s/it]  8%|▊         | 249/3086 [08:28<1:29:10,  1.89s/it]  8%|▊         | 250/3086 [08:30<1:31:24,  1.93s/it]                                                    {'loss': 0.5943, 'grad_norm': 0.06917573511600494, 'learning_rate': 0.0005513933895009721, 'epoch': 0.08}
  8%|▊         | 250/3086 [08:30<1:31:24,  1.93s/it]  8%|▊         | 251/3086 [08:32<1:28:35,  1.87s/it]  8%|▊         | 252/3086 [08:34<1:29:30,  1.89s/it]  8%|▊         | 253/3086 [08:36<1:35:13,  2.02s/it]  8%|▊         | 254/3086 [08:39<1:36:58,  2.05s/it]  8%|▊         | 255/3086 [08:40<1:28:30,  1.88s/it]  8%|▊         | 256/3086 [08:43<1:38:35,  2.09s/it]  8%|▊         | 257/3086 [08:45<1:41:32,  2.15s/it]  8%|▊         | 258/3086 [08:47<1:35:19,  2.02s/it]  8%|▊         | 259/3086 [08:49<1:35:16,  2.02s/it]  8%|▊         | 260/3086 [08:50<1:28:27,  1.88s/it]                                                    {'loss': 0.5948, 'grad_norm': 0.0796164721250534, 'learning_rate': 0.000549449125081011, 'epoch': 0.08}
  8%|▊         | 260/3086 [08:50<1:28:27,  1.88s/it]  8%|▊         | 261/3086 [08:52<1:25:59,  1.83s/it]  8%|▊         | 262/3086 [08:54<1:31:05,  1.94s/it]  9%|▊         | 263/3086 [08:56<1:29:58,  1.91s/it]  9%|▊         | 264/3086 [08:58<1:35:29,  2.03s/it]  9%|▊         | 265/3086 [09:00<1:35:02,  2.02s/it]  9%|▊         | 266/3086 [09:02<1:33:38,  1.99s/it]  9%|▊         | 267/3086 [09:04<1:30:20,  1.92s/it]  9%|▊         | 268/3086 [09:06<1:33:11,  1.98s/it]  9%|▊         | 269/3086 [09:08<1:32:55,  1.98s/it]  9%|▊         | 270/3086 [09:10<1:28:23,  1.88s/it]                                                    {'loss': 0.5931, 'grad_norm': 0.05624285340309143, 'learning_rate': 0.0005475048606610499, 'epoch': 0.09}
  9%|▊         | 270/3086 [09:10<1:28:23,  1.88s/it]  9%|▉         | 271/3086 [09:11<1:27:43,  1.87s/it]  9%|▉         | 272/3086 [09:14<1:41:21,  2.16s/it]  9%|▉         | 273/3086 [09:16<1:38:19,  2.10s/it]  9%|▉         | 274/3086 [09:18<1:38:11,  2.10s/it]  9%|▉         | 275/3086 [09:20<1:38:24,  2.10s/it]  9%|▉         | 276/3086 [09:23<1:43:18,  2.21s/it]  9%|▉         | 277/3086 [09:25<1:44:27,  2.23s/it]  9%|▉         | 278/3086 [09:27<1:43:10,  2.20s/it]  9%|▉         | 279/3086 [09:29<1:35:53,  2.05s/it]  9%|▉         | 280/3086 [09:31<1:39:47,  2.13s/it]                                                    {'loss': 0.5928, 'grad_norm': 0.0683085173368454, 'learning_rate': 0.0005455605962410887, 'epoch': 0.09}
  9%|▉         | 280/3086 [09:31<1:39:47,  2.13s/it]  9%|▉         | 281/3086 [09:33<1:32:12,  1.97s/it]  9%|▉         | 282/3086 [09:35<1:35:39,  2.05s/it]  9%|▉         | 283/3086 [09:37<1:33:19,  2.00s/it]  9%|▉         | 284/3086 [09:39<1:30:32,  1.94s/it]  9%|▉         | 285/3086 [09:41<1:30:21,  1.94s/it]  9%|▉         | 286/3086 [09:43<1:40:13,  2.15s/it]  9%|▉         | 287/3086 [09:45<1:33:39,  2.01s/it]  9%|▉         | 288/3086 [09:48<1:44:09,  2.23s/it]  9%|▉         | 289/3086 [09:50<1:38:29,  2.11s/it]  9%|▉         | 290/3086 [09:52<1:36:25,  2.07s/it]                                                    {'loss': 0.5663, 'grad_norm': 0.08891347795724869, 'learning_rate': 0.0005436163318211277, 'epoch': 0.09}
  9%|▉         | 290/3086 [09:52<1:36:25,  2.07s/it]  9%|▉         | 291/3086 [09:53<1:30:56,  1.95s/it]  9%|▉         | 292/3086 [09:55<1:33:03,  2.00s/it]  9%|▉         | 293/3086 [09:58<1:35:34,  2.05s/it] 10%|▉         | 294/3086 [10:00<1:32:43,  1.99s/it] 10%|▉         | 295/3086 [10:02<1:37:11,  2.09s/it] 10%|▉         | 296/3086 [10:03<1:30:31,  1.95s/it] 10%|▉         | 297/3086 [10:05<1:31:56,  1.98s/it] 10%|▉         | 298/3086 [10:08<1:32:49,  2.00s/it] 10%|▉         | 299/3086 [10:10<1:32:47,  2.00s/it] 10%|▉         | 300/3086 [10:11<1:27:16,  1.88s/it]                                                    {'loss': 0.5825, 'grad_norm': 0.06319117546081543, 'learning_rate': 0.0005416720674011666, 'epoch': 0.1}
 10%|▉         | 300/3086 [10:11<1:27:16,  1.88s/it] 10%|▉         | 301/3086 [10:13<1:31:09,  1.96s/it] 10%|▉         | 302/3086 [10:15<1:27:18,  1.88s/it] 10%|▉         | 303/3086 [10:17<1:32:00,  1.98s/it] 10%|▉         | 304/3086 [10:20<1:37:54,  2.11s/it] 10%|▉         | 305/3086 [10:22<1:38:29,  2.13s/it] 10%|▉         | 306/3086 [10:24<1:40:18,  2.16s/it] 10%|▉         | 307/3086 [10:26<1:41:11,  2.18s/it] 10%|▉         | 308/3086 [10:28<1:35:10,  2.06s/it] 10%|█         | 309/3086 [10:30<1:28:42,  1.92s/it] 10%|█         | 310/3086 [10:31<1:24:59,  1.84s/it]                                                    {'loss': 0.587, 'grad_norm': 0.07322104275226593, 'learning_rate': 0.0005397278029812053, 'epoch': 0.1}
 10%|█         | 310/3086 [10:31<1:24:59,  1.84s/it] 10%|█         | 311/3086 [10:33<1:24:29,  1.83s/it] 10%|█         | 312/3086 [10:35<1:26:20,  1.87s/it] 10%|█         | 313/3086 [10:37<1:23:56,  1.82s/it] 10%|█         | 314/3086 [10:39<1:24:32,  1.83s/it] 10%|█         | 315/3086 [10:41<1:29:55,  1.95s/it] 10%|█         | 316/3086 [10:43<1:26:34,  1.88s/it] 10%|█         | 317/3086 [10:44<1:27:23,  1.89s/it] 10%|█         | 318/3086 [10:47<1:29:30,  1.94s/it] 10%|█         | 319/3086 [10:49<1:30:20,  1.96s/it] 10%|█         | 320/3086 [10:50<1:30:18,  1.96s/it]                                                    {'loss': 0.5631, 'grad_norm': 0.061452120542526245, 'learning_rate': 0.0005377835385612443, 'epoch': 0.1}
 10%|█         | 320/3086 [10:50<1:30:18,  1.96s/it] 10%|█         | 321/3086 [10:53<1:31:37,  1.99s/it] 10%|█         | 322/3086 [10:54<1:27:54,  1.91s/it] 10%|█         | 323/3086 [10:56<1:25:24,  1.85s/it] 10%|█         | 324/3086 [10:58<1:26:45,  1.88s/it] 11%|█         | 325/3086 [11:00<1:23:58,  1.83s/it] 11%|█         | 326/3086 [11:02<1:30:59,  1.98s/it] 11%|█         | 327/3086 [11:04<1:28:19,  1.92s/it] 11%|█         | 328/3086 [11:06<1:29:26,  1.95s/it] 11%|█         | 329/3086 [11:08<1:33:59,  2.05s/it] 11%|█         | 330/3086 [11:10<1:34:13,  2.05s/it]                                                    {'loss': 0.5767, 'grad_norm': 0.09793346375226974, 'learning_rate': 0.0005358392741412831, 'epoch': 0.11}
 11%|█         | 330/3086 [11:10<1:34:13,  2.05s/it] 11%|█         | 331/3086 [11:12<1:36:04,  2.09s/it] 11%|█         | 332/3086 [11:14<1:32:48,  2.02s/it] 11%|█         | 333/3086 [11:16<1:29:03,  1.94s/it] 11%|█         | 334/3086 [11:18<1:37:25,  2.12s/it] 11%|█         | 335/3086 [11:20<1:35:10,  2.08s/it] 11%|█         | 336/3086 [11:23<1:35:28,  2.08s/it] 11%|█         | 337/3086 [11:24<1:32:25,  2.02s/it] 11%|█         | 338/3086 [11:26<1:27:54,  1.92s/it] 11%|█         | 339/3086 [11:28<1:29:13,  1.95s/it] 11%|█         | 340/3086 [11:30<1:34:57,  2.07s/it]                                                    {'loss': 0.5656, 'grad_norm': 0.07213053852319717, 'learning_rate': 0.000533895009721322, 'epoch': 0.11}
 11%|█         | 340/3086 [11:30<1:34:57,  2.07s/it] 11%|█         | 341/3086 [11:32<1:31:29,  2.00s/it] 11%|█         | 342/3086 [11:34<1:30:57,  1.99s/it] 11%|█         | 343/3086 [11:36<1:24:30,  1.85s/it] 11%|█         | 344/3086 [11:37<1:21:09,  1.78s/it] 11%|█         | 345/3086 [11:40<1:29:05,  1.95s/it] 11%|█         | 346/3086 [11:42<1:35:22,  2.09s/it] 11%|█         | 347/3086 [11:44<1:30:27,  1.98s/it] 11%|█▏        | 348/3086 [11:46<1:32:53,  2.04s/it] 11%|█▏        | 349/3086 [11:48<1:32:30,  2.03s/it] 11%|█▏        | 350/3086 [11:50<1:34:06,  2.06s/it]                                                    {'loss': 0.5761, 'grad_norm': 0.055029548704624176, 'learning_rate': 0.000531950745301361, 'epoch': 0.11}
 11%|█▏        | 350/3086 [11:50<1:34:06,  2.06s/it] 11%|█▏        | 351/3086 [11:52<1:29:24,  1.96s/it] 11%|█▏        | 352/3086 [11:54<1:29:09,  1.96s/it] 11%|█▏        | 353/3086 [11:56<1:28:54,  1.95s/it] 11%|█▏        | 354/3086 [11:58<1:31:41,  2.01s/it] 12%|█▏        | 355/3086 [12:00<1:28:59,  1.96s/it] 12%|█▏        | 356/3086 [12:02<1:28:41,  1.95s/it] 12%|█▏        | 357/3086 [12:03<1:23:25,  1.83s/it] 12%|█▏        | 358/3086 [12:06<1:30:56,  2.00s/it] 12%|█▏        | 359/3086 [12:08<1:33:57,  2.07s/it] 12%|█▏        | 360/3086 [12:09<1:27:13,  1.92s/it]                                                    {'loss': 0.5676, 'grad_norm': 0.0642017051577568, 'learning_rate': 0.0005300064808813998, 'epoch': 0.12}
 12%|█▏        | 360/3086 [12:09<1:27:13,  1.92s/it] 12%|█▏        | 361/3086 [12:12<1:31:27,  2.01s/it] 12%|█▏        | 362/3086 [12:14<1:32:49,  2.04s/it] 12%|█▏        | 363/3086 [12:16<1:29:03,  1.96s/it] 12%|█▏        | 364/3086 [12:18<1:29:56,  1.98s/it] 12%|█▏        | 365/3086 [12:20<1:29:30,  1.97s/it] 12%|█▏        | 366/3086 [12:21<1:28:59,  1.96s/it] 12%|█▏        | 367/3086 [12:24<1:31:58,  2.03s/it] 12%|█▏        | 368/3086 [12:26<1:31:28,  2.02s/it] 12%|█▏        | 369/3086 [12:27<1:28:48,  1.96s/it] 12%|█▏        | 370/3086 [12:29<1:27:51,  1.94s/it]                                                    {'loss': 0.5765, 'grad_norm': 0.07866086810827255, 'learning_rate': 0.0005280622164614388, 'epoch': 0.12}
 12%|█▏        | 370/3086 [12:29<1:27:51,  1.94s/it] 12%|█▏        | 371/3086 [12:31<1:29:22,  1.98s/it] 12%|█▏        | 372/3086 [12:33<1:26:35,  1.91s/it] 12%|█▏        | 373/3086 [12:35<1:27:40,  1.94s/it] 12%|█▏        | 374/3086 [12:37<1:24:37,  1.87s/it] 12%|█▏        | 375/3086 [12:39<1:30:45,  2.01s/it] 12%|█▏        | 376/3086 [12:41<1:30:37,  2.01s/it] 12%|█▏        | 377/3086 [12:43<1:30:28,  2.00s/it] 12%|█▏        | 378/3086 [12:45<1:30:16,  2.00s/it] 12%|█▏        | 379/3086 [12:48<1:41:33,  2.25s/it] 12%|█▏        | 380/3086 [12:50<1:39:38,  2.21s/it]                                                    {'loss': 0.5668, 'grad_norm': 0.06390749663114548, 'learning_rate': 0.0005261179520414775, 'epoch': 0.12}
 12%|█▏        | 380/3086 [12:50<1:39:38,  2.21s/it] 12%|█▏        | 381/3086 [12:52<1:35:58,  2.13s/it] 12%|█▏        | 382/3086 [12:54<1:37:57,  2.17s/it] 12%|█▏        | 383/3086 [12:57<1:38:08,  2.18s/it] 12%|█▏        | 384/3086 [12:59<1:38:30,  2.19s/it] 12%|█▏        | 385/3086 [13:01<1:37:31,  2.17s/it] 13%|█▎        | 386/3086 [13:03<1:36:10,  2.14s/it] 13%|█▎        | 387/3086 [13:05<1:37:56,  2.18s/it] 13%|█▎        | 388/3086 [13:07<1:32:58,  2.07s/it] 13%|█▎        | 389/3086 [13:09<1:33:57,  2.09s/it] 13%|█▎        | 390/3086 [13:11<1:27:31,  1.95s/it]                                                    {'loss': 0.5737, 'grad_norm': 0.07206735014915466, 'learning_rate': 0.0005241736876215164, 'epoch': 0.13}
 13%|█▎        | 390/3086 [13:11<1:27:31,  1.95s/it] 13%|█▎        | 391/3086 [13:14<1:40:44,  2.24s/it] 13%|█▎        | 392/3086 [13:16<1:40:09,  2.23s/it] 13%|█▎        | 393/3086 [13:18<1:34:38,  2.11s/it] 13%|█▎        | 394/3086 [13:20<1:37:44,  2.18s/it] 13%|█▎        | 395/3086 [13:22<1:36:43,  2.16s/it] 13%|█▎        | 396/3086 [13:24<1:34:27,  2.11s/it] 13%|█▎        | 397/3086 [13:26<1:26:06,  1.92s/it] 13%|█▎        | 398/3086 [13:27<1:22:23,  1.84s/it] 13%|█▎        | 399/3086 [13:29<1:26:02,  1.92s/it] 13%|█▎        | 400/3086 [13:32<1:31:10,  2.04s/it]                                                    {'loss': 0.5691, 'grad_norm': 0.0756513774394989, 'learning_rate': 0.0005222294232015554, 'epoch': 0.13}
 13%|█▎        | 400/3086 [13:32<1:31:10,  2.04s/it] 13%|█▎        | 401/3086 [13:34<1:33:26,  2.09s/it] 13%|█▎        | 402/3086 [13:36<1:34:24,  2.11s/it] 13%|█▎        | 403/3086 [13:38<1:29:08,  1.99s/it] 13%|█▎        | 404/3086 [13:40<1:35:44,  2.14s/it] 13%|█▎        | 405/3086 [13:42<1:34:36,  2.12s/it] 13%|█▎        | 406/3086 [13:45<1:38:25,  2.20s/it] 13%|█▎        | 407/3086 [13:47<1:40:25,  2.25s/it] 13%|█▎        | 408/3086 [13:49<1:38:17,  2.20s/it] 13%|█▎        | 409/3086 [13:51<1:35:11,  2.13s/it] 13%|█▎        | 410/3086 [13:53<1:27:24,  1.96s/it]                                                    {'loss': 0.5499, 'grad_norm': 0.07666122913360596, 'learning_rate': 0.0005202851587815942, 'epoch': 0.13}
 13%|█▎        | 410/3086 [13:53<1:27:24,  1.96s/it] 13%|█▎        | 411/3086 [13:55<1:32:02,  2.06s/it] 13%|█▎        | 412/3086 [13:57<1:27:44,  1.97s/it] 13%|█▎        | 413/3086 [13:59<1:24:45,  1.90s/it] 13%|█▎        | 414/3086 [14:00<1:23:08,  1.87s/it] 13%|█▎        | 415/3086 [14:03<1:28:51,  2.00s/it] 13%|█▎        | 416/3086 [14:05<1:34:23,  2.12s/it] 14%|█▎        | 417/3086 [14:07<1:29:16,  2.01s/it] 14%|█▎        | 418/3086 [14:09<1:28:41,  1.99s/it] 14%|█▎        | 419/3086 [14:11<1:30:24,  2.03s/it] 14%|█▎        | 420/3086 [14:14<1:40:13,  2.26s/it]                                                    {'loss': 0.5612, 'grad_norm': 0.059387024492025375, 'learning_rate': 0.0005183408943616331, 'epoch': 0.14}
 14%|█▎        | 420/3086 [14:14<1:40:13,  2.26s/it] 14%|█▎        | 421/3086 [14:16<1:35:09,  2.14s/it] 14%|█▎        | 422/3086 [14:17<1:31:58,  2.07s/it] 14%|█▎        | 423/3086 [14:19<1:25:49,  1.93s/it] 14%|█▎        | 424/3086 [14:22<1:33:58,  2.12s/it] 14%|█▍        | 425/3086 [14:24<1:40:00,  2.25s/it] 14%|█▍        | 426/3086 [14:26<1:34:02,  2.12s/it] 14%|█▍        | 427/3086 [14:28<1:38:28,  2.22s/it] 14%|█▍        | 428/3086 [14:30<1:33:19,  2.11s/it] 14%|█▍        | 429/3086 [14:32<1:33:27,  2.11s/it] 14%|█▍        | 430/3086 [14:35<1:34:29,  2.13s/it]                                                    {'loss': 0.5643, 'grad_norm': 0.0593925416469574, 'learning_rate': 0.0005163966299416721, 'epoch': 0.14}
 14%|█▍        | 430/3086 [14:35<1:34:29,  2.13s/it] 14%|█▍        | 431/3086 [14:37<1:30:54,  2.05s/it] 14%|█▍        | 432/3086 [14:39<1:40:19,  2.27s/it] 14%|█▍        | 433/3086 [14:41<1:35:15,  2.15s/it] 14%|█▍        | 434/3086 [14:43<1:31:09,  2.06s/it] 14%|█▍        | 435/3086 [14:46<1:37:31,  2.21s/it] 14%|█▍        | 436/3086 [14:47<1:33:12,  2.11s/it] 14%|█▍        | 437/3086 [14:50<1:37:38,  2.21s/it] 14%|█▍        | 438/3086 [14:52<1:30:45,  2.06s/it] 14%|█▍        | 439/3086 [14:54<1:30:22,  2.05s/it] 14%|█▍        | 440/3086 [14:55<1:25:44,  1.94s/it]                                                    {'loss': 0.5445, 'grad_norm': 0.07007869333028793, 'learning_rate': 0.0005144523655217109, 'epoch': 0.14}
 14%|█▍        | 440/3086 [14:55<1:25:44,  1.94s/it] 14%|█▍        | 441/3086 [14:58<1:32:06,  2.09s/it] 14%|█▍        | 442/3086 [15:00<1:28:06,  2.00s/it] 14%|█▍        | 443/3086 [15:02<1:33:15,  2.12s/it] 14%|█▍        | 444/3086 [15:04<1:35:47,  2.18s/it] 14%|█▍        | 445/3086 [15:06<1:30:04,  2.05s/it] 14%|█▍        | 446/3086 [15:08<1:28:08,  2.00s/it] 14%|█▍        | 447/3086 [15:10<1:27:22,  1.99s/it] 15%|█▍        | 448/3086 [15:11<1:23:05,  1.89s/it] 15%|█▍        | 449/3086 [15:14<1:25:00,  1.93s/it] 15%|█▍        | 450/3086 [15:15<1:20:08,  1.82s/it]                                                    {'loss': 0.5599, 'grad_norm': 0.08058411628007889, 'learning_rate': 0.0005125081011017499, 'epoch': 0.15}
 15%|█▍        | 450/3086 [15:15<1:20:08,  1.82s/it] 15%|█▍        | 451/3086 [15:17<1:24:14,  1.92s/it] 15%|█▍        | 452/3086 [15:19<1:18:20,  1.78s/it] 15%|█▍        | 453/3086 [15:21<1:19:45,  1.82s/it] 15%|█▍        | 454/3086 [15:23<1:24:03,  1.92s/it] 15%|█▍        | 455/3086 [15:25<1:27:41,  2.00s/it] 15%|█▍        | 456/3086 [15:27<1:28:50,  2.03s/it] 15%|█▍        | 457/3086 [15:29<1:33:54,  2.14s/it] 15%|█▍        | 458/3086 [15:31<1:28:21,  2.02s/it] 15%|█▍        | 459/3086 [15:33<1:24:12,  1.92s/it] 15%|█▍        | 460/3086 [15:35<1:21:57,  1.87s/it]                                                    {'loss': 0.5544, 'grad_norm': 0.0837000161409378, 'learning_rate': 0.0005105638366817886, 'epoch': 0.15}
 15%|█▍        | 460/3086 [15:35<1:21:57,  1.87s/it] 15%|█▍        | 461/3086 [15:37<1:24:50,  1.94s/it] 15%|█▍        | 462/3086 [15:38<1:21:09,  1.86s/it] 15%|█▌        | 463/3086 [15:40<1:20:42,  1.85s/it] 15%|█▌        | 464/3086 [15:43<1:28:10,  2.02s/it] 15%|█▌        | 465/3086 [15:45<1:31:05,  2.09s/it] 15%|█▌        | 466/3086 [15:47<1:33:10,  2.13s/it] 15%|█▌        | 467/3086 [15:49<1:27:31,  2.01s/it] 15%|█▌        | 468/3086 [15:51<1:32:23,  2.12s/it] 15%|█▌        | 469/3086 [15:53<1:26:21,  1.98s/it] 15%|█▌        | 470/3086 [15:55<1:30:11,  2.07s/it]                                                    {'loss': 0.552, 'grad_norm': 0.10513642430305481, 'learning_rate': 0.0005086195722618275, 'epoch': 0.15}
 15%|█▌        | 470/3086 [15:55<1:30:11,  2.07s/it] 15%|█▌        | 471/3086 [15:57<1:23:50,  1.92s/it] 15%|█▌        | 472/3086 [15:59<1:22:03,  1.88s/it] 15%|█▌        | 473/3086 [16:00<1:19:43,  1.83s/it] 15%|█▌        | 474/3086 [16:02<1:20:49,  1.86s/it] 15%|█▌        | 475/3086 [16:04<1:23:26,  1.92s/it] 15%|█▌        | 476/3086 [16:06<1:21:34,  1.88s/it] 15%|█▌        | 477/3086 [16:08<1:24:44,  1.95s/it] 15%|█▌        | 478/3086 [16:10<1:27:46,  2.02s/it] 16%|█▌        | 479/3086 [16:12<1:27:49,  2.02s/it] 16%|█▌        | 480/3086 [16:14<1:28:57,  2.05s/it]                                                    {'loss': 0.5568, 'grad_norm': 0.07063217461109161, 'learning_rate': 0.0005066753078418665, 'epoch': 0.16}
 16%|█▌        | 480/3086 [16:14<1:28:57,  2.05s/it] 16%|█▌        | 481/3086 [16:16<1:26:27,  1.99s/it] 16%|█▌        | 482/3086 [16:18<1:25:01,  1.96s/it] 16%|█▌        | 483/3086 [16:20<1:26:31,  1.99s/it] 16%|█▌        | 484/3086 [16:22<1:26:25,  1.99s/it] 16%|█▌        | 485/3086 [16:24<1:25:59,  1.98s/it] 16%|█▌        | 486/3086 [16:26<1:24:26,  1.95s/it] 16%|█▌        | 487/3086 [16:28<1:21:38,  1.88s/it] 16%|█▌        | 488/3086 [16:30<1:20:19,  1.86s/it] 16%|█▌        | 489/3086 [16:32<1:22:30,  1.91s/it] 16%|█▌        | 490/3086 [16:34<1:25:51,  1.98s/it]                                                    {'loss': 0.5431, 'grad_norm': 0.05506271868944168, 'learning_rate': 0.0005047310434219053, 'epoch': 0.16}
 16%|█▌        | 490/3086 [16:34<1:25:51,  1.98s/it] 16%|█▌        | 491/3086 [16:36<1:25:23,  1.97s/it] 16%|█▌        | 492/3086 [16:38<1:23:32,  1.93s/it] 16%|█▌        | 493/3086 [16:39<1:20:30,  1.86s/it] 16%|█▌        | 494/3086 [16:41<1:22:56,  1.92s/it] 16%|█▌        | 495/3086 [16:43<1:18:41,  1.82s/it] 16%|█▌        | 496/3086 [16:45<1:21:59,  1.90s/it] 16%|█▌        | 497/3086 [16:46<1:16:43,  1.78s/it] 16%|█▌        | 498/3086 [16:48<1:17:53,  1.81s/it] 16%|█▌        | 499/3086 [16:50<1:18:35,  1.82s/it] 16%|█▌        | 500/3086 [16:53<1:33:14,  2.16s/it]                                                    {'loss': 0.5528, 'grad_norm': 0.08250871300697327, 'learning_rate': 0.0005027867790019442, 'epoch': 0.16}
 16%|█▌        | 500/3086 [16:53<1:33:14,  2.16s/it] 16%|█▌        | 501/3086 [16:55<1:29:38,  2.08s/it] 16%|█▋        | 502/3086 [16:57<1:24:27,  1.96s/it] 16%|█▋        | 503/3086 [16:58<1:20:20,  1.87s/it] 16%|█▋        | 504/3086 [17:00<1:19:22,  1.84s/it] 16%|█▋        | 505/3086 [17:03<1:26:17,  2.01s/it] 16%|█▋        | 506/3086 [17:04<1:23:37,  1.94s/it] 16%|█▋        | 507/3086 [17:06<1:25:32,  1.99s/it] 16%|█▋        | 508/3086 [17:08<1:24:22,  1.96s/it] 16%|█▋        | 509/3086 [17:10<1:21:31,  1.90s/it] 17%|█▋        | 510/3086 [17:12<1:20:09,  1.87s/it]                                                    {'loss': 0.5486, 'grad_norm': 0.07478214055299759, 'learning_rate': 0.0005008425145819831, 'epoch': 0.17}
 17%|█▋        | 510/3086 [17:12<1:20:09,  1.87s/it] 17%|█▋        | 511/3086 [17:14<1:17:53,  1.82s/it] 17%|█▋        | 512/3086 [17:15<1:18:05,  1.82s/it] 17%|█▋        | 513/3086 [17:17<1:19:00,  1.84s/it] 17%|█▋        | 514/3086 [17:19<1:22:57,  1.94s/it] 17%|█▋        | 515/3086 [17:21<1:20:59,  1.89s/it] 17%|█▋        | 516/3086 [17:24<1:36:28,  2.25s/it] 17%|█▋        | 517/3086 [17:26<1:27:34,  2.05s/it] 17%|█▋        | 518/3086 [17:28<1:28:39,  2.07s/it] 17%|█▋        | 519/3086 [17:31<1:35:44,  2.24s/it] 17%|█▋        | 520/3086 [17:33<1:32:10,  2.16s/it]                                                    {'loss': 0.5391, 'grad_norm': 0.06371234357357025, 'learning_rate': 0.000498898250162022, 'epoch': 0.17}
 17%|█▋        | 520/3086 [17:33<1:32:10,  2.16s/it] 17%|█▋        | 521/3086 [17:35<1:32:20,  2.16s/it] 17%|█▋        | 522/3086 [17:37<1:36:12,  2.25s/it] 17%|█▋        | 523/3086 [17:40<1:38:35,  2.31s/it] 17%|█▋        | 524/3086 [17:42<1:38:02,  2.30s/it] 17%|█▋        | 525/3086 [17:44<1:34:10,  2.21s/it] 17%|█▋        | 526/3086 [17:46<1:30:48,  2.13s/it] 17%|█▋        | 527/3086 [17:48<1:36:21,  2.26s/it] 17%|█▋        | 528/3086 [17:51<1:37:04,  2.28s/it] 17%|█▋        | 529/3086 [17:53<1:35:46,  2.25s/it] 17%|█▋        | 530/3086 [17:55<1:33:56,  2.21s/it]                                                    {'loss': 0.5402, 'grad_norm': 0.061936259269714355, 'learning_rate': 0.0004969539857420609, 'epoch': 0.17}
 17%|█▋        | 530/3086 [17:55<1:33:56,  2.21s/it] 17%|█▋        | 531/3086 [17:57<1:34:06,  2.21s/it] 17%|█▋        | 532/3086 [17:59<1:33:08,  2.19s/it] 17%|█▋        | 533/3086 [18:02<1:32:39,  2.18s/it] 17%|█▋        | 534/3086 [18:04<1:33:01,  2.19s/it] 17%|█▋        | 535/3086 [18:06<1:36:45,  2.28s/it] 17%|█▋        | 536/3086 [18:08<1:31:22,  2.15s/it] 17%|█▋        | 537/3086 [18:10<1:24:36,  1.99s/it] 17%|█▋        | 538/3086 [18:12<1:23:49,  1.97s/it] 17%|█▋        | 539/3086 [18:14<1:28:04,  2.07s/it] 17%|█▋        | 540/3086 [18:16<1:23:04,  1.96s/it]                                                    {'loss': 0.551, 'grad_norm': 0.0699436366558075, 'learning_rate': 0.0004950097213220997, 'epoch': 0.17}
 17%|█▋        | 540/3086 [18:16<1:23:04,  1.96s/it] 18%|█▊        | 541/3086 [18:18<1:24:08,  1.98s/it] 18%|█▊        | 542/3086 [18:20<1:29:29,  2.11s/it] 18%|█▊        | 543/3086 [18:22<1:27:21,  2.06s/it] 18%|█▊        | 544/3086 [18:24<1:23:06,  1.96s/it] 18%|█▊        | 545/3086 [18:26<1:20:28,  1.90s/it] 18%|█▊        | 546/3086 [18:27<1:17:54,  1.84s/it] 18%|█▊        | 547/3086 [18:29<1:14:06,  1.75s/it] 18%|█▊        | 548/3086 [18:31<1:21:43,  1.93s/it] 18%|█▊        | 549/3086 [18:33<1:17:38,  1.84s/it] 18%|█▊        | 550/3086 [18:35<1:17:41,  1.84s/it]                                                    {'loss': 0.5402, 'grad_norm': 0.06783188879489899, 'learning_rate': 0.0004930654569021386, 'epoch': 0.18}
 18%|█▊        | 550/3086 [18:35<1:17:41,  1.84s/it] 18%|█▊        | 551/3086 [18:37<1:20:39,  1.91s/it] 18%|█▊        | 552/3086 [18:38<1:17:11,  1.83s/it] 18%|█▊        | 553/3086 [18:40<1:17:07,  1.83s/it] 18%|█▊        | 554/3086 [18:42<1:13:56,  1.75s/it] 18%|█▊        | 555/3086 [18:43<1:13:46,  1.75s/it] 18%|█▊        | 556/3086 [18:45<1:12:09,  1.71s/it] 18%|█▊        | 557/3086 [18:47<1:15:52,  1.80s/it] 18%|█▊        | 558/3086 [18:49<1:15:48,  1.80s/it] 18%|█▊        | 559/3086 [18:51<1:17:22,  1.84s/it] 18%|█▊        | 560/3086 [18:53<1:22:12,  1.95s/it]                                                    {'loss': 0.5313, 'grad_norm': 0.0627385750412941, 'learning_rate': 0.0004911211924821775, 'epoch': 0.18}
 18%|█▊        | 560/3086 [18:53<1:22:12,  1.95s/it] 18%|█▊        | 561/3086 [18:55<1:20:57,  1.92s/it] 18%|█▊        | 562/3086 [18:57<1:21:22,  1.93s/it] 18%|█▊        | 563/3086 [18:59<1:20:52,  1.92s/it] 18%|█▊        | 564/3086 [19:01<1:20:18,  1.91s/it] 18%|█▊        | 565/3086 [19:02<1:17:51,  1.85s/it] 18%|█▊        | 566/3086 [19:04<1:17:22,  1.84s/it] 18%|█▊        | 567/3086 [19:06<1:17:35,  1.85s/it] 18%|█▊        | 568/3086 [19:08<1:17:50,  1.85s/it] 18%|█▊        | 569/3086 [19:10<1:18:55,  1.88s/it] 18%|█▊        | 570/3086 [19:12<1:16:51,  1.83s/it]                                                    {'loss': 0.5374, 'grad_norm': 0.06662850826978683, 'learning_rate': 0.0004891769280622164, 'epoch': 0.18}
 18%|█▊        | 570/3086 [19:12<1:16:51,  1.83s/it] 19%|█▊        | 571/3086 [19:14<1:20:59,  1.93s/it] 19%|█▊        | 572/3086 [19:16<1:19:24,  1.90s/it] 19%|█▊        | 573/3086 [19:17<1:18:34,  1.88s/it] 19%|█▊        | 574/3086 [19:20<1:27:01,  2.08s/it] 19%|█▊        | 575/3086 [19:23<1:35:21,  2.28s/it] 19%|█▊        | 576/3086 [19:24<1:26:36,  2.07s/it] 19%|█▊        | 577/3086 [19:27<1:29:32,  2.14s/it] 19%|█▊        | 578/3086 [19:29<1:27:30,  2.09s/it] 19%|█▉        | 579/3086 [19:31<1:28:50,  2.13s/it] 19%|█▉        | 580/3086 [19:32<1:22:05,  1.97s/it]                                                    {'loss': 0.5391, 'grad_norm': 0.07052440196275711, 'learning_rate': 0.0004872326636422553, 'epoch': 0.19}
 19%|█▉        | 580/3086 [19:32<1:22:05,  1.97s/it] 19%|█▉        | 581/3086 [19:34<1:23:43,  2.01s/it] 19%|█▉        | 582/3086 [19:36<1:19:59,  1.92s/it] 19%|█▉        | 583/3086 [19:38<1:23:02,  1.99s/it] 19%|█▉        | 584/3086 [19:41<1:27:52,  2.11s/it] 19%|█▉        | 585/3086 [19:42<1:22:46,  1.99s/it] 19%|█▉        | 586/3086 [19:45<1:26:24,  2.07s/it] 19%|█▉        | 587/3086 [19:47<1:24:40,  2.03s/it] 19%|█▉        | 588/3086 [19:49<1:24:42,  2.03s/it] 19%|█▉        | 589/3086 [19:51<1:24:28,  2.03s/it] 19%|█▉        | 590/3086 [19:53<1:25:37,  2.06s/it]                                                    {'loss': 0.5272, 'grad_norm': 0.0632665827870369, 'learning_rate': 0.0004852883992222942, 'epoch': 0.19}
 19%|█▉        | 590/3086 [19:53<1:25:37,  2.06s/it] 19%|█▉        | 591/3086 [19:54<1:20:25,  1.93s/it] 19%|█▉        | 592/3086 [19:56<1:18:46,  1.90s/it] 19%|█▉        | 593/3086 [19:58<1:22:43,  1.99s/it] 19%|█▉        | 594/3086 [20:00<1:21:28,  1.96s/it] 19%|█▉        | 595/3086 [20:03<1:24:27,  2.03s/it] 19%|█▉        | 596/3086 [20:04<1:21:31,  1.96s/it] 19%|█▉        | 597/3086 [20:07<1:25:27,  2.06s/it] 19%|█▉        | 598/3086 [20:09<1:24:46,  2.04s/it] 19%|█▉        | 599/3086 [20:12<1:37:35,  2.35s/it] 19%|█▉        | 600/3086 [20:14<1:38:03,  2.37s/it]                                                    {'loss': 0.5291, 'grad_norm': 0.0678849071264267, 'learning_rate': 0.00048334413480233306, 'epoch': 0.19}
 19%|█▉        | 600/3086 [20:14<1:38:03,  2.37s/it] 19%|█▉        | 601/3086 [20:16<1:35:47,  2.31s/it] 20%|█▉        | 602/3086 [20:18<1:29:53,  2.17s/it] 20%|█▉        | 603/3086 [20:21<1:32:42,  2.24s/it] 20%|█▉        | 604/3086 [20:23<1:36:05,  2.32s/it] 20%|█▉        | 605/3086 [20:25<1:31:02,  2.20s/it] 20%|█▉        | 606/3086 [20:27<1:27:02,  2.11s/it] 20%|█▉        | 607/3086 [20:29<1:30:26,  2.19s/it] 20%|█▉        | 608/3086 [20:32<1:42:16,  2.48s/it] 20%|█▉        | 609/3086 [20:34<1:32:45,  2.25s/it] 20%|█▉        | 610/3086 [20:36<1:31:01,  2.21s/it]                                                    {'loss': 0.5246, 'grad_norm': 0.06228727102279663, 'learning_rate': 0.00048139987038237195, 'epoch': 0.2}
 20%|█▉        | 610/3086 [20:36<1:31:01,  2.21s/it] 20%|█▉        | 611/3086 [20:39<1:32:15,  2.24s/it] 20%|█▉        | 612/3086 [20:41<1:32:46,  2.25s/it] 20%|█▉        | 613/3086 [20:43<1:28:17,  2.14s/it] 20%|█▉        | 614/3086 [20:46<1:39:44,  2.42s/it] 20%|█▉        | 615/3086 [20:48<1:38:58,  2.40s/it] 20%|█▉        | 616/3086 [20:50<1:31:51,  2.23s/it] 20%|█▉        | 617/3086 [20:52<1:24:25,  2.05s/it] 20%|██        | 618/3086 [20:54<1:27:29,  2.13s/it] 20%|██        | 619/3086 [20:56<1:24:00,  2.04s/it] 20%|██        | 620/3086 [20:58<1:20:23,  1.96s/it]                                                    {'loss': 0.5365, 'grad_norm': 0.06907810270786285, 'learning_rate': 0.00047945560596241083, 'epoch': 0.2}
 20%|██        | 620/3086 [20:58<1:20:23,  1.96s/it] 20%|██        | 621/3086 [20:59<1:18:41,  1.92s/it] 20%|██        | 622/3086 [21:02<1:25:48,  2.09s/it] 20%|██        | 623/3086 [21:04<1:25:06,  2.07s/it] 20%|██        | 624/3086 [21:06<1:21:35,  1.99s/it] 20%|██        | 625/3086 [21:08<1:21:10,  1.98s/it] 20%|██        | 626/3086 [21:09<1:16:30,  1.87s/it] 20%|██        | 627/3086 [21:11<1:21:28,  1.99s/it] 20%|██        | 628/3086 [21:13<1:20:47,  1.97s/it] 20%|██        | 629/3086 [21:16<1:24:13,  2.06s/it] 20%|██        | 630/3086 [21:18<1:25:24,  2.09s/it]                                                    {'loss': 0.5264, 'grad_norm': 0.08040115237236023, 'learning_rate': 0.0004775113415424497, 'epoch': 0.2}
 20%|██        | 630/3086 [21:18<1:25:24,  2.09s/it] 20%|██        | 631/3086 [21:20<1:23:03,  2.03s/it] 20%|██        | 632/3086 [21:22<1:25:54,  2.10s/it] 21%|██        | 633/3086 [21:24<1:23:12,  2.04s/it] 21%|██        | 634/3086 [21:27<1:32:41,  2.27s/it] 21%|██        | 635/3086 [21:29<1:29:50,  2.20s/it] 21%|██        | 636/3086 [21:31<1:26:11,  2.11s/it] 21%|██        | 637/3086 [21:33<1:29:45,  2.20s/it] 21%|██        | 638/3086 [21:36<1:37:17,  2.38s/it] 21%|██        | 639/3086 [21:38<1:36:58,  2.38s/it] 21%|██        | 640/3086 [21:40<1:28:07,  2.16s/it]                                                    {'loss': 0.5244, 'grad_norm': 0.06500089168548584, 'learning_rate': 0.00047556707712248866, 'epoch': 0.21}
 21%|██        | 640/3086 [21:40<1:28:07,  2.16s/it] 21%|██        | 641/3086 [21:42<1:23:39,  2.05s/it] 21%|██        | 642/3086 [21:44<1:24:53,  2.08s/it] 21%|██        | 643/3086 [21:46<1:28:50,  2.18s/it] 21%|██        | 644/3086 [21:48<1:27:57,  2.16s/it] 21%|██        | 645/3086 [21:51<1:29:51,  2.21s/it] 21%|██        | 646/3086 [21:53<1:30:46,  2.23s/it] 21%|██        | 647/3086 [21:55<1:33:31,  2.30s/it] 21%|██        | 648/3086 [21:57<1:29:59,  2.21s/it] 21%|██        | 649/3086 [21:59<1:23:18,  2.05s/it] 21%|██        | 650/3086 [22:01<1:21:21,  2.00s/it]                                                    {'loss': 0.5303, 'grad_norm': 0.06513328105211258, 'learning_rate': 0.0004736228127025275, 'epoch': 0.21}
 21%|██        | 650/3086 [22:01<1:21:21,  2.00s/it] 21%|██        | 651/3086 [22:03<1:16:23,  1.88s/it] 21%|██        | 652/3086 [22:04<1:14:17,  1.83s/it] 21%|██        | 653/3086 [22:06<1:15:20,  1.86s/it] 21%|██        | 654/3086 [22:08<1:15:31,  1.86s/it] 21%|██        | 655/3086 [22:10<1:20:55,  2.00s/it] 21%|██▏       | 656/3086 [22:12<1:19:17,  1.96s/it] 21%|██▏       | 657/3086 [22:14<1:21:34,  2.02s/it] 21%|██▏       | 658/3086 [22:17<1:24:31,  2.09s/it] 21%|██▏       | 659/3086 [22:20<1:40:19,  2.48s/it] 21%|██▏       | 660/3086 [22:22<1:33:01,  2.30s/it]                                                    {'loss': 0.516, 'grad_norm': 0.06612399965524673, 'learning_rate': 0.0004716785482825664, 'epoch': 0.21}
 21%|██▏       | 660/3086 [22:22<1:33:01,  2.30s/it] 21%|██▏       | 661/3086 [22:24<1:27:43,  2.17s/it] 21%|██▏       | 662/3086 [22:26<1:22:23,  2.04s/it] 21%|██▏       | 663/3086 [22:27<1:18:15,  1.94s/it] 22%|██▏       | 664/3086 [22:29<1:20:33,  2.00s/it] 22%|██▏       | 665/3086 [22:31<1:21:35,  2.02s/it] 22%|██▏       | 666/3086 [22:33<1:19:04,  1.96s/it] 22%|██▏       | 667/3086 [22:36<1:26:52,  2.15s/it] 22%|██▏       | 668/3086 [22:39<1:36:18,  2.39s/it] 22%|██▏       | 669/3086 [22:41<1:34:52,  2.36s/it] 22%|██▏       | 670/3086 [22:43<1:29:41,  2.23s/it]                                                    {'loss': 0.5204, 'grad_norm': 0.06401524692773819, 'learning_rate': 0.0004697342838626053, 'epoch': 0.22}
 22%|██▏       | 670/3086 [22:43<1:29:41,  2.23s/it] 22%|██▏       | 671/3086 [22:45<1:27:19,  2.17s/it] 22%|██▏       | 672/3086 [22:47<1:23:44,  2.08s/it] 22%|██▏       | 673/3086 [22:49<1:24:14,  2.09s/it] 22%|██▏       | 674/3086 [22:51<1:21:11,  2.02s/it] 22%|██▏       | 675/3086 [22:53<1:23:41,  2.08s/it] 22%|██▏       | 676/3086 [22:55<1:20:11,  2.00s/it] 22%|██▏       | 677/3086 [22:57<1:16:27,  1.90s/it] 22%|██▏       | 678/3086 [22:59<1:15:46,  1.89s/it] 22%|██▏       | 679/3086 [23:00<1:16:25,  1.91s/it] 22%|██▏       | 680/3086 [23:02<1:15:13,  1.88s/it]                                                    {'loss': 0.5182, 'grad_norm': 0.0630761981010437, 'learning_rate': 0.00046779001944264416, 'epoch': 0.22}
 22%|██▏       | 680/3086 [23:02<1:15:13,  1.88s/it] 22%|██▏       | 681/3086 [23:04<1:13:28,  1.83s/it] 22%|██▏       | 682/3086 [23:06<1:11:48,  1.79s/it] 22%|██▏       | 683/3086 [23:08<1:17:05,  1.92s/it] 22%|██▏       | 684/3086 [23:10<1:13:23,  1.83s/it] 22%|██▏       | 685/3086 [23:11<1:11:09,  1.78s/it] 22%|██▏       | 686/3086 [23:13<1:13:10,  1.83s/it] 22%|██▏       | 687/3086 [23:15<1:13:10,  1.83s/it] 22%|██▏       | 688/3086 [23:17<1:15:47,  1.90s/it] 22%|██▏       | 689/3086 [23:19<1:11:45,  1.80s/it] 22%|██▏       | 690/3086 [23:21<1:21:23,  2.04s/it]                                                    {'loss': 0.5218, 'grad_norm': 0.0790516585111618, 'learning_rate': 0.00046584575502268304, 'epoch': 0.22}
 22%|██▏       | 690/3086 [23:21<1:21:23,  2.04s/it] 22%|██▏       | 691/3086 [23:23<1:19:57,  2.00s/it] 22%|██▏       | 692/3086 [23:25<1:23:12,  2.09s/it] 22%|██▏       | 693/3086 [23:27<1:18:25,  1.97s/it] 22%|██▏       | 694/3086 [23:29<1:20:06,  2.01s/it] 23%|██▎       | 695/3086 [23:31<1:18:59,  1.98s/it] 23%|██▎       | 696/3086 [23:33<1:17:37,  1.95s/it] 23%|██▎       | 697/3086 [23:35<1:16:08,  1.91s/it] 23%|██▎       | 698/3086 [23:37<1:17:32,  1.95s/it] 23%|██▎       | 699/3086 [23:39<1:17:28,  1.95s/it] 23%|██▎       | 700/3086 [23:41<1:18:25,  1.97s/it]                                                    {'loss': 0.5188, 'grad_norm': 0.06977978348731995, 'learning_rate': 0.00046390149060272193, 'epoch': 0.23}
 23%|██▎       | 700/3086 [23:41<1:18:25,  1.97s/it] 23%|██▎       | 701/3086 [23:43<1:18:19,  1.97s/it] 23%|██▎       | 702/3086 [23:45<1:18:48,  1.98s/it] 23%|██▎       | 703/3086 [23:47<1:18:37,  1.98s/it] 23%|██▎       | 704/3086 [23:49<1:17:50,  1.96s/it] 23%|██▎       | 705/3086 [23:51<1:16:42,  1.93s/it] 23%|██▎       | 706/3086 [23:53<1:22:00,  2.07s/it] 23%|██▎       | 707/3086 [23:55<1:23:14,  2.10s/it] 23%|██▎       | 708/3086 [23:57<1:19:53,  2.02s/it] 23%|██▎       | 709/3086 [23:59<1:20:22,  2.03s/it] 23%|██▎       | 710/3086 [24:01<1:19:39,  2.01s/it]                                                    {'loss': 0.5149, 'grad_norm': 0.07287944108247757, 'learning_rate': 0.0004619572261827608, 'epoch': 0.23}
 23%|██▎       | 710/3086 [24:01<1:19:39,  2.01s/it] 23%|██▎       | 711/3086 [24:03<1:20:02,  2.02s/it] 23%|██▎       | 712/3086 [24:05<1:18:58,  2.00s/it] 23%|██▎       | 713/3086 [24:07<1:22:37,  2.09s/it] 23%|██▎       | 714/3086 [24:09<1:19:06,  2.00s/it] 23%|██▎       | 715/3086 [24:11<1:16:29,  1.94s/it] 23%|██▎       | 716/3086 [24:12<1:12:51,  1.84s/it] 23%|██▎       | 717/3086 [24:14<1:12:19,  1.83s/it] 23%|██▎       | 718/3086 [24:16<1:13:37,  1.87s/it] 23%|██▎       | 719/3086 [24:18<1:13:05,  1.85s/it] 23%|██▎       | 720/3086 [24:20<1:12:52,  1.85s/it]                                                    {'loss': 0.5141, 'grad_norm': 0.06448560208082199, 'learning_rate': 0.00046001296176279965, 'epoch': 0.23}
 23%|██▎       | 720/3086 [24:20<1:12:52,  1.85s/it] 23%|██▎       | 721/3086 [24:22<1:14:37,  1.89s/it] 23%|██▎       | 722/3086 [24:25<1:23:43,  2.12s/it] 23%|██▎       | 723/3086 [24:26<1:18:07,  1.98s/it] 23%|██▎       | 724/3086 [24:28<1:20:29,  2.04s/it] 23%|██▎       | 725/3086 [24:30<1:16:24,  1.94s/it] 24%|██▎       | 726/3086 [24:32<1:13:47,  1.88s/it] 24%|██▎       | 727/3086 [24:34<1:16:07,  1.94s/it] 24%|██▎       | 728/3086 [24:36<1:14:14,  1.89s/it] 24%|██▎       | 729/3086 [24:38<1:16:44,  1.95s/it] 24%|██▎       | 730/3086 [24:40<1:19:05,  2.01s/it]                                                    {'loss': 0.5143, 'grad_norm': 0.06650219112634659, 'learning_rate': 0.0004580686973428386, 'epoch': 0.24}
 24%|██▎       | 730/3086 [24:40<1:19:05,  2.01s/it] 24%|██▎       | 731/3086 [24:42<1:21:20,  2.07s/it] 24%|██▎       | 732/3086 [24:44<1:19:56,  2.04s/it] 24%|██▍       | 733/3086 [24:46<1:24:14,  2.15s/it] 24%|██▍       | 734/3086 [24:49<1:25:33,  2.18s/it] 24%|██▍       | 735/3086 [24:51<1:29:19,  2.28s/it] 24%|██▍       | 736/3086 [24:53<1:23:18,  2.13s/it] 24%|██▍       | 737/3086 [24:55<1:20:30,  2.06s/it] 24%|██▍       | 738/3086 [24:57<1:22:31,  2.11s/it] 24%|██▍       | 739/3086 [24:59<1:20:40,  2.06s/it] 24%|██▍       | 740/3086 [25:01<1:17:19,  1.98s/it]                                                    {'loss': 0.5029, 'grad_norm': 0.06465443968772888, 'learning_rate': 0.0004561244329228775, 'epoch': 0.24}
 24%|██▍       | 740/3086 [25:01<1:17:19,  1.98s/it] 24%|██▍       | 741/3086 [25:03<1:15:55,  1.94s/it] 24%|██▍       | 742/3086 [25:05<1:16:50,  1.97s/it] 24%|██▍       | 743/3086 [25:07<1:14:48,  1.92s/it] 24%|██▍       | 744/3086 [25:09<1:15:36,  1.94s/it] 24%|██▍       | 745/3086 [25:11<1:15:57,  1.95s/it] 24%|██▍       | 746/3086 [25:12<1:16:06,  1.95s/it] 24%|██▍       | 747/3086 [25:14<1:17:03,  1.98s/it] 24%|██▍       | 748/3086 [25:17<1:22:10,  2.11s/it] 24%|██▍       | 749/3086 [25:19<1:16:31,  1.96s/it] 24%|██▍       | 750/3086 [25:21<1:17:26,  1.99s/it]                                                    {'loss': 0.4955, 'grad_norm': 0.0695321336388588, 'learning_rate': 0.00045418016850291636, 'epoch': 0.24}
 24%|██▍       | 750/3086 [25:21<1:17:26,  1.99s/it] 24%|██▍       | 751/3086 [25:22<1:15:51,  1.95s/it] 24%|██▍       | 752/3086 [25:25<1:21:48,  2.10s/it] 24%|██▍       | 753/3086 [25:27<1:18:00,  2.01s/it] 24%|██▍       | 754/3086 [25:29<1:21:46,  2.10s/it] 24%|██▍       | 755/3086 [25:31<1:19:08,  2.04s/it] 24%|██▍       | 756/3086 [25:33<1:16:16,  1.96s/it] 25%|██▍       | 757/3086 [25:35<1:14:45,  1.93s/it] 25%|██▍       | 758/3086 [25:36<1:10:09,  1.81s/it] 25%|██▍       | 759/3086 [25:39<1:17:44,  2.00s/it] 25%|██▍       | 760/3086 [25:41<1:21:08,  2.09s/it]                                                    {'loss': 0.5133, 'grad_norm': 0.06446520239114761, 'learning_rate': 0.00045223590408295525, 'epoch': 0.25}
 25%|██▍       | 760/3086 [25:41<1:21:08,  2.09s/it] 25%|██▍       | 761/3086 [25:43<1:23:08,  2.15s/it] 25%|██▍       | 762/3086 [25:46<1:27:15,  2.25s/it] 25%|██▍       | 763/3086 [25:48<1:23:30,  2.16s/it] 25%|██▍       | 764/3086 [25:49<1:18:40,  2.03s/it] 25%|██▍       | 765/3086 [25:51<1:14:16,  1.92s/it] 25%|██▍       | 766/3086 [25:53<1:16:41,  1.98s/it] 25%|██▍       | 767/3086 [25:55<1:14:33,  1.93s/it] 25%|██▍       | 768/3086 [25:57<1:15:57,  1.97s/it] 25%|██▍       | 769/3086 [25:59<1:19:04,  2.05s/it] 25%|██▍       | 770/3086 [26:01<1:15:21,  1.95s/it]                                                    {'loss': 0.5086, 'grad_norm': 0.05999006703495979, 'learning_rate': 0.0004502916396629941, 'epoch': 0.25}
 25%|██▍       | 770/3086 [26:01<1:15:21,  1.95s/it] 25%|██▍       | 771/3086 [26:03<1:14:32,  1.93s/it] 25%|██▌       | 772/3086 [26:06<1:26:02,  2.23s/it] 25%|██▌       | 773/3086 [26:08<1:26:27,  2.24s/it] 25%|██▌       | 774/3086 [26:09<1:16:12,  1.98s/it] 25%|██▌       | 775/3086 [26:12<1:18:44,  2.04s/it] 25%|██▌       | 776/3086 [26:14<1:19:24,  2.06s/it] 25%|██▌       | 777/3086 [26:15<1:14:40,  1.94s/it] 25%|██▌       | 778/3086 [26:17<1:13:11,  1.90s/it] 25%|██▌       | 779/3086 [26:19<1:13:00,  1.90s/it] 25%|██▌       | 780/3086 [26:21<1:10:38,  1.84s/it]                                                    {'loss': 0.4965, 'grad_norm': 0.06569834798574448, 'learning_rate': 0.000448347375243033, 'epoch': 0.25}
 25%|██▌       | 780/3086 [26:21<1:10:38,  1.84s/it] 25%|██▌       | 781/3086 [26:22<1:08:44,  1.79s/it] 25%|██▌       | 782/3086 [26:24<1:10:54,  1.85s/it] 25%|██▌       | 783/3086 [26:26<1:09:01,  1.80s/it] 25%|██▌       | 784/3086 [26:28<1:12:20,  1.89s/it] 25%|██▌       | 785/3086 [26:30<1:11:40,  1.87s/it] 25%|██▌       | 786/3086 [26:31<1:07:24,  1.76s/it] 26%|██▌       | 787/3086 [26:34<1:12:20,  1.89s/it] 26%|██▌       | 788/3086 [26:35<1:10:49,  1.85s/it] 26%|██▌       | 789/3086 [26:37<1:08:46,  1.80s/it] 26%|██▌       | 790/3086 [26:39<1:09:07,  1.81s/it]                                                    {'loss': 0.496, 'grad_norm': 0.06756346672773361, 'learning_rate': 0.0004464031108230719, 'epoch': 0.26}
 26%|██▌       | 790/3086 [26:39<1:09:07,  1.81s/it] 26%|██▌       | 791/3086 [26:41<1:08:23,  1.79s/it] 26%|██▌       | 792/3086 [26:42<1:07:56,  1.78s/it] 26%|██▌       | 793/3086 [26:45<1:13:16,  1.92s/it] 26%|██▌       | 794/3086 [26:47<1:14:02,  1.94s/it] 26%|██▌       | 795/3086 [26:49<1:17:45,  2.04s/it] 26%|██▌       | 796/3086 [26:51<1:23:19,  2.18s/it] 26%|██▌       | 797/3086 [26:53<1:18:15,  2.05s/it] 26%|██▌       | 798/3086 [26:56<1:23:04,  2.18s/it] 26%|██▌       | 799/3086 [26:58<1:19:50,  2.09s/it] 26%|██▌       | 800/3086 [27:00<1:23:07,  2.18s/it]                                                    {'loss': 0.4905, 'grad_norm': 0.0704025849699974, 'learning_rate': 0.00044445884640311075, 'epoch': 0.26}
 26%|██▌       | 800/3086 [27:00<1:23:07,  2.18s/it] 26%|██▌       | 801/3086 [27:02<1:20:09,  2.10s/it] 26%|██▌       | 802/3086 [27:04<1:15:36,  1.99s/it] 26%|██▌       | 803/3086 [27:06<1:18:13,  2.06s/it] 26%|██▌       | 804/3086 [27:07<1:12:21,  1.90s/it] 26%|██▌       | 805/3086 [27:09<1:14:56,  1.97s/it] 26%|██▌       | 806/3086 [27:12<1:22:12,  2.16s/it] 26%|██▌       | 807/3086 [27:14<1:16:12,  2.01s/it] 26%|██▌       | 808/3086 [27:15<1:13:41,  1.94s/it] 26%|██▌       | 809/3086 [27:17<1:11:12,  1.88s/it] 26%|██▌       | 810/3086 [27:19<1:11:59,  1.90s/it]                                                    {'loss': 0.4911, 'grad_norm': 0.08887659758329391, 'learning_rate': 0.0004425145819831497, 'epoch': 0.26}
 26%|██▌       | 810/3086 [27:19<1:11:59,  1.90s/it] 26%|██▋       | 811/3086 [27:22<1:21:40,  2.15s/it] 26%|██▋       | 812/3086 [27:24<1:19:38,  2.10s/it] 26%|██▋       | 813/3086 [27:26<1:19:20,  2.09s/it] 26%|██▋       | 814/3086 [27:28<1:19:08,  2.09s/it] 26%|██▋       | 815/3086 [27:30<1:17:44,  2.05s/it] 26%|██▋       | 816/3086 [27:32<1:18:43,  2.08s/it] 26%|██▋       | 817/3086 [27:34<1:13:37,  1.95s/it] 27%|██▋       | 818/3086 [27:36<1:13:27,  1.94s/it] 27%|██▋       | 819/3086 [27:38<1:13:56,  1.96s/it] 27%|██▋       | 820/3086 [27:40<1:16:42,  2.03s/it]                                                    {'loss': 0.4875, 'grad_norm': 0.07390104234218597, 'learning_rate': 0.0004405703175631885, 'epoch': 0.27}
 27%|██▋       | 820/3086 [27:40<1:16:42,  2.03s/it] 27%|██▋       | 821/3086 [27:42<1:17:29,  2.05s/it] 27%|██▋       | 822/3086 [27:44<1:12:44,  1.93s/it] 27%|██▋       | 823/3086 [27:46<1:16:53,  2.04s/it] 27%|██▋       | 824/3086 [27:48<1:15:02,  1.99s/it] 27%|██▋       | 825/3086 [27:50<1:18:51,  2.09s/it] 27%|██▋       | 826/3086 [27:52<1:15:14,  2.00s/it] 27%|██▋       | 827/3086 [27:54<1:20:47,  2.15s/it] 27%|██▋       | 828/3086 [27:57<1:25:58,  2.28s/it] 27%|██▋       | 829/3086 [27:59<1:21:42,  2.17s/it] 27%|██▋       | 830/3086 [28:01<1:24:28,  2.25s/it]                                                    {'loss': 0.4801, 'grad_norm': 0.059855878353118896, 'learning_rate': 0.0004386260531432274, 'epoch': 0.27}
 27%|██▋       | 830/3086 [28:01<1:24:28,  2.25s/it] 27%|██▋       | 831/3086 [28:03<1:22:40,  2.20s/it] 27%|██▋       | 832/3086 [28:07<1:35:32,  2.54s/it] 27%|██▋       | 833/3086 [28:08<1:25:35,  2.28s/it] 27%|██▋       | 834/3086 [28:11<1:22:59,  2.21s/it] 27%|██▋       | 835/3086 [28:13<1:20:25,  2.14s/it] 27%|██▋       | 836/3086 [28:15<1:18:48,  2.10s/it] 27%|██▋       | 837/3086 [28:16<1:15:19,  2.01s/it] 27%|██▋       | 838/3086 [28:19<1:17:25,  2.07s/it] 27%|██▋       | 839/3086 [28:21<1:24:11,  2.25s/it] 27%|██▋       | 840/3086 [28:23<1:20:13,  2.14s/it]                                                    {'loss': 0.489, 'grad_norm': 0.07370331883430481, 'learning_rate': 0.00043668178872326635, 'epoch': 0.27}
 27%|██▋       | 840/3086 [28:23<1:20:13,  2.14s/it] 27%|██▋       | 841/3086 [28:25<1:21:04,  2.17s/it] 27%|██▋       | 842/3086 [28:27<1:16:52,  2.06s/it] 27%|██▋       | 843/3086 [28:30<1:21:42,  2.19s/it] 27%|██▋       | 844/3086 [28:32<1:19:00,  2.11s/it] 27%|██▋       | 845/3086 [28:34<1:24:13,  2.25s/it] 27%|██▋       | 846/3086 [28:37<1:28:09,  2.36s/it] 27%|██▋       | 847/3086 [28:39<1:24:06,  2.25s/it] 27%|██▋       | 848/3086 [28:41<1:18:34,  2.11s/it] 28%|██▊       | 849/3086 [28:42<1:15:18,  2.02s/it] 28%|██▊       | 850/3086 [28:44<1:10:32,  1.89s/it]                                                    {'loss': 0.4801, 'grad_norm': 0.07432394474744797, 'learning_rate': 0.0004347375243033052, 'epoch': 0.28}
 28%|██▊       | 850/3086 [28:44<1:10:32,  1.89s/it] 28%|██▊       | 851/3086 [28:46<1:09:36,  1.87s/it] 28%|██▊       | 852/3086 [28:48<1:09:32,  1.87s/it] 28%|██▊       | 853/3086 [28:50<1:11:38,  1.92s/it] 28%|██▊       | 854/3086 [28:52<1:11:54,  1.93s/it] 28%|██▊       | 855/3086 [28:54<1:22:08,  2.21s/it] 28%|██▊       | 856/3086 [28:57<1:24:44,  2.28s/it] 28%|██▊       | 857/3086 [28:59<1:21:51,  2.20s/it] 28%|██▊       | 858/3086 [29:01<1:25:48,  2.31s/it] 28%|██▊       | 859/3086 [29:03<1:20:44,  2.18s/it] 28%|██▊       | 860/3086 [29:05<1:16:19,  2.06s/it]                                                    {'loss': 0.4838, 'grad_norm': 0.05807923153042793, 'learning_rate': 0.0004327932598833441, 'epoch': 0.28}
 28%|██▊       | 860/3086 [29:05<1:16:19,  2.06s/it] 28%|██▊       | 861/3086 [29:07<1:11:06,  1.92s/it] 28%|██▊       | 862/3086 [29:09<1:12:42,  1.96s/it] 28%|██▊       | 863/3086 [29:11<1:13:49,  1.99s/it] 28%|██▊       | 864/3086 [29:13<1:11:24,  1.93s/it] 28%|██▊       | 865/3086 [29:14<1:09:59,  1.89s/it] 28%|██▊       | 866/3086 [29:17<1:14:13,  2.01s/it] 28%|██▊       | 867/3086 [29:18<1:11:19,  1.93s/it] 28%|██▊       | 868/3086 [29:20<1:11:34,  1.94s/it] 28%|██▊       | 869/3086 [29:22<1:12:31,  1.96s/it] 28%|██▊       | 870/3086 [29:24<1:12:22,  1.96s/it]                                                    {'loss': 0.4879, 'grad_norm': 0.06516219675540924, 'learning_rate': 0.000430848995463383, 'epoch': 0.28}
 28%|██▊       | 870/3086 [29:24<1:12:22,  1.96s/it] 28%|██▊       | 871/3086 [29:26<1:10:34,  1.91s/it] 28%|██▊       | 872/3086 [29:28<1:11:15,  1.93s/it] 28%|██▊       | 873/3086 [29:30<1:11:27,  1.94s/it] 28%|██▊       | 874/3086 [29:32<1:15:11,  2.04s/it] 28%|██▊       | 875/3086 [29:34<1:14:18,  2.02s/it] 28%|██▊       | 876/3086 [29:37<1:25:00,  2.31s/it] 28%|██▊       | 877/3086 [29:40<1:23:32,  2.27s/it] 28%|██▊       | 878/3086 [29:42<1:24:31,  2.30s/it] 28%|██▊       | 879/3086 [29:44<1:17:44,  2.11s/it] 29%|██▊       | 880/3086 [29:46<1:16:28,  2.08s/it]                                                    {'loss': 0.4831, 'grad_norm': 0.07221923023462296, 'learning_rate': 0.00042890473104342184, 'epoch': 0.29}
 29%|██▊       | 880/3086 [29:46<1:16:28,  2.08s/it] 29%|██▊       | 881/3086 [29:48<1:14:49,  2.04s/it] 29%|██▊       | 882/3086 [29:49<1:09:41,  1.90s/it] 29%|██▊       | 883/3086 [29:51<1:12:11,  1.97s/it] 29%|██▊       | 884/3086 [29:53<1:13:17,  2.00s/it] 29%|██▊       | 885/3086 [29:56<1:22:08,  2.24s/it] 29%|██▊       | 886/3086 [29:58<1:14:01,  2.02s/it] 29%|██▊       | 887/3086 [30:00<1:21:54,  2.24s/it] 29%|██▉       | 888/3086 [30:03<1:26:26,  2.36s/it] 29%|██▉       | 889/3086 [30:05<1:19:30,  2.17s/it] 29%|██▉       | 890/3086 [30:07<1:22:51,  2.26s/it]                                                    {'loss': 0.4717, 'grad_norm': 0.06178714707493782, 'learning_rate': 0.0004269604666234608, 'epoch': 0.29}
 29%|██▉       | 890/3086 [30:07<1:22:51,  2.26s/it] 29%|██▉       | 891/3086 [30:09<1:19:13,  2.17s/it] 29%|██▉       | 892/3086 [30:11<1:17:04,  2.11s/it] 29%|██▉       | 893/3086 [30:13<1:14:43,  2.04s/it] 29%|██▉       | 894/3086 [30:15<1:12:45,  1.99s/it] 29%|██▉       | 895/3086 [30:17<1:10:04,  1.92s/it] 29%|██▉       | 896/3086 [30:20<1:23:07,  2.28s/it] 29%|██▉       | 897/3086 [30:22<1:25:09,  2.33s/it] 29%|██▉       | 898/3086 [30:24<1:22:02,  2.25s/it] 29%|██▉       | 899/3086 [30:26<1:20:26,  2.21s/it] 29%|██▉       | 900/3086 [30:29<1:21:15,  2.23s/it]                                                    {'loss': 0.4685, 'grad_norm': 0.07312183082103729, 'learning_rate': 0.0004250162022034996, 'epoch': 0.29}
 29%|██▉       | 900/3086 [30:29<1:21:15,  2.23s/it] 29%|██▉       | 901/3086 [30:30<1:15:44,  2.08s/it] 29%|██▉       | 902/3086 [30:32<1:12:13,  1.98s/it] 29%|██▉       | 903/3086 [30:34<1:11:36,  1.97s/it] 29%|██▉       | 904/3086 [30:36<1:08:24,  1.88s/it] 29%|██▉       | 905/3086 [30:38<1:10:24,  1.94s/it] 29%|██▉       | 906/3086 [30:40<1:07:48,  1.87s/it] 29%|██▉       | 907/3086 [30:42<1:14:56,  2.06s/it] 29%|██▉       | 908/3086 [30:45<1:23:35,  2.30s/it] 29%|██▉       | 909/3086 [30:47<1:17:20,  2.13s/it] 29%|██▉       | 910/3086 [30:49<1:16:53,  2.12s/it]                                                    {'loss': 0.4789, 'grad_norm': 0.07389624416828156, 'learning_rate': 0.0004230719377835385, 'epoch': 0.29}
 29%|██▉       | 910/3086 [30:49<1:16:53,  2.12s/it] 30%|██▉       | 911/3086 [30:51<1:17:15,  2.13s/it] 30%|██▉       | 912/3086 [30:53<1:13:39,  2.03s/it] 30%|██▉       | 913/3086 [30:55<1:13:05,  2.02s/it] 30%|██▉       | 914/3086 [30:57<1:12:47,  2.01s/it] 30%|██▉       | 915/3086 [30:58<1:09:27,  1.92s/it] 30%|██▉       | 916/3086 [31:01<1:14:03,  2.05s/it] 30%|██▉       | 917/3086 [31:02<1:10:11,  1.94s/it] 30%|██▉       | 918/3086 [31:04<1:08:31,  1.90s/it] 30%|██▉       | 919/3086 [31:06<1:08:53,  1.91s/it] 30%|██▉       | 920/3086 [31:08<1:05:50,  1.82s/it]                                                    {'loss': 0.4684, 'grad_norm': 0.09716537594795227, 'learning_rate': 0.00042112767336357744, 'epoch': 0.3}
 30%|██▉       | 920/3086 [31:08<1:05:50,  1.82s/it] 30%|██▉       | 921/3086 [31:09<1:04:43,  1.79s/it] 30%|██▉       | 922/3086 [31:11<1:04:55,  1.80s/it] 30%|██▉       | 923/3086 [31:13<1:08:09,  1.89s/it] 30%|██▉       | 924/3086 [31:16<1:11:58,  2.00s/it] 30%|██▉       | 925/3086 [31:18<1:11:32,  1.99s/it] 30%|███       | 926/3086 [31:20<1:11:19,  1.98s/it] 30%|███       | 927/3086 [31:22<1:13:00,  2.03s/it] 30%|███       | 928/3086 [31:24<1:14:12,  2.06s/it] 30%|███       | 929/3086 [31:26<1:14:07,  2.06s/it] 30%|███       | 930/3086 [31:28<1:13:48,  2.05s/it]                                                    {'loss': 0.4593, 'grad_norm': 0.06618838012218475, 'learning_rate': 0.0004191834089436163, 'epoch': 0.3}
 30%|███       | 930/3086 [31:28<1:13:48,  2.05s/it] 30%|███       | 931/3086 [31:30<1:09:20,  1.93s/it] 30%|███       | 932/3086 [31:31<1:08:37,  1.91s/it] 30%|███       | 933/3086 [31:34<1:11:19,  1.99s/it] 30%|███       | 934/3086 [31:36<1:18:07,  2.18s/it] 30%|███       | 935/3086 [31:38<1:17:34,  2.16s/it] 30%|███       | 936/3086 [31:40<1:16:33,  2.14s/it] 30%|███       | 937/3086 [31:42<1:10:58,  1.98s/it] 30%|███       | 938/3086 [31:44<1:08:16,  1.91s/it] 30%|███       | 939/3086 [31:46<1:10:03,  1.96s/it] 30%|███       | 940/3086 [31:48<1:12:42,  2.03s/it]                                                    {'loss': 0.4652, 'grad_norm': 0.06897801160812378, 'learning_rate': 0.0004172391445236552, 'epoch': 0.3}
 30%|███       | 940/3086 [31:48<1:12:42,  2.03s/it] 30%|███       | 941/3086 [31:51<1:16:54,  2.15s/it] 31%|███       | 942/3086 [31:53<1:23:42,  2.34s/it] 31%|███       | 943/3086 [31:55<1:20:43,  2.26s/it] 31%|███       | 944/3086 [31:58<1:20:06,  2.24s/it] 31%|███       | 945/3086 [31:59<1:15:19,  2.11s/it] 31%|███       | 946/3086 [32:02<1:16:38,  2.15s/it] 31%|███       | 947/3086 [32:03<1:13:12,  2.05s/it] 31%|███       | 948/3086 [32:06<1:13:35,  2.07s/it] 31%|███       | 949/3086 [32:07<1:10:45,  1.99s/it] 31%|███       | 950/3086 [32:10<1:14:17,  2.09s/it]                                                    {'loss': 0.4547, 'grad_norm': 0.06864991784095764, 'learning_rate': 0.00041529488010369405, 'epoch': 0.31}
 31%|███       | 950/3086 [32:10<1:14:17,  2.09s/it] 31%|███       | 951/3086 [32:12<1:15:00,  2.11s/it] 31%|███       | 952/3086 [32:14<1:19:51,  2.25s/it] 31%|███       | 953/3086 [32:16<1:15:37,  2.13s/it] 31%|███       | 954/3086 [32:18<1:14:00,  2.08s/it] 31%|███       | 955/3086 [32:20<1:12:04,  2.03s/it] 31%|███       | 956/3086 [32:22<1:15:29,  2.13s/it] 31%|███       | 957/3086 [32:25<1:15:59,  2.14s/it] 31%|███       | 958/3086 [32:27<1:17:50,  2.19s/it] 31%|███       | 959/3086 [32:29<1:16:06,  2.15s/it] 31%|███       | 960/3086 [32:31<1:16:40,  2.16s/it]                                                    {'loss': 0.4439, 'grad_norm': 0.06421409547328949, 'learning_rate': 0.00041335061568373294, 'epoch': 0.31}
 31%|███       | 960/3086 [32:31<1:16:40,  2.16s/it] 31%|███       | 961/3086 [32:34<1:19:51,  2.25s/it] 31%|███       | 962/3086 [32:36<1:15:18,  2.13s/it] 31%|███       | 963/3086 [32:38<1:15:41,  2.14s/it] 31%|███       | 964/3086 [32:39<1:10:56,  2.01s/it] 31%|███▏      | 965/3086 [32:42<1:14:06,  2.10s/it] 31%|███▏      | 966/3086 [32:44<1:17:07,  2.18s/it] 31%|███▏      | 967/3086 [32:46<1:18:11,  2.21s/it] 31%|███▏      | 968/3086 [32:48<1:12:49,  2.06s/it] 31%|███▏      | 969/3086 [32:50<1:07:55,  1.92s/it] 31%|███▏      | 970/3086 [32:52<1:09:46,  1.98s/it]                                                    {'loss': 0.4411, 'grad_norm': 0.0796741172671318, 'learning_rate': 0.0004114063512637719, 'epoch': 0.31}
 31%|███▏      | 970/3086 [32:52<1:09:46,  1.98s/it] 31%|███▏      | 971/3086 [32:53<1:07:08,  1.90s/it] 31%|███▏      | 972/3086 [32:55<1:05:53,  1.87s/it] 32%|███▏      | 973/3086 [32:57<1:04:02,  1.82s/it] 32%|███▏      | 974/3086 [32:59<1:09:35,  1.98s/it] 32%|███▏      | 975/3086 [33:01<1:05:01,  1.85s/it] 32%|███▏      | 976/3086 [33:03<1:07:06,  1.91s/it] 32%|███▏      | 977/3086 [33:05<1:08:33,  1.95s/it] 32%|███▏      | 978/3086 [33:07<1:09:22,  1.97s/it] 32%|███▏      | 979/3086 [33:09<1:08:01,  1.94s/it] 32%|███▏      | 980/3086 [33:11<1:05:46,  1.87s/it]                                                    {'loss': 0.4259, 'grad_norm': 0.07705405354499817, 'learning_rate': 0.0004094620868438107, 'epoch': 0.32}
 32%|███▏      | 980/3086 [33:11<1:05:46,  1.87s/it] 32%|███▏      | 981/3086 [33:13<1:09:05,  1.97s/it] 32%|███▏      | 982/3086 [33:15<1:11:43,  2.05s/it] 32%|███▏      | 983/3086 [33:17<1:12:50,  2.08s/it] 32%|███▏      | 984/3086 [33:19<1:09:15,  1.98s/it] 32%|███▏      | 985/3086 [33:21<1:10:02,  2.00s/it] 32%|███▏      | 986/3086 [33:23<1:09:46,  1.99s/it] 32%|███▏      | 987/3086 [33:25<1:10:45,  2.02s/it] 32%|███▏      | 988/3086 [33:27<1:06:55,  1.91s/it] 32%|███▏      | 989/3086 [33:29<1:07:07,  1.92s/it] 32%|███▏      | 990/3086 [33:30<1:03:01,  1.80s/it]                                                    {'loss': 0.4214, 'grad_norm': 0.06879791617393494, 'learning_rate': 0.0004075178224238496, 'epoch': 0.32}
 32%|███▏      | 990/3086 [33:30<1:03:01,  1.80s/it] 32%|███▏      | 991/3086 [33:32<1:06:19,  1.90s/it] 32%|███▏      | 992/3086 [33:34<1:04:45,  1.86s/it] 32%|███▏      | 993/3086 [33:36<1:07:06,  1.92s/it] 32%|███▏      | 994/3086 [33:38<1:07:42,  1.94s/it] 32%|███▏      | 995/3086 [33:40<1:04:51,  1.86s/it] 32%|███▏      | 996/3086 [33:42<1:06:57,  1.92s/it] 32%|███▏      | 997/3086 [33:44<1:06:20,  1.91s/it] 32%|███▏      | 998/3086 [33:46<1:11:56,  2.07s/it] 32%|███▏      | 999/3086 [33:48<1:08:16,  1.96s/it] 32%|███▏      | 1000/3086 [33:49<1:04:21,  1.85s/it]                                                     {'loss': 0.3996, 'grad_norm': 0.07008315622806549, 'learning_rate': 0.0004055735580038885, 'epoch': 0.32}
 32%|███▏      | 1000/3086 [33:49<1:04:21,  1.85s/it] 32%|███▏      | 1001/3086 [33:52<1:08:57,  1.98s/it] 32%|███▏      | 1002/3086 [33:54<1:13:32,  2.12s/it] 33%|███▎      | 1003/3086 [33:56<1:08:18,  1.97s/it] 33%|███▎      | 1004/3086 [33:57<1:04:25,  1.86s/it] 33%|███▎      | 1005/3086 [33:59<1:06:05,  1.91s/it] 33%|███▎      | 1006/3086 [34:02<1:08:52,  1.99s/it] 33%|███▎      | 1007/3086 [34:04<1:13:52,  2.13s/it] 33%|███▎      | 1008/3086 [34:06<1:14:06,  2.14s/it] 33%|███▎      | 1009/3086 [34:08<1:13:38,  2.13s/it] 33%|███▎      | 1010/3086 [34:10<1:11:47,  2.07s/it]                                                     {'loss': 0.3835, 'grad_norm': 0.061622150242328644, 'learning_rate': 0.00040362929358392737, 'epoch': 0.33}
 33%|███▎      | 1010/3086 [34:10<1:11:47,  2.07s/it] 33%|███▎      | 1011/3086 [34:12<1:07:27,  1.95s/it] 33%|███▎      | 1012/3086 [34:14<1:11:41,  2.07s/it] 33%|███▎      | 1013/3086 [34:16<1:07:43,  1.96s/it] 33%|███▎      | 1014/3086 [34:18<1:04:17,  1.86s/it] 33%|███▎      | 1015/3086 [34:20<1:13:15,  2.12s/it] 33%|███▎      | 1016/3086 [34:22<1:07:57,  1.97s/it] 33%|███▎      | 1017/3086 [34:24<1:09:57,  2.03s/it] 33%|███▎      | 1018/3086 [34:26<1:07:29,  1.96s/it] 33%|███▎      | 1019/3086 [34:28<1:07:56,  1.97s/it] 33%|███▎      | 1020/3086 [34:30<1:08:53,  2.00s/it]                                                     {'loss': 0.3599, 'grad_norm': 0.07198424637317657, 'learning_rate': 0.0004016850291639662, 'epoch': 0.33}
 33%|███▎      | 1020/3086 [34:30<1:08:53,  2.00s/it] 33%|███▎      | 1021/3086 [34:32<1:05:15,  1.90s/it] 33%|███▎      | 1022/3086 [34:34<1:09:52,  2.03s/it] 33%|███▎      | 1023/3086 [34:36<1:06:58,  1.95s/it] 33%|███▎      | 1024/3086 [34:38<1:09:39,  2.03s/it] 33%|███▎      | 1025/3086 [34:40<1:06:46,  1.94s/it] 33%|███▎      | 1026/3086 [34:41<1:04:20,  1.87s/it] 33%|███▎      | 1027/3086 [34:43<1:02:15,  1.81s/it] 33%|███▎      | 1028/3086 [34:45<1:05:52,  1.92s/it] 33%|███▎      | 1029/3086 [34:48<1:10:06,  2.05s/it] 33%|███▎      | 1030/3086 [34:49<1:05:48,  1.92s/it]                                                     {'loss': 0.3425, 'grad_norm': 0.0688219964504242, 'learning_rate': 0.00039974076474400515, 'epoch': 0.33}
 33%|███▎      | 1030/3086 [34:49<1:05:48,  1.92s/it] 33%|███▎      | 1031/3086 [34:51<1:07:45,  1.98s/it] 33%|███▎      | 1032/3086 [34:53<1:06:43,  1.95s/it] 33%|███▎      | 1033/3086 [34:55<1:04:00,  1.87s/it] 34%|███▎      | 1034/3086 [34:57<1:10:05,  2.05s/it] 34%|███▎      | 1035/3086 [35:00<1:11:05,  2.08s/it] 34%|███▎      | 1036/3086 [35:01<1:09:00,  2.02s/it] 34%|███▎      | 1037/3086 [35:04<1:10:38,  2.07s/it] 34%|███▎      | 1038/3086 [35:05<1:05:40,  1.92s/it] 34%|███▎      | 1039/3086 [35:07<1:02:11,  1.82s/it] 34%|███▎      | 1040/3086 [35:09<1:06:38,  1.95s/it]                                                     {'loss': 0.3279, 'grad_norm': 0.07423383742570877, 'learning_rate': 0.00039779650032404403, 'epoch': 0.34}
 34%|███▎      | 1040/3086 [35:09<1:06:38,  1.95s/it] 34%|███▎      | 1041/3086 [35:11<1:05:26,  1.92s/it] 34%|███▍      | 1042/3086 [35:13<1:09:37,  2.04s/it] 34%|███▍      | 1043/3086 [35:15<1:06:30,  1.95s/it] 34%|███▍      | 1044/3086 [35:17<1:06:28,  1.95s/it] 34%|███▍      | 1045/3086 [35:19<1:07:52,  2.00s/it] 34%|███▍      | 1046/3086 [35:21<1:08:48,  2.02s/it] 34%|███▍      | 1047/3086 [35:23<1:07:43,  1.99s/it] 34%|███▍      | 1048/3086 [35:25<1:08:28,  2.02s/it] 34%|███▍      | 1049/3086 [35:27<1:06:34,  1.96s/it] 34%|███▍      | 1050/3086 [35:29<1:08:30,  2.02s/it]                                                     {'loss': 0.3156, 'grad_norm': 0.060126569122076035, 'learning_rate': 0.000395852235904083, 'epoch': 0.34}
 34%|███▍      | 1050/3086 [35:29<1:08:30,  2.02s/it] 34%|███▍      | 1051/3086 [35:31<1:04:18,  1.90s/it] 34%|███▍      | 1052/3086 [35:32<1:02:45,  1.85s/it] 34%|███▍      | 1053/3086 [35:34<1:02:37,  1.85s/it] 34%|███▍      | 1054/3086 [35:36<59:22,  1.75s/it]   34%|███▍      | 1055/3086 [35:38<1:08:52,  2.03s/it] 34%|███▍      | 1056/3086 [35:40<1:03:48,  1.89s/it] 34%|███▍      | 1057/3086 [35:42<1:07:16,  1.99s/it] 34%|███▍      | 1058/3086 [35:44<1:08:02,  2.01s/it] 34%|███▍      | 1059/3086 [35:47<1:19:58,  2.37s/it] 34%|███▍      | 1060/3086 [35:50<1:17:14,  2.29s/it]                                                     {'loss': 0.3064, 'grad_norm': 0.065083809196949, 'learning_rate': 0.0003939079714841218, 'epoch': 0.34}
 34%|███▍      | 1060/3086 [35:50<1:17:14,  2.29s/it] 34%|███▍      | 1061/3086 [35:51<1:11:08,  2.11s/it] 34%|███▍      | 1062/3086 [35:53<1:10:48,  2.10s/it] 34%|███▍      | 1063/3086 [35:55<1:06:28,  1.97s/it] 34%|███▍      | 1064/3086 [35:58<1:15:46,  2.25s/it] 35%|███▍      | 1065/3086 [36:00<1:11:45,  2.13s/it] 35%|███▍      | 1066/3086 [36:01<1:06:55,  1.99s/it] 35%|███▍      | 1067/3086 [36:03<1:06:57,  1.99s/it] 35%|███▍      | 1068/3086 [36:06<1:11:49,  2.14s/it] 35%|███▍      | 1069/3086 [36:08<1:08:59,  2.05s/it] 35%|███▍      | 1070/3086 [36:10<1:08:56,  2.05s/it]                                                     {'loss': 0.2979, 'grad_norm': 0.06237224489450455, 'learning_rate': 0.0003919637070641607, 'epoch': 0.35}
 35%|███▍      | 1070/3086 [36:10<1:08:56,  2.05s/it] 35%|███▍      | 1071/3086 [36:12<1:10:07,  2.09s/it] 35%|███▍      | 1072/3086 [36:14<1:05:53,  1.96s/it] 35%|███▍      | 1073/3086 [36:16<1:07:39,  2.02s/it] 35%|███▍      | 1074/3086 [36:18<1:09:09,  2.06s/it] 35%|███▍      | 1075/3086 [36:20<1:03:49,  1.90s/it] 35%|███▍      | 1076/3086 [36:21<1:03:03,  1.88s/it] 35%|███▍      | 1077/3086 [36:24<1:05:53,  1.97s/it] 35%|███▍      | 1078/3086 [36:25<1:05:09,  1.95s/it] 35%|███▍      | 1079/3086 [36:27<1:02:58,  1.88s/it] 35%|███▍      | 1080/3086 [36:29<1:04:12,  1.92s/it]                                                     {'loss': 0.2879, 'grad_norm': 0.05828271061182022, 'learning_rate': 0.0003900194426441996, 'epoch': 0.35}
 35%|███▍      | 1080/3086 [36:29<1:04:12,  1.92s/it] 35%|███▌      | 1081/3086 [36:31<1:06:53,  2.00s/it] 35%|███▌      | 1082/3086 [36:34<1:10:12,  2.10s/it] 35%|███▌      | 1083/3086 [36:36<1:11:50,  2.15s/it] 35%|███▌      | 1084/3086 [36:38<1:08:14,  2.05s/it] 35%|███▌      | 1085/3086 [36:40<1:14:16,  2.23s/it] 35%|███▌      | 1086/3086 [36:42<1:10:49,  2.12s/it] 35%|███▌      | 1087/3086 [36:44<1:11:31,  2.15s/it] 35%|███▌      | 1088/3086 [36:47<1:12:01,  2.16s/it] 35%|███▌      | 1089/3086 [36:49<1:10:47,  2.13s/it] 35%|███▌      | 1090/3086 [36:51<1:09:00,  2.07s/it]                                                     {'loss': 0.2821, 'grad_norm': 0.05877915397286415, 'learning_rate': 0.00038807517822423847, 'epoch': 0.35}
 35%|███▌      | 1090/3086 [36:51<1:09:00,  2.07s/it] 35%|███▌      | 1091/3086 [36:53<1:06:45,  2.01s/it] 35%|███▌      | 1092/3086 [36:55<1:08:56,  2.07s/it] 35%|███▌      | 1093/3086 [36:58<1:17:30,  2.33s/it] 35%|███▌      | 1094/3086 [37:00<1:14:46,  2.25s/it] 35%|███▌      | 1095/3086 [37:02<1:10:45,  2.13s/it] 36%|███▌      | 1096/3086 [37:04<1:12:19,  2.18s/it] 36%|███▌      | 1097/3086 [37:06<1:10:33,  2.13s/it] 36%|███▌      | 1098/3086 [37:08<1:10:04,  2.12s/it] 36%|███▌      | 1099/3086 [37:11<1:14:38,  2.25s/it] 36%|███▌      | 1100/3086 [37:12<1:08:26,  2.07s/it]                                                     {'loss': 0.2757, 'grad_norm': 0.06963738054037094, 'learning_rate': 0.0003861309138042773, 'epoch': 0.36}
 36%|███▌      | 1100/3086 [37:12<1:08:26,  2.07s/it] 36%|███▌      | 1101/3086 [37:14<1:05:58,  1.99s/it] 36%|███▌      | 1102/3086 [37:16<1:09:01,  2.09s/it] 36%|███▌      | 1103/3086 [37:18<1:07:15,  2.04s/it] 36%|███▌      | 1104/3086 [37:20<1:09:05,  2.09s/it] 36%|███▌      | 1105/3086 [37:23<1:08:49,  2.08s/it] 36%|███▌      | 1106/3086 [37:25<1:09:30,  2.11s/it] 36%|███▌      | 1107/3086 [37:27<1:09:32,  2.11s/it] 36%|███▌      | 1108/3086 [37:28<1:05:13,  1.98s/it] 36%|███▌      | 1109/3086 [37:31<1:06:14,  2.01s/it] 36%|███▌      | 1110/3086 [37:33<1:06:18,  2.01s/it]                                                     {'loss': 0.2714, 'grad_norm': 0.06597556173801422, 'learning_rate': 0.00038418664938431624, 'epoch': 0.36}
 36%|███▌      | 1110/3086 [37:33<1:06:18,  2.01s/it] 36%|███▌      | 1111/3086 [37:35<1:05:28,  1.99s/it] 36%|███▌      | 1112/3086 [37:37<1:05:53,  2.00s/it] 36%|███▌      | 1113/3086 [37:38<1:03:49,  1.94s/it] 36%|███▌      | 1114/3086 [37:41<1:10:02,  2.13s/it] 36%|███▌      | 1115/3086 [37:43<1:13:10,  2.23s/it] 36%|███▌      | 1116/3086 [37:45<1:07:42,  2.06s/it] 36%|███▌      | 1117/3086 [37:48<1:15:18,  2.29s/it] 36%|███▌      | 1118/3086 [37:50<1:11:19,  2.17s/it] 36%|███▋      | 1119/3086 [37:52<1:08:15,  2.08s/it] 36%|███▋      | 1120/3086 [37:54<1:07:44,  2.07s/it]                                                     {'loss': 0.2711, 'grad_norm': 0.05973467603325844, 'learning_rate': 0.00038224238496435513, 'epoch': 0.36}
 36%|███▋      | 1120/3086 [37:54<1:07:44,  2.07s/it] 36%|███▋      | 1121/3086 [37:56<1:07:15,  2.05s/it] 36%|███▋      | 1122/3086 [37:57<1:04:17,  1.96s/it] 36%|███▋      | 1123/3086 [37:59<1:04:24,  1.97s/it] 36%|███▋      | 1124/3086 [38:02<1:08:27,  2.09s/it] 36%|███▋      | 1125/3086 [38:04<1:06:14,  2.03s/it] 36%|███▋      | 1126/3086 [38:06<1:05:10,  1.99s/it] 37%|███▋      | 1127/3086 [38:07<1:03:48,  1.95s/it] 37%|███▋      | 1128/3086 [38:10<1:04:48,  1.99s/it] 37%|███▋      | 1129/3086 [38:11<1:02:15,  1.91s/it] 37%|███▋      | 1130/3086 [38:13<1:01:57,  1.90s/it]                                                     {'loss': 0.2637, 'grad_norm': 0.056215353310108185, 'learning_rate': 0.000380298120544394, 'epoch': 0.37}
 37%|███▋      | 1130/3086 [38:13<1:01:57,  1.90s/it] 37%|███▋      | 1131/3086 [38:15<1:05:19,  2.00s/it] 37%|███▋      | 1132/3086 [38:17<1:02:01,  1.90s/it] 37%|███▋      | 1133/3086 [38:19<1:00:28,  1.86s/it] 37%|███▋      | 1134/3086 [38:21<1:00:29,  1.86s/it] 37%|███▋      | 1135/3086 [38:23<1:05:04,  2.00s/it] 37%|███▋      | 1136/3086 [38:25<1:01:17,  1.89s/it] 37%|███▋      | 1137/3086 [38:27<1:01:47,  1.90s/it] 37%|███▋      | 1138/3086 [38:29<1:03:08,  1.95s/it] 37%|███▋      | 1139/3086 [38:31<1:06:57,  2.06s/it] 37%|███▋      | 1140/3086 [38:33<1:11:14,  2.20s/it]                                                     {'loss': 0.257, 'grad_norm': 0.058183107525110245, 'learning_rate': 0.0003783538561244329, 'epoch': 0.37}
 37%|███▋      | 1140/3086 [38:33<1:11:14,  2.20s/it] 37%|███▋      | 1141/3086 [38:36<1:11:43,  2.21s/it] 37%|███▋      | 1142/3086 [38:38<1:09:11,  2.14s/it] 37%|███▋      | 1143/3086 [38:39<1:06:01,  2.04s/it] 37%|███▋      | 1144/3086 [38:42<1:06:45,  2.06s/it] 37%|███▋      | 1145/3086 [38:44<1:05:40,  2.03s/it] 37%|███▋      | 1146/3086 [38:45<1:03:35,  1.97s/it] 37%|███▋      | 1147/3086 [38:47<1:03:40,  1.97s/it] 37%|███▋      | 1148/3086 [38:49<1:01:50,  1.91s/it] 37%|███▋      | 1149/3086 [38:51<58:27,  1.81s/it]   37%|███▋      | 1150/3086 [38:53<1:04:59,  2.01s/it]                                                     {'loss': 0.2574, 'grad_norm': 0.05828888714313507, 'learning_rate': 0.00037640959170447174, 'epoch': 0.37}
 37%|███▋      | 1150/3086 [38:53<1:04:59,  2.01s/it] 37%|███▋      | 1151/3086 [38:55<1:04:15,  1.99s/it] 37%|███▋      | 1152/3086 [38:58<1:12:20,  2.24s/it] 37%|███▋      | 1153/3086 [39:00<1:07:16,  2.09s/it] 37%|███▋      | 1154/3086 [39:02<1:06:04,  2.05s/it] 37%|███▋      | 1155/3086 [39:04<1:10:04,  2.18s/it] 37%|███▋      | 1156/3086 [39:06<1:08:20,  2.12s/it] 37%|███▋      | 1157/3086 [39:08<1:06:41,  2.07s/it] 38%|███▊      | 1158/3086 [39:10<1:05:57,  2.05s/it] 38%|███▊      | 1159/3086 [39:12<1:07:58,  2.12s/it] 38%|███▊      | 1160/3086 [39:15<1:11:39,  2.23s/it]                                                     {'loss': 0.2519, 'grad_norm': 0.05814695730805397, 'learning_rate': 0.0003744653272845107, 'epoch': 0.38}
 38%|███▊      | 1160/3086 [39:15<1:11:39,  2.23s/it] 38%|███▊      | 1161/3086 [39:16<1:04:43,  2.02s/it] 38%|███▊      | 1162/3086 [39:19<1:07:52,  2.12s/it] 38%|███▊      | 1163/3086 [39:20<1:03:19,  1.98s/it] 38%|███▊      | 1164/3086 [39:22<1:03:16,  1.98s/it] 38%|███▊      | 1165/3086 [39:24<1:04:22,  2.01s/it] 38%|███▊      | 1166/3086 [39:27<1:11:19,  2.23s/it] 38%|███▊      | 1167/3086 [39:29<1:11:45,  2.24s/it] 38%|███▊      | 1168/3086 [39:31<1:07:10,  2.10s/it] 38%|███▊      | 1169/3086 [39:34<1:13:59,  2.32s/it] 38%|███▊      | 1170/3086 [39:36<1:07:10,  2.10s/it]                                                     {'loss': 0.2548, 'grad_norm': 0.05651695281267166, 'learning_rate': 0.00037252106286454956, 'epoch': 0.38}
 38%|███▊      | 1170/3086 [39:36<1:07:10,  2.10s/it] 38%|███▊      | 1171/3086 [39:38<1:05:06,  2.04s/it] 38%|███▊      | 1172/3086 [39:41<1:15:09,  2.36s/it] 38%|███▊      | 1173/3086 [39:42<1:08:44,  2.16s/it] 38%|███▊      | 1174/3086 [39:44<1:05:32,  2.06s/it] 38%|███▊      | 1175/3086 [39:46<1:07:28,  2.12s/it] 38%|███▊      | 1176/3086 [39:48<1:05:52,  2.07s/it] 38%|███▊      | 1177/3086 [39:50<1:02:52,  1.98s/it] 38%|███▊      | 1178/3086 [39:52<1:04:12,  2.02s/it] 38%|███▊      | 1179/3086 [39:54<1:01:19,  1.93s/it] 38%|███▊      | 1180/3086 [39:56<58:08,  1.83s/it]                                                     {'loss': 0.2387, 'grad_norm': 0.04838228225708008, 'learning_rate': 0.0003705767984445884, 'epoch': 0.38}
 38%|███▊      | 1180/3086 [39:56<58:08,  1.83s/it] 38%|███▊      | 1181/3086 [39:57<57:00,  1.80s/it] 38%|███▊      | 1182/3086 [39:59<55:53,  1.76s/it] 38%|███▊      | 1183/3086 [40:01<56:01,  1.77s/it] 38%|███▊      | 1184/3086 [40:03<56:51,  1.79s/it] 38%|███▊      | 1185/3086 [40:04<56:20,  1.78s/it] 38%|███▊      | 1186/3086 [40:07<1:03:05,  1.99s/it] 38%|███▊      | 1187/3086 [40:09<1:01:11,  1.93s/it] 38%|███▊      | 1188/3086 [40:11<1:01:53,  1.96s/it] 39%|███▊      | 1189/3086 [40:12<1:00:35,  1.92s/it] 39%|███▊      | 1190/3086 [40:15<1:08:06,  2.16s/it]                                                     {'loss': 0.237, 'grad_norm': 0.05302882939577103, 'learning_rate': 0.00036863253402462734, 'epoch': 0.39}
 39%|███▊      | 1190/3086 [40:15<1:08:06,  2.16s/it] 39%|███▊      | 1191/3086 [40:17<1:07:55,  2.15s/it] 39%|███▊      | 1192/3086 [40:20<1:08:35,  2.17s/it] 39%|███▊      | 1193/3086 [40:22<1:07:26,  2.14s/it] 39%|███▊      | 1194/3086 [40:23<1:03:36,  2.02s/it] 39%|███▊      | 1195/3086 [40:25<1:04:22,  2.04s/it] 39%|███▉      | 1196/3086 [40:27<1:02:36,  1.99s/it] 39%|███▉      | 1197/3086 [40:29<1:04:32,  2.05s/it] 39%|███▉      | 1198/3086 [40:31<1:02:04,  1.97s/it] 39%|███▉      | 1199/3086 [40:33<1:01:21,  1.95s/it] 39%|███▉      | 1200/3086 [40:35<1:01:37,  1.96s/it]                                                     {'loss': 0.2378, 'grad_norm': 0.04552532359957695, 'learning_rate': 0.00036668826960466617, 'epoch': 0.39}
 39%|███▉      | 1200/3086 [40:35<1:01:37,  1.96s/it] 39%|███▉      | 1201/3086 [40:38<1:05:32,  2.09s/it] 39%|███▉      | 1202/3086 [40:40<1:12:48,  2.32s/it] 39%|███▉      | 1203/3086 [40:43<1:11:44,  2.29s/it] 39%|███▉      | 1204/3086 [40:45<1:10:36,  2.25s/it] 39%|███▉      | 1205/3086 [40:47<1:07:17,  2.15s/it] 39%|███▉      | 1206/3086 [40:48<1:03:56,  2.04s/it] 39%|███▉      | 1207/3086 [40:50<1:03:33,  2.03s/it] 39%|███▉      | 1208/3086 [40:52<1:00:49,  1.94s/it] 39%|███▉      | 1209/3086 [40:54<1:00:46,  1.94s/it] 39%|███▉      | 1210/3086 [40:56<1:02:08,  1.99s/it]                                                     {'loss': 0.2414, 'grad_norm': 0.05547724664211273, 'learning_rate': 0.00036474400518470506, 'epoch': 0.39}
 39%|███▉      | 1210/3086 [40:56<1:02:08,  1.99s/it] 39%|███▉      | 1211/3086 [40:58<1:03:13,  2.02s/it] 39%|███▉      | 1212/3086 [41:00<1:00:11,  1.93s/it] 39%|███▉      | 1213/3086 [41:02<1:00:31,  1.94s/it] 39%|███▉      | 1214/3086 [41:04<59:45,  1.92s/it]   39%|███▉      | 1215/3086 [41:06<1:02:47,  2.01s/it] 39%|███▉      | 1216/3086 [41:08<1:01:10,  1.96s/it] 39%|███▉      | 1217/3086 [41:10<1:01:42,  1.98s/it] 39%|███▉      | 1218/3086 [41:12<1:05:00,  2.09s/it] 40%|███▉      | 1219/3086 [41:15<1:05:48,  2.11s/it] 40%|███▉      | 1220/3086 [41:17<1:12:18,  2.33s/it]                                                     {'loss': 0.2399, 'grad_norm': 0.0591430626809597, 'learning_rate': 0.000362799740764744, 'epoch': 0.4}
 40%|███▉      | 1220/3086 [41:17<1:12:18,  2.33s/it] 40%|███▉      | 1221/3086 [41:19<1:09:20,  2.23s/it] 40%|███▉      | 1222/3086 [41:22<1:09:43,  2.24s/it] 40%|███▉      | 1223/3086 [41:23<1:05:10,  2.10s/it] 40%|███▉      | 1224/3086 [41:26<1:09:23,  2.24s/it] 40%|███▉      | 1225/3086 [41:28<1:06:36,  2.15s/it] 40%|███▉      | 1226/3086 [41:30<1:07:40,  2.18s/it] 40%|███▉      | 1227/3086 [41:32<1:08:23,  2.21s/it] 40%|███▉      | 1228/3086 [41:35<1:07:56,  2.19s/it] 40%|███▉      | 1229/3086 [41:36<1:03:03,  2.04s/it] 40%|███▉      | 1230/3086 [41:39<1:05:47,  2.13s/it]                                                     {'loss': 0.2372, 'grad_norm': 0.05112684145569801, 'learning_rate': 0.00036085547634478283, 'epoch': 0.4}
 40%|███▉      | 1230/3086 [41:39<1:05:47,  2.13s/it] 40%|███▉      | 1231/3086 [41:41<1:07:11,  2.17s/it] 40%|███▉      | 1232/3086 [41:43<1:09:54,  2.26s/it] 40%|███▉      | 1233/3086 [41:45<1:06:55,  2.17s/it] 40%|███▉      | 1234/3086 [41:47<1:05:27,  2.12s/it] 40%|████      | 1235/3086 [41:49<1:00:58,  1.98s/it] 40%|████      | 1236/3086 [41:51<1:03:33,  2.06s/it] 40%|████      | 1237/3086 [41:53<1:03:06,  2.05s/it] 40%|████      | 1238/3086 [41:55<1:00:52,  1.98s/it] 40%|████      | 1239/3086 [41:57<1:01:35,  2.00s/it] 40%|████      | 1240/3086 [42:00<1:07:15,  2.19s/it]                                                     {'loss': 0.2363, 'grad_norm': 0.06393703073263168, 'learning_rate': 0.0003589112119248218, 'epoch': 0.4}
 40%|████      | 1240/3086 [42:00<1:07:15,  2.19s/it] 40%|████      | 1241/3086 [42:02<1:07:00,  2.18s/it] 40%|████      | 1242/3086 [42:04<1:02:22,  2.03s/it] 40%|████      | 1243/3086 [42:06<1:04:35,  2.10s/it] 40%|████      | 1244/3086 [42:08<1:05:04,  2.12s/it] 40%|████      | 1245/3086 [42:10<1:01:58,  2.02s/it] 40%|████      | 1246/3086 [42:12<1:01:26,  2.00s/it] 40%|████      | 1247/3086 [42:13<57:29,  1.88s/it]   40%|████      | 1248/3086 [42:15<55:34,  1.81s/it] 40%|████      | 1249/3086 [42:17<57:28,  1.88s/it] 41%|████      | 1250/3086 [42:19<1:00:02,  1.96s/it]                                                     {'loss': 0.2351, 'grad_norm': 0.049682874232530594, 'learning_rate': 0.00035696694750486066, 'epoch': 0.41}
 41%|████      | 1250/3086 [42:19<1:00:02,  1.96s/it] 41%|████      | 1251/3086 [42:21<58:56,  1.93s/it]   41%|████      | 1252/3086 [42:23<57:56,  1.90s/it] 41%|████      | 1253/3086 [42:25<56:22,  1.85s/it] 41%|████      | 1254/3086 [42:27<1:02:17,  2.04s/it] 41%|████      | 1255/3086 [42:29<1:02:23,  2.04s/it] 41%|████      | 1256/3086 [42:32<1:10:44,  2.32s/it] 41%|████      | 1257/3086 [42:34<1:10:42,  2.32s/it] 41%|████      | 1258/3086 [42:36<1:08:12,  2.24s/it] 41%|████      | 1259/3086 [42:39<1:11:13,  2.34s/it] 41%|████      | 1260/3086 [42:41<1:07:52,  2.23s/it]                                                     {'loss': 0.2321, 'grad_norm': 0.05113866180181503, 'learning_rate': 0.0003550226830848995, 'epoch': 0.41}
 41%|████      | 1260/3086 [42:41<1:07:52,  2.23s/it] 41%|████      | 1261/3086 [42:43<1:03:49,  2.10s/it] 41%|████      | 1262/3086 [42:45<1:01:23,  2.02s/it] 41%|████      | 1263/3086 [42:46<59:17,  1.95s/it]   41%|████      | 1264/3086 [42:49<1:02:19,  2.05s/it] 41%|████      | 1265/3086 [42:51<1:04:11,  2.12s/it] 41%|████      | 1266/3086 [42:53<1:00:28,  1.99s/it] 41%|████      | 1267/3086 [42:54<57:50,  1.91s/it]   41%|████      | 1268/3086 [42:56<57:54,  1.91s/it] 41%|████      | 1269/3086 [42:59<1:01:23,  2.03s/it] 41%|████      | 1270/3086 [43:00<59:45,  1.97s/it]                                                     {'loss': 0.2324, 'grad_norm': 0.048408325761556625, 'learning_rate': 0.00035307841866493843, 'epoch': 0.41}
 41%|████      | 1270/3086 [43:00<59:45,  1.97s/it] 41%|████      | 1271/3086 [43:02<58:42,  1.94s/it] 41%|████      | 1272/3086 [43:04<59:51,  1.98s/it] 41%|████▏     | 1273/3086 [43:06<59:36,  1.97s/it] 41%|████▏     | 1274/3086 [43:09<1:01:33,  2.04s/it] 41%|████▏     | 1275/3086 [43:11<1:05:44,  2.18s/it] 41%|████▏     | 1276/3086 [43:13<1:01:49,  2.05s/it] 41%|████▏     | 1277/3086 [43:15<1:01:28,  2.04s/it] 41%|████▏     | 1278/3086 [43:17<1:02:44,  2.08s/it] 41%|████▏     | 1279/3086 [43:20<1:10:16,  2.33s/it] 41%|████▏     | 1280/3086 [43:22<1:05:12,  2.17s/it]                                                     {'loss': 0.2289, 'grad_norm': 0.051200419664382935, 'learning_rate': 0.00035113415424497727, 'epoch': 0.41}
 41%|████▏     | 1280/3086 [43:22<1:05:12,  2.17s/it] 42%|████▏     | 1281/3086 [43:24<1:05:56,  2.19s/it] 42%|████▏     | 1282/3086 [43:26<1:09:22,  2.31s/it] 42%|████▏     | 1283/3086 [43:29<1:09:18,  2.31s/it] 42%|████▏     | 1284/3086 [43:31<1:12:32,  2.42s/it] 42%|████▏     | 1285/3086 [43:34<1:11:55,  2.40s/it] 42%|████▏     | 1286/3086 [43:36<1:07:05,  2.24s/it] 42%|████▏     | 1287/3086 [43:38<1:11:43,  2.39s/it] 42%|████▏     | 1288/3086 [43:40<1:05:28,  2.18s/it] 42%|████▏     | 1289/3086 [43:42<1:00:57,  2.04s/it] 42%|████▏     | 1290/3086 [43:44<58:29,  1.95s/it]                                                     {'loss': 0.2285, 'grad_norm': 0.04707909747958183, 'learning_rate': 0.00034918988982501615, 'epoch': 0.42}
 42%|████▏     | 1290/3086 [43:44<58:29,  1.95s/it] 42%|████▏     | 1291/3086 [43:45<56:26,  1.89s/it] 42%|████▏     | 1292/3086 [43:47<55:49,  1.87s/it] 42%|████▏     | 1293/3086 [43:49<54:40,  1.83s/it] 42%|████▏     | 1294/3086 [43:50<52:07,  1.75s/it] 42%|████▏     | 1295/3086 [43:52<51:36,  1.73s/it] 42%|████▏     | 1296/3086 [43:54<49:17,  1.65s/it] 42%|████▏     | 1297/3086 [43:56<58:22,  1.96s/it] 42%|████▏     | 1298/3086 [43:58<57:12,  1.92s/it] 42%|████▏     | 1299/3086 [44:00<58:41,  1.97s/it] 42%|████▏     | 1300/3086 [44:03<1:07:39,  2.27s/it]                                                     {'loss': 0.2329, 'grad_norm': 0.04506285861134529, 'learning_rate': 0.0003472456254050551, 'epoch': 0.42}
 42%|████▏     | 1300/3086 [44:03<1:07:39,  2.27s/it] 42%|████▏     | 1301/3086 [44:05<1:03:42,  2.14s/it] 42%|████▏     | 1302/3086 [44:08<1:07:05,  2.26s/it] 42%|████▏     | 1303/3086 [44:09<1:01:39,  2.07s/it] 42%|████▏     | 1304/3086 [44:11<57:58,  1.95s/it]   42%|████▏     | 1305/3086 [44:13<1:01:10,  2.06s/it] 42%|████▏     | 1306/3086 [44:16<1:03:56,  2.16s/it] 42%|████▏     | 1307/3086 [44:18<1:03:59,  2.16s/it] 42%|████▏     | 1308/3086 [44:20<1:02:25,  2.11s/it] 42%|████▏     | 1309/3086 [44:22<1:05:31,  2.21s/it] 42%|████▏     | 1310/3086 [44:24<1:01:38,  2.08s/it]                                                     {'loss': 0.2269, 'grad_norm': 0.04379282146692276, 'learning_rate': 0.00034530136098509393, 'epoch': 0.42}
 42%|████▏     | 1310/3086 [44:24<1:01:38,  2.08s/it] 42%|████▏     | 1311/3086 [44:26<57:57,  1.96s/it]   43%|████▎     | 1312/3086 [44:28<58:53,  1.99s/it] 43%|████▎     | 1313/3086 [44:30<1:02:05,  2.10s/it] 43%|████▎     | 1314/3086 [44:32<1:00:16,  2.04s/it] 43%|████▎     | 1315/3086 [44:34<57:21,  1.94s/it]   43%|████▎     | 1316/3086 [44:36<1:00:52,  2.06s/it] 43%|████▎     | 1317/3086 [44:38<58:48,  1.99s/it]   43%|████▎     | 1318/3086 [44:39<55:46,  1.89s/it] 43%|████▎     | 1319/3086 [44:41<56:37,  1.92s/it] 43%|████▎     | 1320/3086 [44:44<1:00:41,  2.06s/it]                                                     {'loss': 0.2265, 'grad_norm': 0.055616844445466995, 'learning_rate': 0.00034335709656513287, 'epoch': 0.43}
 43%|████▎     | 1320/3086 [44:44<1:00:41,  2.06s/it] 43%|████▎     | 1321/3086 [44:46<58:53,  2.00s/it]   43%|████▎     | 1322/3086 [44:47<54:17,  1.85s/it] 43%|████▎     | 1323/3086 [44:49<58:22,  1.99s/it] 43%|████▎     | 1324/3086 [44:52<1:06:45,  2.27s/it] 43%|████▎     | 1325/3086 [44:54<1:00:48,  2.07s/it] 43%|████▎     | 1326/3086 [44:56<57:09,  1.95s/it]   43%|████▎     | 1327/3086 [44:58<59:43,  2.04s/it] 43%|████▎     | 1328/3086 [45:00<58:32,  2.00s/it] 43%|████▎     | 1329/3086 [45:02<57:21,  1.96s/it] 43%|████▎     | 1330/3086 [45:04<59:22,  2.03s/it]                                                   {'loss': 0.2312, 'grad_norm': 0.07547254860401154, 'learning_rate': 0.0003414128321451717, 'epoch': 0.43}
 43%|████▎     | 1330/3086 [45:04<59:22,  2.03s/it] 43%|████▎     | 1331/3086 [45:06<58:44,  2.01s/it] 43%|████▎     | 1332/3086 [45:09<1:07:08,  2.30s/it] 43%|████▎     | 1333/3086 [45:11<1:03:19,  2.17s/it] 43%|████▎     | 1334/3086 [45:13<1:04:03,  2.19s/it] 43%|████▎     | 1335/3086 [45:15<1:02:48,  2.15s/it] 43%|████▎     | 1336/3086 [45:17<1:03:37,  2.18s/it] 43%|████▎     | 1337/3086 [45:19<1:02:26,  2.14s/it] 43%|████▎     | 1338/3086 [45:21<1:00:15,  2.07s/it] 43%|████▎     | 1339/3086 [45:24<1:05:33,  2.25s/it] 43%|████▎     | 1340/3086 [45:27<1:09:40,  2.39s/it]                                                     {'loss': 0.227, 'grad_norm': 0.04937813803553581, 'learning_rate': 0.0003394685677252106, 'epoch': 0.43}
 43%|████▎     | 1340/3086 [45:27<1:09:40,  2.39s/it] 43%|████▎     | 1341/3086 [45:29<1:12:52,  2.51s/it] 43%|████▎     | 1342/3086 [45:31<1:06:32,  2.29s/it] 44%|████▎     | 1343/3086 [45:34<1:07:18,  2.32s/it] 44%|████▎     | 1344/3086 [45:36<1:06:47,  2.30s/it] 44%|████▎     | 1345/3086 [45:38<1:07:05,  2.31s/it] 44%|████▎     | 1346/3086 [45:41<1:09:17,  2.39s/it] 44%|████▎     | 1347/3086 [45:42<1:03:32,  2.19s/it] 44%|████▎     | 1348/3086 [45:44<1:01:33,  2.13s/it] 44%|████▎     | 1349/3086 [45:46<57:59,  2.00s/it]   44%|████▎     | 1350/3086 [45:48<57:37,  1.99s/it]                                                   {'loss': 0.2206, 'grad_norm': 0.045610975474119186, 'learning_rate': 0.00033752430330524953, 'epoch': 0.44}
 44%|████▎     | 1350/3086 [45:48<57:37,  1.99s/it] 44%|████▍     | 1351/3086 [45:51<1:03:30,  2.20s/it] 44%|████▍     | 1352/3086 [45:53<1:04:14,  2.22s/it] 44%|████▍     | 1353/3086 [45:55<1:01:17,  2.12s/it] 44%|████▍     | 1354/3086 [45:57<58:21,  2.02s/it]   44%|████▍     | 1355/3086 [45:59<57:15,  1.98s/it] 44%|████▍     | 1356/3086 [46:01<1:01:43,  2.14s/it] 44%|████▍     | 1357/3086 [46:03<1:03:20,  2.20s/it] 44%|████▍     | 1358/3086 [46:05<1:00:49,  2.11s/it] 44%|████▍     | 1359/3086 [46:07<58:32,  2.03s/it]   44%|████▍     | 1360/3086 [46:10<1:00:58,  2.12s/it]                                                     {'loss': 0.2204, 'grad_norm': 0.04083685204386711, 'learning_rate': 0.00033558003888528836, 'epoch': 0.44}
 44%|████▍     | 1360/3086 [46:10<1:00:58,  2.12s/it] 44%|████▍     | 1361/3086 [46:12<59:54,  2.08s/it]   44%|████▍     | 1362/3086 [46:14<1:01:16,  2.13s/it] 44%|████▍     | 1363/3086 [46:16<57:57,  2.02s/it]   44%|████▍     | 1364/3086 [46:17<56:36,  1.97s/it] 44%|████▍     | 1365/3086 [46:19<57:19,  2.00s/it] 44%|████▍     | 1366/3086 [46:21<55:18,  1.93s/it] 44%|████▍     | 1367/3086 [46:23<52:41,  1.84s/it] 44%|████▍     | 1368/3086 [46:25<52:24,  1.83s/it] 44%|████▍     | 1369/3086 [46:27<54:00,  1.89s/it] 44%|████▍     | 1370/3086 [46:29<56:52,  1.99s/it]                                                   {'loss': 0.2251, 'grad_norm': 0.04267379641532898, 'learning_rate': 0.00033363577446532725, 'epoch': 0.44}
 44%|████▍     | 1370/3086 [46:29<56:52,  1.99s/it] 44%|████▍     | 1371/3086 [46:31<55:39,  1.95s/it] 44%|████▍     | 1372/3086 [46:33<1:00:57,  2.13s/it] 44%|████▍     | 1373/3086 [46:36<1:08:33,  2.40s/it] 45%|████▍     | 1374/3086 [46:38<1:04:42,  2.27s/it] 45%|████▍     | 1375/3086 [46:41<1:07:56,  2.38s/it] 45%|████▍     | 1376/3086 [46:43<1:03:28,  2.23s/it] 45%|████▍     | 1377/3086 [46:45<1:05:04,  2.28s/it] 45%|████▍     | 1378/3086 [46:47<1:00:21,  2.12s/it] 45%|████▍     | 1379/3086 [46:49<58:02,  2.04s/it]   45%|████▍     | 1380/3086 [46:51<57:24,  2.02s/it]                                                   {'loss': 0.2213, 'grad_norm': 0.04753373935818672, 'learning_rate': 0.00033169151004536614, 'epoch': 0.45}
 45%|████▍     | 1380/3086 [46:51<57:24,  2.02s/it] 45%|████▍     | 1381/3086 [46:53<54:49,  1.93s/it] 45%|████▍     | 1382/3086 [46:55<56:13,  1.98s/it] 45%|████▍     | 1383/3086 [46:57<55:50,  1.97s/it] 45%|████▍     | 1384/3086 [46:58<54:22,  1.92s/it] 45%|████▍     | 1385/3086 [47:00<51:40,  1.82s/it] 45%|████▍     | 1386/3086 [47:02<52:44,  1.86s/it] 45%|████▍     | 1387/3086 [47:05<1:01:13,  2.16s/it] 45%|████▍     | 1388/3086 [47:07<1:03:49,  2.26s/it] 45%|████▌     | 1389/3086 [47:09<58:42,  2.08s/it]   45%|████▌     | 1390/3086 [47:11<57:13,  2.02s/it]                                                   {'loss': 0.2221, 'grad_norm': 0.05381644144654274, 'learning_rate': 0.000329747245625405, 'epoch': 0.45}
 45%|████▌     | 1390/3086 [47:11<57:13,  2.02s/it] 45%|████▌     | 1391/3086 [47:13<59:32,  2.11s/it] 45%|████▌     | 1392/3086 [47:15<58:12,  2.06s/it] 45%|████▌     | 1393/3086 [47:17<56:27,  2.00s/it] 45%|████▌     | 1394/3086 [47:19<54:51,  1.95s/it] 45%|████▌     | 1395/3086 [47:21<54:56,  1.95s/it] 45%|████▌     | 1396/3086 [47:23<1:00:43,  2.16s/it] 45%|████▌     | 1397/3086 [47:26<1:05:14,  2.32s/it] 45%|████▌     | 1398/3086 [47:28<1:02:08,  2.21s/it] 45%|████▌     | 1399/3086 [47:30<1:01:15,  2.18s/it] 45%|████▌     | 1400/3086 [47:33<1:04:12,  2.28s/it]                                                     {'loss': 0.2203, 'grad_norm': 0.04424333572387695, 'learning_rate': 0.00032780298120544386, 'epoch': 0.45}
 45%|████▌     | 1400/3086 [47:33<1:04:12,  2.28s/it] 45%|████▌     | 1401/3086 [47:35<1:05:43,  2.34s/it] 45%|████▌     | 1402/3086 [47:37<1:02:01,  2.21s/it] 45%|████▌     | 1403/3086 [47:39<58:35,  2.09s/it]   45%|████▌     | 1404/3086 [47:41<56:56,  2.03s/it] 46%|████▌     | 1405/3086 [47:43<56:42,  2.02s/it] 46%|████▌     | 1406/3086 [47:45<57:58,  2.07s/it] 46%|████▌     | 1407/3086 [47:47<57:42,  2.06s/it] 46%|████▌     | 1408/3086 [47:49<59:14,  2.12s/it] 46%|████▌     | 1409/3086 [47:51<53:51,  1.93s/it] 46%|████▌     | 1410/3086 [47:53<56:44,  2.03s/it]                                                   {'loss': 0.2205, 'grad_norm': 0.052624206990003586, 'learning_rate': 0.0003258587167854828, 'epoch': 0.46}
 46%|████▌     | 1410/3086 [47:53<56:44,  2.03s/it] 46%|████▌     | 1411/3086 [47:55<53:41,  1.92s/it] 46%|████▌     | 1412/3086 [47:56<53:00,  1.90s/it] 46%|████▌     | 1413/3086 [47:59<55:19,  1.98s/it] 46%|████▌     | 1414/3086 [48:01<57:00,  2.05s/it] 46%|████▌     | 1415/3086 [48:03<57:00,  2.05s/it] 46%|████▌     | 1416/3086 [48:05<54:51,  1.97s/it] 46%|████▌     | 1417/3086 [48:07<58:23,  2.10s/it] 46%|████▌     | 1418/3086 [48:09<55:16,  1.99s/it] 46%|████▌     | 1419/3086 [48:11<55:09,  1.99s/it] 46%|████▌     | 1420/3086 [48:13<53:50,  1.94s/it]                                                   {'loss': 0.2206, 'grad_norm': 0.04202616959810257, 'learning_rate': 0.0003239144523655217, 'epoch': 0.46}
 46%|████▌     | 1420/3086 [48:13<53:50,  1.94s/it] 46%|████▌     | 1421/3086 [48:15<57:22,  2.07s/it] 46%|████▌     | 1422/3086 [48:17<54:48,  1.98s/it] 46%|████▌     | 1423/3086 [48:19<55:16,  1.99s/it] 46%|████▌     | 1424/3086 [48:21<56:28,  2.04s/it] 46%|████▌     | 1425/3086 [48:23<55:03,  1.99s/it] 46%|████▌     | 1426/3086 [48:25<52:39,  1.90s/it] 46%|████▌     | 1427/3086 [48:27<53:20,  1.93s/it] 46%|████▋     | 1428/3086 [48:29<55:17,  2.00s/it] 46%|████▋     | 1429/3086 [48:31<54:43,  1.98s/it] 46%|████▋     | 1430/3086 [48:32<53:14,  1.93s/it]                                                   {'loss': 0.219, 'grad_norm': 0.043895136564970016, 'learning_rate': 0.00032197018794556057, 'epoch': 0.46}
 46%|████▋     | 1430/3086 [48:32<53:14,  1.93s/it] 46%|████▋     | 1431/3086 [48:35<57:22,  2.08s/it] 46%|████▋     | 1432/3086 [48:37<56:54,  2.06s/it] 46%|████▋     | 1433/3086 [48:39<54:09,  1.97s/it] 46%|████▋     | 1434/3086 [48:41<54:18,  1.97s/it] 47%|████▋     | 1435/3086 [48:43<54:57,  2.00s/it] 47%|████▋     | 1436/3086 [48:45<58:10,  2.12s/it] 47%|████▋     | 1437/3086 [48:47<55:00,  2.00s/it] 47%|████▋     | 1438/3086 [48:49<57:53,  2.11s/it] 47%|████▋     | 1439/3086 [48:52<1:00:10,  2.19s/it] 47%|████▋     | 1440/3086 [48:54<59:34,  2.17s/it]                                                     {'loss': 0.2238, 'grad_norm': 0.05337149277329445, 'learning_rate': 0.00032002592352559946, 'epoch': 0.47}
 47%|████▋     | 1440/3086 [48:54<59:34,  2.17s/it] 47%|████▋     | 1441/3086 [48:56<58:14,  2.12s/it] 47%|████▋     | 1442/3086 [48:58<56:59,  2.08s/it] 47%|████▋     | 1443/3086 [49:00<57:24,  2.10s/it] 47%|████▋     | 1444/3086 [49:02<57:44,  2.11s/it] 47%|████▋     | 1445/3086 [49:04<57:47,  2.11s/it] 47%|████▋     | 1446/3086 [49:06<55:45,  2.04s/it] 47%|████▋     | 1447/3086 [49:08<56:54,  2.08s/it] 47%|████▋     | 1448/3086 [49:10<56:51,  2.08s/it] 47%|████▋     | 1449/3086 [49:12<58:30,  2.14s/it] 47%|████▋     | 1450/3086 [49:15<1:02:58,  2.31s/it]                                                     {'loss': 0.2218, 'grad_norm': 0.057791948318481445, 'learning_rate': 0.00031808165910563835, 'epoch': 0.47}
 47%|████▋     | 1450/3086 [49:15<1:02:58,  2.31s/it] 47%|████▋     | 1451/3086 [49:17<59:38,  2.19s/it]   47%|████▋     | 1452/3086 [49:19<56:32,  2.08s/it] 47%|████▋     | 1453/3086 [49:21<56:32,  2.08s/it] 47%|████▋     | 1454/3086 [49:23<56:34,  2.08s/it] 47%|████▋     | 1455/3086 [49:25<54:54,  2.02s/it] 47%|████▋     | 1456/3086 [49:27<54:04,  1.99s/it] 47%|████▋     | 1457/3086 [49:28<49:34,  1.83s/it] 47%|████▋     | 1458/3086 [49:30<50:16,  1.85s/it] 47%|████▋     | 1459/3086 [49:32<50:11,  1.85s/it] 47%|████▋     | 1460/3086 [49:34<52:53,  1.95s/it]                                                   {'loss': 0.2195, 'grad_norm': 0.04254775121808052, 'learning_rate': 0.00031613739468567723, 'epoch': 0.47}
 47%|████▋     | 1460/3086 [49:34<52:53,  1.95s/it] 47%|████▋     | 1461/3086 [49:37<57:16,  2.11s/it] 47%|████▋     | 1462/3086 [49:39<57:13,  2.11s/it] 47%|████▋     | 1463/3086 [49:41<54:22,  2.01s/it] 47%|████▋     | 1464/3086 [49:43<54:24,  2.01s/it] 47%|████▋     | 1465/3086 [49:45<57:34,  2.13s/it] 48%|████▊     | 1466/3086 [49:47<54:06,  2.00s/it] 48%|████▊     | 1467/3086 [49:49<53:55,  2.00s/it] 48%|████▊     | 1468/3086 [49:51<53:35,  1.99s/it] 48%|████▊     | 1469/3086 [49:53<52:45,  1.96s/it] 48%|████▊     | 1470/3086 [49:55<53:02,  1.97s/it]                                                   {'loss': 0.2225, 'grad_norm': 0.04633913189172745, 'learning_rate': 0.0003141931302657161, 'epoch': 0.48}
 48%|████▊     | 1470/3086 [49:55<53:02,  1.97s/it] 48%|████▊     | 1471/3086 [49:56<51:16,  1.90s/it] 48%|████▊     | 1472/3086 [49:58<53:00,  1.97s/it] 48%|████▊     | 1473/3086 [50:01<54:23,  2.02s/it] 48%|████▊     | 1474/3086 [50:03<53:39,  2.00s/it] 48%|████▊     | 1475/3086 [50:04<51:17,  1.91s/it] 48%|████▊     | 1476/3086 [50:06<54:00,  2.01s/it] 48%|████▊     | 1477/3086 [50:08<51:10,  1.91s/it] 48%|████▊     | 1478/3086 [50:11<55:46,  2.08s/it] 48%|████▊     | 1479/3086 [50:13<56:25,  2.11s/it] 48%|████▊     | 1480/3086 [50:15<58:40,  2.19s/it]                                                   {'loss': 0.2145, 'grad_norm': 0.056007467210292816, 'learning_rate': 0.00031224886584575495, 'epoch': 0.48}
 48%|████▊     | 1480/3086 [50:15<58:40,  2.19s/it] 48%|████▊     | 1481/3086 [50:17<55:42,  2.08s/it] 48%|████▊     | 1482/3086 [50:19<57:02,  2.13s/it] 48%|████▊     | 1483/3086 [50:21<53:35,  2.01s/it] 48%|████▊     | 1484/3086 [50:23<50:36,  1.90s/it] 48%|████▊     | 1485/3086 [50:24<49:19,  1.85s/it] 48%|████▊     | 1486/3086 [50:26<48:41,  1.83s/it] 48%|████▊     | 1487/3086 [50:28<50:09,  1.88s/it] 48%|████▊     | 1488/3086 [50:30<50:05,  1.88s/it] 48%|████▊     | 1489/3086 [50:32<49:11,  1.85s/it] 48%|████▊     | 1490/3086 [50:34<52:48,  1.99s/it]                                                   {'loss': 0.2191, 'grad_norm': 0.04165903478860855, 'learning_rate': 0.0003103046014257939, 'epoch': 0.48}
 48%|████▊     | 1490/3086 [50:34<52:48,  1.99s/it] 48%|████▊     | 1491/3086 [50:36<54:21,  2.04s/it] 48%|████▊     | 1492/3086 [50:38<55:03,  2.07s/it] 48%|████▊     | 1493/3086 [50:41<57:09,  2.15s/it] 48%|████▊     | 1494/3086 [50:43<56:57,  2.15s/it] 48%|████▊     | 1495/3086 [50:45<52:46,  1.99s/it] 48%|████▊     | 1496/3086 [50:47<54:26,  2.05s/it] 49%|████▊     | 1497/3086 [50:49<53:03,  2.00s/it] 49%|████▊     | 1498/3086 [50:51<53:41,  2.03s/it] 49%|████▊     | 1499/3086 [50:53<57:37,  2.18s/it] 49%|████▊     | 1500/3086 [50:55<53:39,  2.03s/it]                                                   {'loss': 0.2139, 'grad_norm': 0.044828664511442184, 'learning_rate': 0.0003083603370058328, 'epoch': 0.49}
 49%|████▊     | 1500/3086 [50:55<53:39,  2.03s/it] 49%|████▊     | 1501/3086 [50:57<54:43,  2.07s/it] 49%|████▊     | 1502/3086 [50:59<52:56,  2.01s/it] 49%|████▊     | 1503/3086 [51:01<52:43,  2.00s/it] 49%|████▊     | 1504/3086 [51:03<53:13,  2.02s/it] 49%|████▉     | 1505/3086 [51:05<57:06,  2.17s/it] 49%|████▉     | 1506/3086 [51:08<56:31,  2.15s/it] 49%|████▉     | 1507/3086 [51:09<53:42,  2.04s/it] 49%|████▉     | 1508/3086 [51:11<51:17,  1.95s/it] 49%|████▉     | 1509/3086 [51:13<51:21,  1.95s/it] 49%|████▉     | 1510/3086 [51:15<50:46,  1.93s/it]                                                   {'loss': 0.2126, 'grad_norm': 0.042263634502887726, 'learning_rate': 0.0003064160725858716, 'epoch': 0.49}
 49%|████▉     | 1510/3086 [51:15<50:46,  1.93s/it] 49%|████▉     | 1511/3086 [51:18<55:29,  2.11s/it] 49%|████▉     | 1512/3086 [51:20<56:21,  2.15s/it] 49%|████▉     | 1513/3086 [51:21<52:53,  2.02s/it] 49%|████▉     | 1514/3086 [51:23<50:04,  1.91s/it] 49%|████▉     | 1515/3086 [51:25<49:13,  1.88s/it] 49%|████▉     | 1516/3086 [51:27<51:45,  1.98s/it] 49%|████▉     | 1517/3086 [51:29<52:20,  2.00s/it] 49%|████▉     | 1518/3086 [51:31<52:58,  2.03s/it] 49%|████▉     | 1519/3086 [51:34<54:50,  2.10s/it] 49%|████▉     | 1520/3086 [51:35<53:21,  2.04s/it]                                                   {'loss': 0.2121, 'grad_norm': 0.04271610081195831, 'learning_rate': 0.00030447180816591056, 'epoch': 0.49}
 49%|████▉     | 1520/3086 [51:35<53:21,  2.04s/it] 49%|████▉     | 1521/3086 [51:37<51:43,  1.98s/it] 49%|████▉     | 1522/3086 [51:39<50:37,  1.94s/it] 49%|████▉     | 1523/3086 [51:41<50:30,  1.94s/it] 49%|████▉     | 1524/3086 [51:43<50:16,  1.93s/it] 49%|████▉     | 1525/3086 [51:45<51:55,  2.00s/it] 49%|████▉     | 1526/3086 [51:48<55:10,  2.12s/it] 49%|████▉     | 1527/3086 [51:49<53:01,  2.04s/it] 50%|████▉     | 1528/3086 [51:51<49:41,  1.91s/it] 50%|████▉     | 1529/3086 [51:53<50:30,  1.95s/it] 50%|████▉     | 1530/3086 [51:56<55:01,  2.12s/it]                                                   {'loss': 0.2249, 'grad_norm': 0.04229427129030228, 'learning_rate': 0.0003025275437459494, 'epoch': 0.5}
 50%|████▉     | 1530/3086 [51:56<55:01,  2.12s/it] 50%|████▉     | 1531/3086 [51:58<54:43,  2.11s/it] 50%|████▉     | 1532/3086 [52:00<56:25,  2.18s/it] 50%|████▉     | 1533/3086 [52:02<54:45,  2.12s/it] 50%|████▉     | 1534/3086 [52:04<52:41,  2.04s/it] 50%|████▉     | 1535/3086 [52:06<54:05,  2.09s/it] 50%|████▉     | 1536/3086 [52:08<53:53,  2.09s/it] 50%|████▉     | 1537/3086 [52:10<53:05,  2.06s/it] 50%|████▉     | 1538/3086 [52:12<52:48,  2.05s/it] 50%|████▉     | 1539/3086 [52:14<53:54,  2.09s/it] 50%|████▉     | 1540/3086 [52:16<54:13,  2.10s/it]                                                   {'loss': 0.2115, 'grad_norm': 0.046077076345682144, 'learning_rate': 0.00030058327932598833, 'epoch': 0.5}
 50%|████▉     | 1540/3086 [52:16<54:13,  2.10s/it] 50%|████▉     | 1541/3086 [52:18<53:03,  2.06s/it] 50%|████▉     | 1542/3086 [52:21<54:13,  2.11s/it] 50%|█████     | 1543/3086 [52:23<53:33,  2.08s/it] 50%|█████     | 1544/3086 [52:24<51:03,  1.99s/it] 50%|█████     | 1545/3086 [52:26<50:42,  1.97s/it] 50%|█████     | 1546/3086 [52:28<49:38,  1.93s/it] 50%|█████     | 1547/3086 [52:30<52:04,  2.03s/it] 50%|█████     | 1548/3086 [52:33<52:10,  2.04s/it] 50%|█████     | 1549/3086 [52:34<51:12,  2.00s/it] 50%|█████     | 1550/3086 [52:36<50:00,  1.95s/it]                                                   {'loss': 0.2116, 'grad_norm': 0.04167642816901207, 'learning_rate': 0.0002986390149060272, 'epoch': 0.5}
 50%|█████     | 1550/3086 [52:36<50:00,  1.95s/it] 50%|█████     | 1551/3086 [52:39<52:27,  2.05s/it] 50%|█████     | 1552/3086 [52:40<51:16,  2.01s/it] 50%|█████     | 1553/3086 [52:43<53:24,  2.09s/it] 50%|█████     | 1554/3086 [52:45<53:14,  2.09s/it] 50%|█████     | 1555/3086 [52:47<50:20,  1.97s/it] 50%|█████     | 1556/3086 [52:48<49:54,  1.96s/it] 50%|█████     | 1557/3086 [52:50<48:26,  1.90s/it] 50%|█████     | 1558/3086 [52:53<54:04,  2.12s/it] 51%|█████     | 1559/3086 [52:55<53:47,  2.11s/it] 51%|█████     | 1560/3086 [52:57<51:07,  2.01s/it]                                                   {'loss': 0.2111, 'grad_norm': 0.044956013560295105, 'learning_rate': 0.0002966947504860661, 'epoch': 0.51}
 51%|█████     | 1560/3086 [52:57<51:07,  2.01s/it] 51%|█████     | 1561/3086 [52:58<48:29,  1.91s/it] 51%|█████     | 1562/3086 [53:00<47:34,  1.87s/it] 51%|█████     | 1563/3086 [53:02<49:38,  1.96s/it] 51%|█████     | 1564/3086 [53:04<47:50,  1.89s/it] 51%|█████     | 1565/3086 [53:06<46:53,  1.85s/it] 51%|█████     | 1566/3086 [53:08<47:12,  1.86s/it] 51%|█████     | 1567/3086 [53:10<50:11,  1.98s/it] 51%|█████     | 1568/3086 [53:12<50:31,  2.00s/it] 51%|█████     | 1569/3086 [53:14<49:03,  1.94s/it] 51%|█████     | 1570/3086 [53:16<51:16,  2.03s/it]                                                   {'loss': 0.216, 'grad_norm': 0.04493191838264465, 'learning_rate': 0.00029475048606610494, 'epoch': 0.51}
 51%|█████     | 1570/3086 [53:16<51:16,  2.03s/it] 51%|█████     | 1571/3086 [53:19<1:00:08,  2.38s/it] 51%|█████     | 1572/3086 [53:21<55:18,  2.19s/it]   51%|█████     | 1573/3086 [53:24<59:25,  2.36s/it] 51%|█████     | 1574/3086 [53:26<56:32,  2.24s/it] 51%|█████     | 1575/3086 [53:27<52:03,  2.07s/it] 51%|█████     | 1576/3086 [53:30<55:30,  2.21s/it] 51%|█████     | 1577/3086 [53:32<51:52,  2.06s/it] 51%|█████     | 1578/3086 [53:33<48:48,  1.94s/it] 51%|█████     | 1579/3086 [53:36<51:00,  2.03s/it] 51%|█████     | 1580/3086 [53:37<50:27,  2.01s/it]                                                   {'loss': 0.2114, 'grad_norm': 0.04069456085562706, 'learning_rate': 0.0002928062216461438, 'epoch': 0.51}
 51%|█████     | 1580/3086 [53:37<50:27,  2.01s/it] 51%|█████     | 1581/3086 [53:39<48:24,  1.93s/it] 51%|█████▏    | 1582/3086 [53:41<47:57,  1.91s/it] 51%|█████▏    | 1583/3086 [53:43<48:04,  1.92s/it] 51%|█████▏    | 1584/3086 [53:45<46:11,  1.85s/it] 51%|█████▏    | 1585/3086 [53:46<44:55,  1.80s/it] 51%|█████▏    | 1586/3086 [53:48<45:44,  1.83s/it] 51%|█████▏    | 1587/3086 [53:51<48:33,  1.94s/it] 51%|█████▏    | 1588/3086 [53:52<48:11,  1.93s/it] 51%|█████▏    | 1589/3086 [53:55<51:40,  2.07s/it] 52%|█████▏    | 1590/3086 [53:57<50:29,  2.03s/it]                                                   {'loss': 0.2066, 'grad_norm': 0.04679626226425171, 'learning_rate': 0.00029086195722618276, 'epoch': 0.52}
 52%|█████▏    | 1590/3086 [53:57<50:29,  2.03s/it] 52%|█████▏    | 1591/3086 [53:59<55:47,  2.24s/it] 52%|█████▏    | 1592/3086 [54:01<53:08,  2.13s/it] 52%|█████▏    | 1593/3086 [54:03<52:45,  2.12s/it] 52%|█████▏    | 1594/3086 [54:06<53:07,  2.14s/it] 52%|█████▏    | 1595/3086 [54:07<51:10,  2.06s/it] 52%|█████▏    | 1596/3086 [54:09<48:41,  1.96s/it] 52%|█████▏    | 1597/3086 [54:11<45:36,  1.84s/it] 52%|█████▏    | 1598/3086 [54:13<47:57,  1.93s/it] 52%|█████▏    | 1599/3086 [54:15<49:17,  1.99s/it] 52%|█████▏    | 1600/3086 [54:17<49:18,  1.99s/it]                                                   {'loss': 0.2141, 'grad_norm': 0.04378972202539444, 'learning_rate': 0.00028891769280622165, 'epoch': 0.52}
 52%|█████▏    | 1600/3086 [54:17<49:18,  1.99s/it] 52%|█████▏    | 1601/3086 [54:20<54:48,  2.21s/it] 52%|█████▏    | 1602/3086 [54:22<52:18,  2.11s/it] 52%|█████▏    | 1603/3086 [54:24<51:06,  2.07s/it] 52%|█████▏    | 1604/3086 [54:26<50:06,  2.03s/it] 52%|█████▏    | 1605/3086 [54:27<47:26,  1.92s/it] 52%|█████▏    | 1606/3086 [54:29<48:15,  1.96s/it] 52%|█████▏    | 1607/3086 [54:31<49:16,  2.00s/it] 52%|█████▏    | 1608/3086 [54:34<53:38,  2.18s/it] 52%|█████▏    | 1609/3086 [54:36<52:12,  2.12s/it] 52%|█████▏    | 1610/3086 [54:39<56:24,  2.29s/it]                                                   {'loss': 0.2148, 'grad_norm': 0.04445347934961319, 'learning_rate': 0.0002869734283862605, 'epoch': 0.52}
 52%|█████▏    | 1610/3086 [54:39<56:24,  2.29s/it] 52%|█████▏    | 1611/3086 [54:41<56:56,  2.32s/it] 52%|█████▏    | 1612/3086 [54:43<51:29,  2.10s/it] 52%|█████▏    | 1613/3086 [54:45<53:52,  2.19s/it] 52%|█████▏    | 1614/3086 [54:47<51:01,  2.08s/it] 52%|█████▏    | 1615/3086 [54:49<53:22,  2.18s/it] 52%|█████▏    | 1616/3086 [54:51<50:41,  2.07s/it] 52%|█████▏    | 1617/3086 [54:53<51:36,  2.11s/it] 52%|█████▏    | 1618/3086 [54:55<49:50,  2.04s/it] 52%|█████▏    | 1619/3086 [54:57<48:51,  2.00s/it] 52%|█████▏    | 1620/3086 [54:59<50:20,  2.06s/it]                                                   {'loss': 0.2075, 'grad_norm': 0.04349510371685028, 'learning_rate': 0.00028502916396629937, 'epoch': 0.52}
 52%|█████▏    | 1620/3086 [54:59<50:20,  2.06s/it] 53%|█████▎    | 1621/3086 [55:01<49:03,  2.01s/it] 53%|█████▎    | 1622/3086 [55:03<47:18,  1.94s/it] 53%|█████▎    | 1623/3086 [55:05<50:13,  2.06s/it] 53%|█████▎    | 1624/3086 [55:07<46:14,  1.90s/it] 53%|█████▎    | 1625/3086 [55:09<47:50,  1.96s/it] 53%|█████▎    | 1626/3086 [55:11<48:20,  1.99s/it] 53%|█████▎    | 1627/3086 [55:13<48:50,  2.01s/it] 53%|█████▎    | 1628/3086 [55:15<47:21,  1.95s/it] 53%|█████▎    | 1629/3086 [55:18<53:37,  2.21s/it] 53%|█████▎    | 1630/3086 [55:21<59:19,  2.44s/it]                                                   {'loss': 0.2128, 'grad_norm': 0.04264025017619133, 'learning_rate': 0.00028308489954633826, 'epoch': 0.53}
 53%|█████▎    | 1630/3086 [55:21<59:19,  2.44s/it] 53%|█████▎    | 1631/3086 [55:23<55:31,  2.29s/it] 53%|█████▎    | 1632/3086 [55:24<51:48,  2.14s/it] 53%|█████▎    | 1633/3086 [55:26<48:18,  2.00s/it] 53%|█████▎    | 1634/3086 [55:28<47:58,  1.98s/it] 53%|█████▎    | 1635/3086 [55:30<46:05,  1.91s/it] 53%|█████▎    | 1636/3086 [55:32<49:11,  2.04s/it] 53%|█████▎    | 1637/3086 [55:35<54:17,  2.25s/it] 53%|█████▎    | 1638/3086 [55:37<53:12,  2.21s/it] 53%|█████▎    | 1639/3086 [55:39<50:50,  2.11s/it] 53%|█████▎    | 1640/3086 [55:41<52:06,  2.16s/it]                                                   {'loss': 0.2106, 'grad_norm': 0.046134717762470245, 'learning_rate': 0.0002811406351263772, 'epoch': 0.53}
 53%|█████▎    | 1640/3086 [55:41<52:06,  2.16s/it] 53%|█████▎    | 1641/3086 [55:43<50:37,  2.10s/it] 53%|█████▎    | 1642/3086 [55:45<49:16,  2.05s/it] 53%|█████▎    | 1643/3086 [55:47<48:27,  2.01s/it] 53%|█████▎    | 1644/3086 [55:49<48:35,  2.02s/it] 53%|█████▎    | 1645/3086 [55:51<50:29,  2.10s/it] 53%|█████▎    | 1646/3086 [55:53<48:49,  2.03s/it] 53%|█████▎    | 1647/3086 [55:55<49:01,  2.04s/it] 53%|█████▎    | 1648/3086 [55:57<48:25,  2.02s/it] 53%|█████▎    | 1649/3086 [55:59<48:35,  2.03s/it] 53%|█████▎    | 1650/3086 [56:01<47:14,  1.97s/it]                                                   {'loss': 0.212, 'grad_norm': 0.041825518012046814, 'learning_rate': 0.00027919637070641603, 'epoch': 0.53}
 53%|█████▎    | 1650/3086 [56:01<47:14,  1.97s/it] 53%|█████▎    | 1651/3086 [56:03<44:53,  1.88s/it] 54%|█████▎    | 1652/3086 [56:04<44:19,  1.85s/it] 54%|█████▎    | 1653/3086 [56:06<44:20,  1.86s/it] 54%|█████▎    | 1654/3086 [56:08<43:49,  1.84s/it] 54%|█████▎    | 1655/3086 [56:10<46:09,  1.94s/it] 54%|█████▎    | 1656/3086 [56:12<46:32,  1.95s/it] 54%|█████▎    | 1657/3086 [56:14<43:01,  1.81s/it] 54%|█████▎    | 1658/3086 [56:16<46:34,  1.96s/it] 54%|█████▍    | 1659/3086 [56:18<46:35,  1.96s/it] 54%|█████▍    | 1660/3086 [56:20<47:06,  1.98s/it]                                                   {'loss': 0.2101, 'grad_norm': 0.04431784525513649, 'learning_rate': 0.0002772521062864549, 'epoch': 0.54}
 54%|█████▍    | 1660/3086 [56:20<47:06,  1.98s/it] 54%|█████▍    | 1661/3086 [56:22<46:00,  1.94s/it] 54%|█████▍    | 1662/3086 [56:24<47:18,  1.99s/it] 54%|█████▍    | 1663/3086 [56:26<45:41,  1.93s/it] 54%|█████▍    | 1664/3086 [56:27<44:18,  1.87s/it] 54%|█████▍    | 1665/3086 [56:30<46:21,  1.96s/it] 54%|█████▍    | 1666/3086 [56:32<47:57,  2.03s/it] 54%|█████▍    | 1667/3086 [56:34<51:54,  2.20s/it] 54%|█████▍    | 1668/3086 [56:36<49:19,  2.09s/it] 54%|█████▍    | 1669/3086 [56:38<47:22,  2.01s/it] 54%|█████▍    | 1670/3086 [56:40<45:47,  1.94s/it]                                                   {'loss': 0.2094, 'grad_norm': 0.04333826154470444, 'learning_rate': 0.0002753078418664938, 'epoch': 0.54}
 54%|█████▍    | 1670/3086 [56:40<45:47,  1.94s/it] 54%|█████▍    | 1671/3086 [56:42<47:56,  2.03s/it] 54%|█████▍    | 1672/3086 [56:44<46:06,  1.96s/it] 54%|█████▍    | 1673/3086 [56:46<46:02,  1.96s/it] 54%|█████▍    | 1674/3086 [56:48<44:29,  1.89s/it] 54%|█████▍    | 1675/3086 [56:49<43:18,  1.84s/it] 54%|█████▍    | 1676/3086 [56:51<44:03,  1.87s/it] 54%|█████▍    | 1677/3086 [56:53<45:39,  1.94s/it] 54%|█████▍    | 1678/3086 [56:56<48:27,  2.07s/it] 54%|█████▍    | 1679/3086 [56:58<47:05,  2.01s/it] 54%|█████▍    | 1680/3086 [57:00<47:51,  2.04s/it]                                                   {'loss': 0.2137, 'grad_norm': 0.04248003289103508, 'learning_rate': 0.0002733635774465327, 'epoch': 0.54}
 54%|█████▍    | 1680/3086 [57:00<47:51,  2.04s/it] 54%|█████▍    | 1681/3086 [57:01<45:12,  1.93s/it] 55%|█████▍    | 1682/3086 [57:03<43:29,  1.86s/it] 55%|█████▍    | 1683/3086 [57:05<42:40,  1.83s/it] 55%|█████▍    | 1684/3086 [57:07<42:25,  1.82s/it] 55%|█████▍    | 1685/3086 [57:09<43:05,  1.85s/it] 55%|█████▍    | 1686/3086 [57:11<46:15,  1.98s/it] 55%|█████▍    | 1687/3086 [57:13<46:34,  2.00s/it] 55%|█████▍    | 1688/3086 [57:15<49:10,  2.11s/it] 55%|█████▍    | 1689/3086 [57:17<48:22,  2.08s/it] 55%|█████▍    | 1690/3086 [57:19<46:04,  1.98s/it]                                                   {'loss': 0.2117, 'grad_norm': 0.04448647424578667, 'learning_rate': 0.0002714193130265716, 'epoch': 0.55}
 55%|█████▍    | 1690/3086 [57:19<46:04,  1.98s/it] 55%|█████▍    | 1691/3086 [57:21<46:00,  1.98s/it] 55%|█████▍    | 1692/3086 [57:23<44:56,  1.93s/it] 55%|█████▍    | 1693/3086 [57:25<46:47,  2.02s/it] 55%|█████▍    | 1694/3086 [57:27<48:11,  2.08s/it] 55%|█████▍    | 1695/3086 [57:29<48:20,  2.09s/it] 55%|█████▍    | 1696/3086 [57:31<47:38,  2.06s/it] 55%|█████▍    | 1697/3086 [57:34<53:47,  2.32s/it] 55%|█████▌    | 1698/3086 [57:36<48:57,  2.12s/it] 55%|█████▌    | 1699/3086 [57:38<48:36,  2.10s/it] 55%|█████▌    | 1700/3086 [57:40<48:27,  2.10s/it]                                                   {'loss': 0.211, 'grad_norm': 0.041827794164419174, 'learning_rate': 0.00026947504860661047, 'epoch': 0.55}
 55%|█████▌    | 1700/3086 [57:40<48:27,  2.10s/it] 55%|█████▌    | 1701/3086 [57:42<50:23,  2.18s/it] 55%|█████▌    | 1702/3086 [57:44<46:50,  2.03s/it] 55%|█████▌    | 1703/3086 [57:46<47:11,  2.05s/it] 55%|█████▌    | 1704/3086 [57:48<44:38,  1.94s/it] 55%|█████▌    | 1705/3086 [57:50<47:33,  2.07s/it] 55%|█████▌    | 1706/3086 [57:53<49:25,  2.15s/it] 55%|█████▌    | 1707/3086 [57:55<48:10,  2.10s/it] 55%|█████▌    | 1708/3086 [57:56<46:17,  2.02s/it] 55%|█████▌    | 1709/3086 [57:58<45:24,  1.98s/it] 55%|█████▌    | 1710/3086 [58:00<46:01,  2.01s/it]                                                   {'loss': 0.2051, 'grad_norm': 0.04409193620085716, 'learning_rate': 0.00026753078418664935, 'epoch': 0.55}
 55%|█████▌    | 1710/3086 [58:00<46:01,  2.01s/it] 55%|█████▌    | 1711/3086 [58:03<52:16,  2.28s/it] 55%|█████▌    | 1712/3086 [58:06<54:31,  2.38s/it] 56%|█████▌    | 1713/3086 [58:08<53:07,  2.32s/it] 56%|█████▌    | 1714/3086 [58:10<48:34,  2.12s/it] 56%|█████▌    | 1715/3086 [58:12<50:16,  2.20s/it] 56%|█████▌    | 1716/3086 [58:14<47:19,  2.07s/it] 56%|█████▌    | 1717/3086 [58:15<42:54,  1.88s/it] 56%|█████▌    | 1718/3086 [58:18<47:38,  2.09s/it] 56%|█████▌    | 1719/3086 [58:20<45:09,  1.98s/it] 56%|█████▌    | 1720/3086 [58:21<42:45,  1.88s/it]                                                   {'loss': 0.2042, 'grad_norm': 0.039227310568094254, 'learning_rate': 0.00026558651976668824, 'epoch': 0.56}
 56%|█████▌    | 1720/3086 [58:21<42:45,  1.88s/it] 56%|█████▌    | 1721/3086 [58:23<42:04,  1.85s/it] 56%|█████▌    | 1722/3086 [58:25<43:18,  1.90s/it] 56%|█████▌    | 1723/3086 [58:28<47:36,  2.10s/it] 56%|█████▌    | 1724/3086 [58:29<46:09,  2.03s/it] 56%|█████▌    | 1725/3086 [58:32<48:36,  2.14s/it] 56%|█████▌    | 1726/3086 [58:34<45:56,  2.03s/it] 56%|█████▌    | 1727/3086 [58:35<44:05,  1.95s/it] 56%|█████▌    | 1728/3086 [58:37<42:05,  1.86s/it] 56%|█████▌    | 1729/3086 [58:39<42:38,  1.89s/it] 56%|█████▌    | 1730/3086 [58:41<42:17,  1.87s/it]                                                   {'loss': 0.2073, 'grad_norm': 0.04076889902353287, 'learning_rate': 0.00026364225534672713, 'epoch': 0.56}
 56%|█████▌    | 1730/3086 [58:41<42:17,  1.87s/it] 56%|█████▌    | 1731/3086 [58:43<44:51,  1.99s/it] 56%|█████▌    | 1732/3086 [58:45<46:59,  2.08s/it] 56%|█████▌    | 1733/3086 [58:47<46:11,  2.05s/it] 56%|█████▌    | 1734/3086 [58:49<45:50,  2.03s/it] 56%|█████▌    | 1735/3086 [58:51<45:09,  2.01s/it] 56%|█████▋    | 1736/3086 [58:53<45:46,  2.03s/it] 56%|█████▋    | 1737/3086 [58:55<45:53,  2.04s/it] 56%|█████▋    | 1738/3086 [58:57<43:22,  1.93s/it] 56%|█████▋    | 1739/3086 [58:59<45:38,  2.03s/it] 56%|█████▋    | 1740/3086 [59:02<52:28,  2.34s/it]                                                   {'loss': 0.2031, 'grad_norm': 0.03846532851457596, 'learning_rate': 0.000261697990926766, 'epoch': 0.56}
 56%|█████▋    | 1740/3086 [59:02<52:28,  2.34s/it] 56%|█████▋    | 1741/3086 [59:04<45:45,  2.04s/it] 56%|█████▋    | 1742/3086 [59:06<46:10,  2.06s/it] 56%|█████▋    | 1743/3086 [59:08<47:02,  2.10s/it] 57%|█████▋    | 1744/3086 [59:10<45:46,  2.05s/it] 57%|█████▋    | 1745/3086 [59:12<46:57,  2.10s/it] 57%|█████▋    | 1746/3086 [59:14<44:42,  2.00s/it] 57%|█████▋    | 1747/3086 [59:16<41:30,  1.86s/it] 57%|█████▋    | 1748/3086 [59:18<46:10,  2.07s/it] 57%|█████▋    | 1749/3086 [59:20<46:47,  2.10s/it] 57%|█████▋    | 1750/3086 [59:22<44:27,  2.00s/it]                                                   {'loss': 0.2091, 'grad_norm': 0.043870192021131516, 'learning_rate': 0.0002597537265068049, 'epoch': 0.57}
 57%|█████▋    | 1750/3086 [59:22<44:27,  2.00s/it] 57%|█████▋    | 1751/3086 [59:24<41:57,  1.89s/it] 57%|█████▋    | 1752/3086 [59:26<42:02,  1.89s/it] 57%|█████▋    | 1753/3086 [59:27<41:19,  1.86s/it] 57%|█████▋    | 1754/3086 [59:29<39:46,  1.79s/it] 57%|█████▋    | 1755/3086 [59:31<39:54,  1.80s/it] 57%|█████▋    | 1756/3086 [59:33<43:39,  1.97s/it] 57%|█████▋    | 1757/3086 [59:35<42:33,  1.92s/it] 57%|█████▋    | 1758/3086 [59:37<43:52,  1.98s/it] 57%|█████▋    | 1759/3086 [59:39<45:31,  2.06s/it] 57%|█████▋    | 1760/3086 [59:41<45:38,  2.07s/it]                                                   {'loss': 0.2155, 'grad_norm': 0.04732134938240051, 'learning_rate': 0.0002578094620868438, 'epoch': 0.57}
 57%|█████▋    | 1760/3086 [59:41<45:38,  2.07s/it] 57%|█████▋    | 1761/3086 [59:44<45:41,  2.07s/it] 57%|█████▋    | 1762/3086 [59:46<47:56,  2.17s/it] 57%|█████▋    | 1763/3086 [59:48<48:21,  2.19s/it] 57%|█████▋    | 1764/3086 [59:50<47:43,  2.17s/it] 57%|█████▋    | 1765/3086 [59:52<45:14,  2.06s/it] 57%|█████▋    | 1766/3086 [59:54<46:50,  2.13s/it] 57%|█████▋    | 1767/3086 [59:56<45:35,  2.07s/it] 57%|█████▋    | 1768/3086 [59:58<42:40,  1.94s/it] 57%|█████▋    | 1769/3086 [1:00:00<43:34,  1.99s/it] 57%|█████▋    | 1770/3086 [1:00:02<43:56,  2.00s/it]                                                     {'loss': 0.2126, 'grad_norm': 0.04209471121430397, 'learning_rate': 0.0002558651976668827, 'epoch': 0.57}
 57%|█████▋    | 1770/3086 [1:00:02<43:56,  2.00s/it] 57%|█████▋    | 1771/3086 [1:00:04<42:50,  1.95s/it] 57%|█████▋    | 1772/3086 [1:00:06<45:01,  2.06s/it] 57%|█████▋    | 1773/3086 [1:00:08<43:40,  2.00s/it] 57%|█████▋    | 1774/3086 [1:00:10<45:27,  2.08s/it] 58%|█████▊    | 1775/3086 [1:00:12<44:48,  2.05s/it] 58%|█████▊    | 1776/3086 [1:00:15<47:17,  2.17s/it] 58%|█████▊    | 1777/3086 [1:00:17<45:39,  2.09s/it] 58%|█████▊    | 1778/3086 [1:00:19<48:51,  2.24s/it] 58%|█████▊    | 1779/3086 [1:00:21<48:36,  2.23s/it] 58%|█████▊    | 1780/3086 [1:00:23<45:34,  2.09s/it]                                                     {'loss': 0.2038, 'grad_norm': 0.04068365320563316, 'learning_rate': 0.00025392093324692156, 'epoch': 0.58}
 58%|█████▊    | 1780/3086 [1:00:23<45:34,  2.09s/it] 58%|█████▊    | 1781/3086 [1:00:25<45:42,  2.10s/it] 58%|█████▊    | 1782/3086 [1:00:27<44:30,  2.05s/it] 58%|█████▊    | 1783/3086 [1:00:29<43:40,  2.01s/it] 58%|█████▊    | 1784/3086 [1:00:31<42:57,  1.98s/it] 58%|█████▊    | 1785/3086 [1:00:33<43:35,  2.01s/it] 58%|█████▊    | 1786/3086 [1:00:36<46:08,  2.13s/it] 58%|█████▊    | 1787/3086 [1:00:38<45:31,  2.10s/it] 58%|█████▊    | 1788/3086 [1:00:40<44:21,  2.05s/it] 58%|█████▊    | 1789/3086 [1:00:41<42:10,  1.95s/it] 58%|█████▊    | 1790/3086 [1:00:43<40:23,  1.87s/it]                                                     {'loss': 0.2032, 'grad_norm': 0.03983562812209129, 'learning_rate': 0.00025197666882696045, 'epoch': 0.58}
 58%|█████▊    | 1790/3086 [1:00:43<40:23,  1.87s/it] 58%|█████▊    | 1791/3086 [1:00:45<40:09,  1.86s/it] 58%|█████▊    | 1792/3086 [1:00:47<42:39,  1.98s/it] 58%|█████▊    | 1793/3086 [1:00:49<44:09,  2.05s/it] 58%|█████▊    | 1794/3086 [1:00:51<42:40,  1.98s/it] 58%|█████▊    | 1795/3086 [1:00:53<44:28,  2.07s/it] 58%|█████▊    | 1796/3086 [1:00:55<41:53,  1.95s/it] 58%|█████▊    | 1797/3086 [1:00:57<40:46,  1.90s/it] 58%|█████▊    | 1798/3086 [1:00:59<41:33,  1.94s/it] 58%|█████▊    | 1799/3086 [1:01:02<46:23,  2.16s/it] 58%|█████▊    | 1800/3086 [1:01:03<44:42,  2.09s/it]                                                     {'loss': 0.208, 'grad_norm': 0.047146208584308624, 'learning_rate': 0.00025003240440699934, 'epoch': 0.58}
 58%|█████▊    | 1800/3086 [1:01:03<44:42,  2.09s/it] 58%|█████▊    | 1801/3086 [1:01:05<43:22,  2.03s/it] 58%|█████▊    | 1802/3086 [1:01:07<40:29,  1.89s/it] 58%|█████▊    | 1803/3086 [1:01:10<45:37,  2.13s/it] 58%|█████▊    | 1804/3086 [1:01:12<48:54,  2.29s/it] 58%|█████▊    | 1805/3086 [1:01:14<45:59,  2.15s/it] 59%|█████▊    | 1806/3086 [1:01:16<46:06,  2.16s/it] 59%|█████▊    | 1807/3086 [1:01:18<43:51,  2.06s/it] 59%|█████▊    | 1808/3086 [1:01:20<42:47,  2.01s/it] 59%|█████▊    | 1809/3086 [1:01:22<44:49,  2.11s/it] 59%|█████▊    | 1810/3086 [1:01:25<45:49,  2.15s/it]                                                     {'loss': 0.2002, 'grad_norm': 0.043838102370500565, 'learning_rate': 0.0002480881399870382, 'epoch': 0.59}
 59%|█████▊    | 1810/3086 [1:01:25<45:49,  2.15s/it] 59%|█████▊    | 1811/3086 [1:01:27<46:37,  2.19s/it] 59%|█████▊    | 1812/3086 [1:01:29<43:57,  2.07s/it] 59%|█████▊    | 1813/3086 [1:01:31<43:18,  2.04s/it] 59%|█████▉    | 1814/3086 [1:01:33<42:45,  2.02s/it] 59%|█████▉    | 1815/3086 [1:01:35<45:47,  2.16s/it] 59%|█████▉    | 1816/3086 [1:01:37<45:22,  2.14s/it] 59%|█████▉    | 1817/3086 [1:01:39<45:05,  2.13s/it] 59%|█████▉    | 1818/3086 [1:01:41<41:52,  1.98s/it] 59%|█████▉    | 1819/3086 [1:01:43<40:56,  1.94s/it] 59%|█████▉    | 1820/3086 [1:01:45<40:15,  1.91s/it]                                                     {'loss': 0.2009, 'grad_norm': 0.04307657107710838, 'learning_rate': 0.0002461438755670771, 'epoch': 0.59}
 59%|█████▉    | 1820/3086 [1:01:45<40:15,  1.91s/it] 59%|█████▉    | 1821/3086 [1:01:47<41:47,  1.98s/it] 59%|█████▉    | 1822/3086 [1:01:49<41:09,  1.95s/it] 59%|█████▉    | 1823/3086 [1:01:51<41:15,  1.96s/it] 59%|█████▉    | 1824/3086 [1:01:53<41:20,  1.97s/it] 59%|█████▉    | 1825/3086 [1:01:54<38:17,  1.82s/it] 59%|█████▉    | 1826/3086 [1:01:56<40:20,  1.92s/it] 59%|█████▉    | 1827/3086 [1:01:58<41:10,  1.96s/it] 59%|█████▉    | 1828/3086 [1:02:00<41:43,  1.99s/it] 59%|█████▉    | 1829/3086 [1:02:02<41:54,  2.00s/it] 59%|█████▉    | 1830/3086 [1:02:04<42:18,  2.02s/it]                                                     {'loss': 0.2043, 'grad_norm': 0.0455453135073185, 'learning_rate': 0.00024419961114711594, 'epoch': 0.59}
 59%|█████▉    | 1830/3086 [1:02:04<42:18,  2.02s/it] 59%|█████▉    | 1831/3086 [1:02:07<47:07,  2.25s/it] 59%|█████▉    | 1832/3086 [1:02:09<46:20,  2.22s/it] 59%|█████▉    | 1833/3086 [1:02:11<43:45,  2.10s/it] 59%|█████▉    | 1834/3086 [1:02:13<42:54,  2.06s/it] 59%|█████▉    | 1835/3086 [1:02:15<41:11,  1.98s/it] 59%|█████▉    | 1836/3086 [1:02:17<41:41,  2.00s/it] 60%|█████▉    | 1837/3086 [1:02:19<44:43,  2.15s/it] 60%|█████▉    | 1838/3086 [1:02:21<42:46,  2.06s/it] 60%|█████▉    | 1839/3086 [1:02:24<45:10,  2.17s/it] 60%|█████▉    | 1840/3086 [1:02:26<44:02,  2.12s/it]                                                     {'loss': 0.2063, 'grad_norm': 0.040200162678956985, 'learning_rate': 0.00024225534672715486, 'epoch': 0.6}
 60%|█████▉    | 1840/3086 [1:02:26<44:02,  2.12s/it] 60%|█████▉    | 1841/3086 [1:02:28<41:40,  2.01s/it] 60%|█████▉    | 1842/3086 [1:02:29<39:44,  1.92s/it] 60%|█████▉    | 1843/3086 [1:02:31<41:21,  2.00s/it] 60%|█████▉    | 1844/3086 [1:02:33<41:39,  2.01s/it] 60%|█████▉    | 1845/3086 [1:02:35<41:01,  1.98s/it] 60%|█████▉    | 1846/3086 [1:02:38<42:34,  2.06s/it] 60%|█████▉    | 1847/3086 [1:02:40<43:10,  2.09s/it] 60%|█████▉    | 1848/3086 [1:02:42<42:28,  2.06s/it] 60%|█████▉    | 1849/3086 [1:02:44<42:20,  2.05s/it] 60%|█████▉    | 1850/3086 [1:02:45<39:33,  1.92s/it]                                                     {'loss': 0.2, 'grad_norm': 0.040751662105321884, 'learning_rate': 0.00024031108230719375, 'epoch': 0.6}
 60%|█████▉    | 1850/3086 [1:02:45<39:33,  1.92s/it] 60%|█████▉    | 1851/3086 [1:02:48<44:56,  2.18s/it] 60%|██████    | 1852/3086 [1:02:50<44:09,  2.15s/it] 60%|██████    | 1853/3086 [1:02:52<42:00,  2.04s/it] 60%|██████    | 1854/3086 [1:02:54<40:11,  1.96s/it] 60%|██████    | 1855/3086 [1:02:56<42:26,  2.07s/it] 60%|██████    | 1856/3086 [1:02:58<42:24,  2.07s/it] 60%|██████    | 1857/3086 [1:03:00<38:54,  1.90s/it] 60%|██████    | 1858/3086 [1:03:02<39:40,  1.94s/it] 60%|██████    | 1859/3086 [1:03:04<39:54,  1.95s/it] 60%|██████    | 1860/3086 [1:03:06<41:19,  2.02s/it]                                                     {'loss': 0.2086, 'grad_norm': 0.04267115145921707, 'learning_rate': 0.00023836681788723266, 'epoch': 0.6}
 60%|██████    | 1860/3086 [1:03:06<41:19,  2.02s/it] 60%|██████    | 1861/3086 [1:03:08<42:55,  2.10s/it] 60%|██████    | 1862/3086 [1:03:11<44:17,  2.17s/it] 60%|██████    | 1863/3086 [1:03:12<41:36,  2.04s/it] 60%|██████    | 1864/3086 [1:03:14<40:10,  1.97s/it] 60%|██████    | 1865/3086 [1:03:16<39:03,  1.92s/it] 60%|██████    | 1866/3086 [1:03:18<39:53,  1.96s/it] 60%|██████    | 1867/3086 [1:03:20<40:47,  2.01s/it] 61%|██████    | 1868/3086 [1:03:22<39:10,  1.93s/it] 61%|██████    | 1869/3086 [1:03:24<38:19,  1.89s/it] 61%|██████    | 1870/3086 [1:03:26<42:03,  2.08s/it]                                                     {'loss': 0.2027, 'grad_norm': 0.03832196816802025, 'learning_rate': 0.00023642255346727152, 'epoch': 0.61}
 61%|██████    | 1870/3086 [1:03:26<42:03,  2.08s/it] 61%|██████    | 1871/3086 [1:03:28<40:54,  2.02s/it] 61%|██████    | 1872/3086 [1:03:30<39:23,  1.95s/it] 61%|██████    | 1873/3086 [1:03:32<38:28,  1.90s/it] 61%|██████    | 1874/3086 [1:03:33<38:09,  1.89s/it] 61%|██████    | 1875/3086 [1:03:35<38:01,  1.88s/it] 61%|██████    | 1876/3086 [1:03:37<39:06,  1.94s/it] 61%|██████    | 1877/3086 [1:03:40<41:37,  2.07s/it] 61%|██████    | 1878/3086 [1:03:42<41:23,  2.06s/it] 61%|██████    | 1879/3086 [1:03:44<40:18,  2.00s/it] 61%|██████    | 1880/3086 [1:03:46<40:38,  2.02s/it]                                                     {'loss': 0.205, 'grad_norm': 0.04210956394672394, 'learning_rate': 0.0002344782890473104, 'epoch': 0.61}
 61%|██████    | 1880/3086 [1:03:46<40:38,  2.02s/it] 61%|██████    | 1881/3086 [1:03:48<40:48,  2.03s/it] 61%|██████    | 1882/3086 [1:03:50<40:54,  2.04s/it] 61%|██████    | 1883/3086 [1:03:52<41:01,  2.05s/it] 61%|██████    | 1884/3086 [1:03:53<38:03,  1.90s/it] 61%|██████    | 1885/3086 [1:03:55<36:17,  1.81s/it] 61%|██████    | 1886/3086 [1:03:57<35:43,  1.79s/it] 61%|██████    | 1887/3086 [1:03:59<37:15,  1.86s/it] 61%|██████    | 1888/3086 [1:04:01<41:43,  2.09s/it] 61%|██████    | 1889/3086 [1:04:03<40:38,  2.04s/it] 61%|██████    | 1890/3086 [1:04:05<39:28,  1.98s/it]                                                     {'loss': 0.2076, 'grad_norm': 0.04283342882990837, 'learning_rate': 0.0002325340246273493, 'epoch': 0.61}
 61%|██████    | 1890/3086 [1:04:05<39:28,  1.98s/it] 61%|██████▏   | 1891/3086 [1:04:08<43:21,  2.18s/it] 61%|██████▏   | 1892/3086 [1:04:10<43:15,  2.17s/it] 61%|██████▏   | 1893/3086 [1:04:12<39:51,  2.00s/it] 61%|██████▏   | 1894/3086 [1:04:13<38:44,  1.95s/it] 61%|██████▏   | 1895/3086 [1:04:15<38:14,  1.93s/it] 61%|██████▏   | 1896/3086 [1:04:18<39:39,  2.00s/it] 61%|██████▏   | 1897/3086 [1:04:19<37:15,  1.88s/it] 62%|██████▏   | 1898/3086 [1:04:21<38:38,  1.95s/it] 62%|██████▏   | 1899/3086 [1:04:23<39:42,  2.01s/it] 62%|██████▏   | 1900/3086 [1:04:25<39:54,  2.02s/it]                                                     {'loss': 0.2018, 'grad_norm': 0.04973233491182327, 'learning_rate': 0.0002305897602073882, 'epoch': 0.62}
 62%|██████▏   | 1900/3086 [1:04:25<39:54,  2.02s/it] 62%|██████▏   | 1901/3086 [1:04:27<39:45,  2.01s/it] 62%|██████▏   | 1902/3086 [1:04:29<37:09,  1.88s/it] 62%|██████▏   | 1903/3086 [1:04:31<37:54,  1.92s/it] 62%|██████▏   | 1904/3086 [1:04:33<40:02,  2.03s/it] 62%|██████▏   | 1905/3086 [1:04:35<40:36,  2.06s/it] 62%|██████▏   | 1906/3086 [1:04:37<40:02,  2.04s/it] 62%|██████▏   | 1907/3086 [1:04:39<39:27,  2.01s/it] 62%|██████▏   | 1908/3086 [1:04:41<38:47,  1.98s/it] 62%|██████▏   | 1909/3086 [1:04:43<38:27,  1.96s/it] 62%|██████▏   | 1910/3086 [1:04:45<38:08,  1.95s/it]                                                     {'loss': 0.2059, 'grad_norm': 0.041537072509527206, 'learning_rate': 0.00022864549578742707, 'epoch': 0.62}
 62%|██████▏   | 1910/3086 [1:04:45<38:08,  1.95s/it] 62%|██████▏   | 1911/3086 [1:04:47<36:10,  1.85s/it] 62%|██████▏   | 1912/3086 [1:04:49<39:16,  2.01s/it] 62%|██████▏   | 1913/3086 [1:04:51<40:35,  2.08s/it] 62%|██████▏   | 1914/3086 [1:04:53<38:00,  1.95s/it] 62%|██████▏   | 1915/3086 [1:04:56<42:45,  2.19s/it] 62%|██████▏   | 1916/3086 [1:04:58<41:53,  2.15s/it] 62%|██████▏   | 1917/3086 [1:05:00<41:14,  2.12s/it] 62%|██████▏   | 1918/3086 [1:05:02<39:53,  2.05s/it] 62%|██████▏   | 1919/3086 [1:05:04<39:12,  2.02s/it] 62%|██████▏   | 1920/3086 [1:05:06<40:40,  2.09s/it]                                                     {'loss': 0.2013, 'grad_norm': 0.04909689724445343, 'learning_rate': 0.00022670123136746595, 'epoch': 0.62}
 62%|██████▏   | 1920/3086 [1:05:06<40:40,  2.09s/it] 62%|██████▏   | 1921/3086 [1:05:08<41:52,  2.16s/it] 62%|██████▏   | 1922/3086 [1:05:10<39:53,  2.06s/it] 62%|██████▏   | 1923/3086 [1:05:12<38:16,  1.97s/it] 62%|██████▏   | 1924/3086 [1:05:14<38:59,  2.01s/it] 62%|██████▏   | 1925/3086 [1:05:15<36:22,  1.88s/it] 62%|██████▏   | 1926/3086 [1:05:17<35:23,  1.83s/it] 62%|██████▏   | 1927/3086 [1:05:20<38:31,  1.99s/it] 62%|██████▏   | 1928/3086 [1:05:22<39:22,  2.04s/it] 63%|██████▎   | 1929/3086 [1:05:24<38:53,  2.02s/it] 63%|██████▎   | 1930/3086 [1:05:25<37:29,  1.95s/it]                                                     {'loss': 0.1991, 'grad_norm': 0.045535217970609665, 'learning_rate': 0.00022475696694750484, 'epoch': 0.63}
 63%|██████▎   | 1930/3086 [1:05:25<37:29,  1.95s/it] 63%|██████▎   | 1931/3086 [1:05:27<36:22,  1.89s/it] 63%|██████▎   | 1932/3086 [1:05:29<35:54,  1.87s/it] 63%|██████▎   | 1933/3086 [1:05:31<35:18,  1.84s/it] 63%|██████▎   | 1934/3086 [1:05:33<36:41,  1.91s/it] 63%|██████▎   | 1935/3086 [1:05:35<36:47,  1.92s/it] 63%|██████▎   | 1936/3086 [1:05:37<35:18,  1.84s/it] 63%|██████▎   | 1937/3086 [1:05:38<34:27,  1.80s/it] 63%|██████▎   | 1938/3086 [1:05:40<36:16,  1.90s/it] 63%|██████▎   | 1939/3086 [1:05:42<37:24,  1.96s/it] 63%|██████▎   | 1940/3086 [1:05:44<35:24,  1.85s/it]                                                     {'loss': 0.2031, 'grad_norm': 0.03885801136493683, 'learning_rate': 0.00022281270252754373, 'epoch': 0.63}
 63%|██████▎   | 1940/3086 [1:05:44<35:24,  1.85s/it] 63%|██████▎   | 1941/3086 [1:05:46<37:18,  1.95s/it] 63%|██████▎   | 1942/3086 [1:05:48<34:11,  1.79s/it] 63%|██████▎   | 1943/3086 [1:05:50<34:48,  1.83s/it] 63%|██████▎   | 1944/3086 [1:05:52<35:46,  1.88s/it] 63%|██████▎   | 1945/3086 [1:05:54<39:51,  2.10s/it] 63%|██████▎   | 1946/3086 [1:05:56<38:59,  2.05s/it] 63%|██████▎   | 1947/3086 [1:05:58<37:21,  1.97s/it] 63%|██████▎   | 1948/3086 [1:06:00<38:44,  2.04s/it] 63%|██████▎   | 1949/3086 [1:06:02<38:34,  2.04s/it] 63%|██████▎   | 1950/3086 [1:06:04<37:41,  1.99s/it]                                                     {'loss': 0.2032, 'grad_norm': 0.04850279539823532, 'learning_rate': 0.0002208684381075826, 'epoch': 0.63}
 63%|██████▎   | 1950/3086 [1:06:04<37:41,  1.99s/it] 63%|██████▎   | 1951/3086 [1:06:06<37:24,  1.98s/it] 63%|██████▎   | 1952/3086 [1:06:08<36:45,  1.94s/it] 63%|██████▎   | 1953/3086 [1:06:10<36:37,  1.94s/it] 63%|██████▎   | 1954/3086 [1:06:12<35:42,  1.89s/it] 63%|██████▎   | 1955/3086 [1:06:14<40:43,  2.16s/it] 63%|██████▎   | 1956/3086 [1:06:16<37:35,  2.00s/it] 63%|██████▎   | 1957/3086 [1:06:18<38:46,  2.06s/it] 63%|██████▎   | 1958/3086 [1:06:20<39:32,  2.10s/it] 63%|██████▎   | 1959/3086 [1:06:23<40:28,  2.15s/it] 64%|██████▎   | 1960/3086 [1:06:24<37:23,  1.99s/it]                                                     {'loss': 0.2016, 'grad_norm': 0.044795356690883636, 'learning_rate': 0.0002189241736876215, 'epoch': 0.64}
 64%|██████▎   | 1960/3086 [1:06:24<37:23,  1.99s/it] 64%|██████▎   | 1961/3086 [1:06:27<39:24,  2.10s/it] 64%|██████▎   | 1962/3086 [1:06:28<37:13,  1.99s/it] 64%|██████▎   | 1963/3086 [1:06:30<36:49,  1.97s/it] 64%|██████▎   | 1964/3086 [1:06:33<39:40,  2.12s/it] 64%|██████▎   | 1965/3086 [1:06:36<45:36,  2.44s/it] 64%|██████▎   | 1966/3086 [1:06:38<42:19,  2.27s/it] 64%|██████▎   | 1967/3086 [1:06:40<40:03,  2.15s/it] 64%|██████▍   | 1968/3086 [1:06:41<37:26,  2.01s/it] 64%|██████▍   | 1969/3086 [1:06:44<41:08,  2.21s/it] 64%|██████▍   | 1970/3086 [1:06:46<39:33,  2.13s/it]                                                     {'loss': 0.1999, 'grad_norm': 0.051323987543582916, 'learning_rate': 0.0002169799092676604, 'epoch': 0.64}
 64%|██████▍   | 1970/3086 [1:06:46<39:33,  2.13s/it] 64%|██████▍   | 1971/3086 [1:06:48<37:08,  2.00s/it] 64%|██████▍   | 1972/3086 [1:06:50<40:01,  2.16s/it] 64%|██████▍   | 1973/3086 [1:06:52<37:34,  2.03s/it] 64%|██████▍   | 1974/3086 [1:06:54<39:06,  2.11s/it] 64%|██████▍   | 1975/3086 [1:06:56<38:32,  2.08s/it] 64%|██████▍   | 1976/3086 [1:06:58<38:44,  2.09s/it] 64%|██████▍   | 1977/3086 [1:07:00<36:43,  1.99s/it] 64%|██████▍   | 1978/3086 [1:07:02<38:07,  2.06s/it] 64%|██████▍   | 1979/3086 [1:07:04<36:49,  2.00s/it] 64%|██████▍   | 1980/3086 [1:07:06<38:40,  2.10s/it]                                                     {'loss': 0.2052, 'grad_norm': 0.03686113283038139, 'learning_rate': 0.00021503564484769928, 'epoch': 0.64}
 64%|██████▍   | 1980/3086 [1:07:06<38:40,  2.10s/it] 64%|██████▍   | 1981/3086 [1:07:08<37:29,  2.04s/it] 64%|██████▍   | 1982/3086 [1:07:11<41:14,  2.24s/it] 64%|██████▍   | 1983/3086 [1:07:13<38:27,  2.09s/it] 64%|██████▍   | 1984/3086 [1:07:15<41:17,  2.25s/it] 64%|██████▍   | 1985/3086 [1:07:17<38:34,  2.10s/it] 64%|██████▍   | 1986/3086 [1:07:19<38:18,  2.09s/it] 64%|██████▍   | 1987/3086 [1:07:21<38:40,  2.11s/it] 64%|██████▍   | 1988/3086 [1:07:24<39:57,  2.18s/it] 64%|██████▍   | 1989/3086 [1:07:26<40:25,  2.21s/it] 64%|██████▍   | 1990/3086 [1:07:28<39:39,  2.17s/it]                                                     {'loss': 0.1985, 'grad_norm': 0.03827312961220741, 'learning_rate': 0.00021309138042773814, 'epoch': 0.64}
 64%|██████▍   | 1990/3086 [1:07:28<39:39,  2.17s/it] 65%|██████▍   | 1991/3086 [1:07:30<39:49,  2.18s/it] 65%|██████▍   | 1992/3086 [1:07:32<39:38,  2.17s/it] 65%|██████▍   | 1993/3086 [1:07:35<39:24,  2.16s/it] 65%|██████▍   | 1994/3086 [1:07:37<39:27,  2.17s/it] 65%|██████▍   | 1995/3086 [1:07:38<36:31,  2.01s/it] 65%|██████▍   | 1996/3086 [1:07:40<36:36,  2.02s/it] 65%|██████▍   | 1997/3086 [1:07:42<35:05,  1.93s/it] 65%|██████▍   | 1998/3086 [1:07:44<36:06,  1.99s/it] 65%|██████▍   | 1999/3086 [1:07:47<37:19,  2.06s/it] 65%|██████▍   | 2000/3086 [1:07:49<40:23,  2.23s/it]                                                     {'loss': 0.2001, 'grad_norm': 0.04422908276319504, 'learning_rate': 0.00021114711600777705, 'epoch': 0.65}
 65%|██████▍   | 2000/3086 [1:07:49<40:23,  2.23s/it][INFO|trainer.py:3503] 2024-11-14 02:39:43,837 >> Saving model checkpoint to /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5xl-aug-qwe2.5-math7b-metamath/checkpoint-2000
[INFO|configuration_utils.py:472] 2024-11-14 02:39:43,843 >> Configuration saved in /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5xl-aug-qwe2.5-math7b-metamath/checkpoint-2000/config.json
[INFO|tokenization_utils_base.py:2684] 2024-11-14 02:39:44,019 >> tokenizer config file saved in /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5xl-aug-qwe2.5-math7b-metamath/checkpoint-2000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2024-11-14 02:39:44,022 >> Special tokens file saved in /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5xl-aug-qwe2.5-math7b-metamath/checkpoint-2000/special_tokens_map.json
 65%|██████▍   | 2001/3086 [1:08:03<1:44:07,  5.76s/it] 65%|██████▍   | 2002/3086 [1:08:06<1:27:06,  4.82s/it] 65%|██████▍   | 2003/3086 [1:08:08<1:12:57,  4.04s/it] 65%|██████▍   | 2004/3086 [1:08:10<1:01:42,  3.42s/it] 65%|██████▍   | 2005/3086 [1:08:12<53:23,  2.96s/it]   65%|██████▌   | 2006/3086 [1:08:14<47:32,  2.64s/it] 65%|██████▌   | 2007/3086 [1:08:16<44:19,  2.46s/it] 65%|██████▌   | 2008/3086 [1:08:18<40:05,  2.23s/it] 65%|██████▌   | 2009/3086 [1:08:19<37:35,  2.09s/it] 65%|██████▌   | 2010/3086 [1:08:21<35:31,  1.98s/it]                                                     {'loss': 0.1992, 'grad_norm': 0.039156198501586914, 'learning_rate': 0.00020920285158781594, 'epoch': 0.65}
 65%|██████▌   | 2010/3086 [1:08:21<35:31,  1.98s/it] 65%|██████▌   | 2011/3086 [1:08:23<34:33,  1.93s/it] 65%|██████▌   | 2012/3086 [1:08:25<33:44,  1.88s/it] 65%|██████▌   | 2013/3086 [1:08:26<32:52,  1.84s/it] 65%|██████▌   | 2014/3086 [1:08:29<36:46,  2.06s/it] 65%|██████▌   | 2015/3086 [1:08:31<35:35,  1.99s/it] 65%|██████▌   | 2016/3086 [1:08:33<35:49,  2.01s/it] 65%|██████▌   | 2017/3086 [1:08:35<38:21,  2.15s/it] 65%|██████▌   | 2018/3086 [1:08:37<37:18,  2.10s/it] 65%|██████▌   | 2019/3086 [1:08:39<35:12,  1.98s/it] 65%|██████▌   | 2020/3086 [1:08:41<36:02,  2.03s/it]                                                     {'loss': 0.2008, 'grad_norm': 0.043642234057188034, 'learning_rate': 0.0002072585871678548, 'epoch': 0.65}
 65%|██████▌   | 2020/3086 [1:08:41<36:02,  2.03s/it] 65%|██████▌   | 2021/3086 [1:08:43<36:53,  2.08s/it] 66%|██████▌   | 2022/3086 [1:08:45<36:07,  2.04s/it] 66%|██████▌   | 2023/3086 [1:08:47<37:10,  2.10s/it] 66%|██████▌   | 2024/3086 [1:08:49<35:09,  1.99s/it] 66%|██████▌   | 2025/3086 [1:08:51<36:30,  2.06s/it] 66%|██████▌   | 2026/3086 [1:08:54<36:56,  2.09s/it] 66%|██████▌   | 2027/3086 [1:08:55<35:19,  2.00s/it] 66%|██████▌   | 2028/3086 [1:08:57<33:28,  1.90s/it] 66%|██████▌   | 2029/3086 [1:08:59<33:58,  1.93s/it] 66%|██████▌   | 2030/3086 [1:09:01<32:14,  1.83s/it]                                                     {'loss': 0.1978, 'grad_norm': 0.04409169405698776, 'learning_rate': 0.00020531432274789368, 'epoch': 0.66}
 66%|██████▌   | 2030/3086 [1:09:01<32:14,  1.83s/it] 66%|██████▌   | 2031/3086 [1:09:02<31:48,  1.81s/it] 66%|██████▌   | 2032/3086 [1:09:04<31:29,  1.79s/it] 66%|██████▌   | 2033/3086 [1:09:06<32:23,  1.85s/it] 66%|██████▌   | 2034/3086 [1:09:08<31:55,  1.82s/it] 66%|██████▌   | 2035/3086 [1:09:10<34:07,  1.95s/it] 66%|██████▌   | 2036/3086 [1:09:12<35:13,  2.01s/it] 66%|██████▌   | 2037/3086 [1:09:15<36:19,  2.08s/it] 66%|██████▌   | 2038/3086 [1:09:17<37:20,  2.14s/it] 66%|██████▌   | 2039/3086 [1:09:18<34:53,  2.00s/it] 66%|██████▌   | 2040/3086 [1:09:20<34:31,  1.98s/it]                                                     {'loss': 0.1953, 'grad_norm': 0.04989448934793472, 'learning_rate': 0.00020337005832793257, 'epoch': 0.66}
 66%|██████▌   | 2040/3086 [1:09:20<34:31,  1.98s/it] 66%|██████▌   | 2041/3086 [1:09:22<34:49,  2.00s/it] 66%|██████▌   | 2042/3086 [1:09:24<33:15,  1.91s/it] 66%|██████▌   | 2043/3086 [1:09:26<34:06,  1.96s/it] 66%|██████▌   | 2044/3086 [1:09:29<36:02,  2.08s/it] 66%|██████▋   | 2045/3086 [1:09:30<34:04,  1.96s/it] 66%|██████▋   | 2046/3086 [1:09:32<34:14,  1.98s/it] 66%|██████▋   | 2047/3086 [1:09:35<36:25,  2.10s/it] 66%|██████▋   | 2048/3086 [1:09:37<38:36,  2.23s/it] 66%|██████▋   | 2049/3086 [1:09:40<40:36,  2.35s/it] 66%|██████▋   | 2050/3086 [1:09:42<41:17,  2.39s/it]                                                     {'loss': 0.2041, 'grad_norm': 0.0387817844748497, 'learning_rate': 0.00020142579390797149, 'epoch': 0.66}
 66%|██████▋   | 2050/3086 [1:09:42<41:17,  2.39s/it] 66%|██████▋   | 2051/3086 [1:09:45<40:37,  2.36s/it] 66%|██████▋   | 2052/3086 [1:09:46<37:31,  2.18s/it] 67%|██████▋   | 2053/3086 [1:09:48<35:56,  2.09s/it] 67%|██████▋   | 2054/3086 [1:09:50<34:04,  1.98s/it] 67%|██████▋   | 2055/3086 [1:09:52<36:04,  2.10s/it] 67%|██████▋   | 2056/3086 [1:09:55<37:35,  2.19s/it] 67%|██████▋   | 2057/3086 [1:09:56<34:47,  2.03s/it] 67%|██████▋   | 2058/3086 [1:09:58<33:49,  1.97s/it] 67%|██████▋   | 2059/3086 [1:10:00<33:03,  1.93s/it] 67%|██████▋   | 2060/3086 [1:10:03<40:25,  2.36s/it]                                                     {'loss': 0.1997, 'grad_norm': 0.0448458306491375, 'learning_rate': 0.00019948152948801035, 'epoch': 0.67}
 67%|██████▋   | 2060/3086 [1:10:03<40:25,  2.36s/it] 67%|██████▋   | 2061/3086 [1:10:06<39:16,  2.30s/it] 67%|██████▋   | 2062/3086 [1:10:07<36:57,  2.17s/it] 67%|██████▋   | 2063/3086 [1:10:10<36:14,  2.13s/it] 67%|██████▋   | 2064/3086 [1:10:12<36:36,  2.15s/it] 67%|██████▋   | 2065/3086 [1:10:14<35:03,  2.06s/it] 67%|██████▋   | 2066/3086 [1:10:15<33:58,  2.00s/it] 67%|██████▋   | 2067/3086 [1:10:17<33:02,  1.95s/it] 67%|██████▋   | 2068/3086 [1:10:19<33:19,  1.96s/it] 67%|██████▋   | 2069/3086 [1:10:22<35:05,  2.07s/it] 67%|██████▋   | 2070/3086 [1:10:24<34:24,  2.03s/it]                                                     {'loss': 0.1984, 'grad_norm': 0.043775223195552826, 'learning_rate': 0.00019753726506804923, 'epoch': 0.67}
 67%|██████▋   | 2070/3086 [1:10:24<34:24,  2.03s/it] 67%|██████▋   | 2071/3086 [1:10:25<33:57,  2.01s/it] 67%|██████▋   | 2072/3086 [1:10:27<32:55,  1.95s/it] 67%|██████▋   | 2073/3086 [1:10:29<32:24,  1.92s/it] 67%|██████▋   | 2074/3086 [1:10:31<33:08,  1.97s/it] 67%|██████▋   | 2075/3086 [1:10:33<31:17,  1.86s/it] 67%|██████▋   | 2076/3086 [1:10:35<30:41,  1.82s/it] 67%|██████▋   | 2077/3086 [1:10:37<31:50,  1.89s/it] 67%|██████▋   | 2078/3086 [1:10:38<31:01,  1.85s/it] 67%|██████▋   | 2079/3086 [1:10:40<31:49,  1.90s/it] 67%|██████▋   | 2080/3086 [1:10:42<31:01,  1.85s/it]                                                     {'loss': 0.2024, 'grad_norm': 0.03756490349769592, 'learning_rate': 0.00019559300064808812, 'epoch': 0.67}
 67%|██████▋   | 2080/3086 [1:10:42<31:01,  1.85s/it] 67%|██████▋   | 2081/3086 [1:10:44<30:27,  1.82s/it] 67%|██████▋   | 2082/3086 [1:10:46<31:25,  1.88s/it] 67%|██████▋   | 2083/3086 [1:10:48<32:26,  1.94s/it] 68%|██████▊   | 2084/3086 [1:10:50<33:04,  1.98s/it] 68%|██████▊   | 2085/3086 [1:10:52<33:05,  1.98s/it] 68%|██████▊   | 2086/3086 [1:10:54<35:22,  2.12s/it] 68%|██████▊   | 2087/3086 [1:10:56<34:57,  2.10s/it] 68%|██████▊   | 2088/3086 [1:10:59<34:55,  2.10s/it] 68%|██████▊   | 2089/3086 [1:11:00<32:56,  1.98s/it] 68%|██████▊   | 2090/3086 [1:11:02<31:38,  1.91s/it]                                                     {'loss': 0.2031, 'grad_norm': 0.03874635323882103, 'learning_rate': 0.00019364873622812703, 'epoch': 0.68}
 68%|██████▊   | 2090/3086 [1:11:02<31:38,  1.91s/it] 68%|██████▊   | 2091/3086 [1:11:04<31:49,  1.92s/it] 68%|██████▊   | 2092/3086 [1:11:07<35:48,  2.16s/it] 68%|██████▊   | 2093/3086 [1:11:08<33:31,  2.03s/it] 68%|██████▊   | 2094/3086 [1:11:11<37:22,  2.26s/it] 68%|██████▊   | 2095/3086 [1:11:13<35:08,  2.13s/it] 68%|██████▊   | 2096/3086 [1:11:15<34:37,  2.10s/it] 68%|██████▊   | 2097/3086 [1:11:17<35:12,  2.14s/it] 68%|██████▊   | 2098/3086 [1:11:19<32:41,  1.99s/it] 68%|██████▊   | 2099/3086 [1:11:21<33:13,  2.02s/it] 68%|██████▊   | 2100/3086 [1:11:23<33:26,  2.04s/it]                                                     {'loss': 0.2012, 'grad_norm': 0.04078051447868347, 'learning_rate': 0.0001917044718081659, 'epoch': 0.68}
 68%|██████▊   | 2100/3086 [1:11:23<33:26,  2.04s/it] 68%|██████▊   | 2101/3086 [1:11:25<30:33,  1.86s/it] 68%|██████▊   | 2102/3086 [1:11:26<30:37,  1.87s/it] 68%|██████▊   | 2103/3086 [1:11:29<33:06,  2.02s/it] 68%|██████▊   | 2104/3086 [1:11:31<34:43,  2.12s/it] 68%|██████▊   | 2105/3086 [1:11:33<33:31,  2.05s/it] 68%|██████▊   | 2106/3086 [1:11:35<33:23,  2.04s/it] 68%|██████▊   | 2107/3086 [1:11:37<33:42,  2.07s/it] 68%|██████▊   | 2108/3086 [1:11:41<40:44,  2.50s/it] 68%|██████▊   | 2109/3086 [1:11:43<37:34,  2.31s/it] 68%|██████▊   | 2110/3086 [1:11:44<35:24,  2.18s/it]                                                     {'loss': 0.2012, 'grad_norm': 0.040942538529634476, 'learning_rate': 0.00018976020738820478, 'epoch': 0.68}
 68%|██████▊   | 2110/3086 [1:11:44<35:24,  2.18s/it] 68%|██████▊   | 2111/3086 [1:11:47<35:58,  2.21s/it] 68%|██████▊   | 2112/3086 [1:11:48<32:55,  2.03s/it] 68%|██████▊   | 2113/3086 [1:11:50<30:26,  1.88s/it] 69%|██████▊   | 2114/3086 [1:11:52<30:06,  1.86s/it] 69%|██████▊   | 2115/3086 [1:11:55<35:03,  2.17s/it] 69%|██████▊   | 2116/3086 [1:11:57<36:08,  2.24s/it] 69%|██████▊   | 2117/3086 [1:11:59<35:31,  2.20s/it] 69%|██████▊   | 2118/3086 [1:12:01<34:18,  2.13s/it] 69%|██████▊   | 2119/3086 [1:12:04<37:14,  2.31s/it] 69%|██████▊   | 2120/3086 [1:12:06<36:45,  2.28s/it]                                                     {'loss': 0.2016, 'grad_norm': 0.04519840329885483, 'learning_rate': 0.00018781594296824367, 'epoch': 0.69}
 69%|██████▊   | 2120/3086 [1:12:06<36:45,  2.28s/it] 69%|██████▊   | 2121/3086 [1:12:08<36:32,  2.27s/it] 69%|██████▉   | 2122/3086 [1:12:10<35:30,  2.21s/it] 69%|██████▉   | 2123/3086 [1:12:12<32:02,  2.00s/it] 69%|██████▉   | 2124/3086 [1:12:14<31:59,  1.99s/it] 69%|██████▉   | 2125/3086 [1:12:16<31:35,  1.97s/it] 69%|██████▉   | 2126/3086 [1:12:17<30:33,  1.91s/it] 69%|██████▉   | 2127/3086 [1:12:19<30:42,  1.92s/it] 69%|██████▉   | 2128/3086 [1:12:21<29:55,  1.87s/it] 69%|██████▉   | 2129/3086 [1:12:23<29:02,  1.82s/it] 69%|██████▉   | 2130/3086 [1:12:25<28:49,  1.81s/it]                                                     {'loss': 0.2037, 'grad_norm': 0.042521871626377106, 'learning_rate': 0.00018587167854828255, 'epoch': 0.69}
 69%|██████▉   | 2130/3086 [1:12:25<28:49,  1.81s/it] 69%|██████▉   | 2131/3086 [1:12:27<33:12,  2.09s/it] 69%|██████▉   | 2132/3086 [1:12:30<33:38,  2.12s/it] 69%|██████▉   | 2133/3086 [1:12:32<32:47,  2.06s/it] 69%|██████▉   | 2134/3086 [1:12:34<33:55,  2.14s/it] 69%|██████▉   | 2135/3086 [1:12:35<31:25,  1.98s/it] 69%|██████▉   | 2136/3086 [1:12:37<31:21,  1.98s/it] 69%|██████▉   | 2137/3086 [1:12:40<32:08,  2.03s/it] 69%|██████▉   | 2138/3086 [1:12:42<31:50,  2.02s/it] 69%|██████▉   | 2139/3086 [1:12:43<31:13,  1.98s/it] 69%|██████▉   | 2140/3086 [1:12:46<32:37,  2.07s/it]                                                     {'loss': 0.1981, 'grad_norm': 0.043165434151887894, 'learning_rate': 0.00018392741412832141, 'epoch': 0.69}
 69%|██████▉   | 2140/3086 [1:12:46<32:37,  2.07s/it] 69%|██████▉   | 2141/3086 [1:12:48<31:29,  2.00s/it] 69%|██████▉   | 2142/3086 [1:12:50<34:24,  2.19s/it] 69%|██████▉   | 2143/3086 [1:12:52<31:48,  2.02s/it] 69%|██████▉   | 2144/3086 [1:12:54<34:00,  2.17s/it] 70%|██████▉   | 2145/3086 [1:12:57<37:00,  2.36s/it] 70%|██████▉   | 2146/3086 [1:12:59<34:20,  2.19s/it] 70%|██████▉   | 2147/3086 [1:13:01<31:36,  2.02s/it] 70%|██████▉   | 2148/3086 [1:13:03<31:19,  2.00s/it] 70%|██████▉   | 2149/3086 [1:13:05<32:51,  2.10s/it] 70%|██████▉   | 2150/3086 [1:13:07<32:18,  2.07s/it]                                                     {'loss': 0.1997, 'grad_norm': 0.036057841032743454, 'learning_rate': 0.00018198314970836033, 'epoch': 0.7}
 70%|██████▉   | 2150/3086 [1:13:07<32:18,  2.07s/it] 70%|██████▉   | 2151/3086 [1:13:09<31:36,  2.03s/it] 70%|██████▉   | 2152/3086 [1:13:11<31:20,  2.01s/it] 70%|██████▉   | 2153/3086 [1:13:13<30:23,  1.95s/it] 70%|██████▉   | 2154/3086 [1:13:14<29:43,  1.91s/it] 70%|██████▉   | 2155/3086 [1:13:16<29:33,  1.90s/it] 70%|██████▉   | 2156/3086 [1:13:19<33:18,  2.15s/it] 70%|██████▉   | 2157/3086 [1:13:21<33:25,  2.16s/it] 70%|██████▉   | 2158/3086 [1:13:24<35:29,  2.29s/it] 70%|██████▉   | 2159/3086 [1:13:26<34:51,  2.26s/it] 70%|██████▉   | 2160/3086 [1:13:28<32:23,  2.10s/it]                                                     {'loss': 0.203, 'grad_norm': 0.043502166867256165, 'learning_rate': 0.00018003888528839922, 'epoch': 0.7}
 70%|██████▉   | 2160/3086 [1:13:28<32:23,  2.10s/it] 70%|███████   | 2161/3086 [1:13:30<34:00,  2.21s/it] 70%|███████   | 2162/3086 [1:13:32<33:37,  2.18s/it] 70%|███████   | 2163/3086 [1:13:35<35:41,  2.32s/it] 70%|███████   | 2164/3086 [1:13:37<35:03,  2.28s/it] 70%|███████   | 2165/3086 [1:13:39<34:37,  2.26s/it] 70%|███████   | 2166/3086 [1:13:41<33:02,  2.15s/it] 70%|███████   | 2167/3086 [1:13:44<34:31,  2.25s/it] 70%|███████   | 2168/3086 [1:13:46<34:02,  2.22s/it] 70%|███████   | 2169/3086 [1:13:48<31:46,  2.08s/it] 70%|███████   | 2170/3086 [1:13:50<35:09,  2.30s/it]                                                     {'loss': 0.2021, 'grad_norm': 0.04066487401723862, 'learning_rate': 0.00017809462086843808, 'epoch': 0.7}
 70%|███████   | 2170/3086 [1:13:50<35:09,  2.30s/it] 70%|███████   | 2171/3086 [1:13:52<31:54,  2.09s/it] 70%|███████   | 2172/3086 [1:13:54<32:11,  2.11s/it] 70%|███████   | 2173/3086 [1:13:57<35:09,  2.31s/it] 70%|███████   | 2174/3086 [1:13:59<34:33,  2.27s/it] 70%|███████   | 2175/3086 [1:14:02<35:02,  2.31s/it] 71%|███████   | 2176/3086 [1:14:03<32:44,  2.16s/it] 71%|███████   | 2177/3086 [1:14:05<30:43,  2.03s/it] 71%|███████   | 2178/3086 [1:14:07<31:11,  2.06s/it] 71%|███████   | 2179/3086 [1:14:09<31:53,  2.11s/it] 71%|███████   | 2180/3086 [1:14:11<31:21,  2.08s/it]                                                     {'loss': 0.202, 'grad_norm': 0.03937356546521187, 'learning_rate': 0.00017615035644847696, 'epoch': 0.71}
 71%|███████   | 2180/3086 [1:14:11<31:21,  2.08s/it] 71%|███████   | 2181/3086 [1:14:14<32:07,  2.13s/it] 71%|███████   | 2182/3086 [1:14:16<31:33,  2.09s/it] 71%|███████   | 2183/3086 [1:14:19<36:18,  2.41s/it] 71%|███████   | 2184/3086 [1:14:20<32:38,  2.17s/it] 71%|███████   | 2185/3086 [1:14:23<32:10,  2.14s/it] 71%|███████   | 2186/3086 [1:14:24<29:37,  1.98s/it] 71%|███████   | 2187/3086 [1:14:26<29:26,  1.97s/it] 71%|███████   | 2188/3086 [1:14:28<28:16,  1.89s/it] 71%|███████   | 2189/3086 [1:14:30<29:24,  1.97s/it] 71%|███████   | 2190/3086 [1:14:32<28:06,  1.88s/it]                                                     {'loss': 0.1947, 'grad_norm': 0.039871782064437866, 'learning_rate': 0.00017420609202851588, 'epoch': 0.71}
 71%|███████   | 2190/3086 [1:14:32<28:06,  1.88s/it] 71%|███████   | 2191/3086 [1:14:33<27:07,  1.82s/it] 71%|███████   | 2192/3086 [1:14:35<26:17,  1.76s/it] 71%|███████   | 2193/3086 [1:14:37<26:05,  1.75s/it] 71%|███████   | 2194/3086 [1:14:39<26:32,  1.79s/it] 71%|███████   | 2195/3086 [1:14:40<26:05,  1.76s/it] 71%|███████   | 2196/3086 [1:14:42<26:59,  1.82s/it] 71%|███████   | 2197/3086 [1:14:45<31:04,  2.10s/it] 71%|███████   | 2198/3086 [1:14:47<31:50,  2.15s/it] 71%|███████▏  | 2199/3086 [1:14:49<31:09,  2.11s/it] 71%|███████▏  | 2200/3086 [1:14:51<30:04,  2.04s/it]                                                     {'loss': 0.1996, 'grad_norm': 0.042377885431051254, 'learning_rate': 0.00017226182760855476, 'epoch': 0.71}
 71%|███████▏  | 2200/3086 [1:14:51<30:04,  2.04s/it] 71%|███████▏  | 2201/3086 [1:14:53<27:52,  1.89s/it] 71%|███████▏  | 2202/3086 [1:14:54<27:29,  1.87s/it] 71%|███████▏  | 2203/3086 [1:14:57<30:02,  2.04s/it] 71%|███████▏  | 2204/3086 [1:14:59<31:32,  2.15s/it] 71%|███████▏  | 2205/3086 [1:15:02<33:40,  2.29s/it] 71%|███████▏  | 2206/3086 [1:15:05<35:26,  2.42s/it] 72%|███████▏  | 2207/3086 [1:15:07<34:12,  2.34s/it] 72%|███████▏  | 2208/3086 [1:15:09<32:48,  2.24s/it] 72%|███████▏  | 2209/3086 [1:15:11<33:53,  2.32s/it] 72%|███████▏  | 2210/3086 [1:15:13<31:45,  2.17s/it]                                                     {'loss': 0.2036, 'grad_norm': 0.03911510854959488, 'learning_rate': 0.00017031756318859362, 'epoch': 0.72}
 72%|███████▏  | 2210/3086 [1:15:13<31:45,  2.17s/it] 72%|███████▏  | 2211/3086 [1:15:15<31:48,  2.18s/it] 72%|███████▏  | 2212/3086 [1:15:17<29:15,  2.01s/it] 72%|███████▏  | 2213/3086 [1:15:19<28:46,  1.98s/it] 72%|███████▏  | 2214/3086 [1:15:21<28:52,  1.99s/it] 72%|███████▏  | 2215/3086 [1:15:23<31:21,  2.16s/it] 72%|███████▏  | 2216/3086 [1:15:25<30:55,  2.13s/it] 72%|███████▏  | 2217/3086 [1:15:28<31:46,  2.19s/it] 72%|███████▏  | 2218/3086 [1:15:30<30:30,  2.11s/it] 72%|███████▏  | 2219/3086 [1:15:32<29:26,  2.04s/it] 72%|███████▏  | 2220/3086 [1:15:33<28:41,  1.99s/it]                                                     {'loss': 0.1998, 'grad_norm': 0.044753361493349075, 'learning_rate': 0.0001683732987686325, 'epoch': 0.72}
 72%|███████▏  | 2220/3086 [1:15:33<28:41,  1.99s/it] 72%|███████▏  | 2221/3086 [1:15:35<27:34,  1.91s/it] 72%|███████▏  | 2222/3086 [1:15:37<28:46,  2.00s/it] 72%|███████▏  | 2223/3086 [1:15:39<28:02,  1.95s/it] 72%|███████▏  | 2224/3086 [1:15:41<27:12,  1.89s/it] 72%|███████▏  | 2225/3086 [1:15:43<29:34,  2.06s/it] 72%|███████▏  | 2226/3086 [1:15:45<29:19,  2.05s/it] 72%|███████▏  | 2227/3086 [1:15:47<28:53,  2.02s/it] 72%|███████▏  | 2228/3086 [1:15:49<27:31,  1.92s/it] 72%|███████▏  | 2229/3086 [1:15:51<27:39,  1.94s/it] 72%|███████▏  | 2230/3086 [1:15:53<29:31,  2.07s/it]                                                     {'loss': 0.1984, 'grad_norm': 0.04379877820611, 'learning_rate': 0.0001664290343486714, 'epoch': 0.72}
 72%|███████▏  | 2230/3086 [1:15:53<29:31,  2.07s/it] 72%|███████▏  | 2231/3086 [1:15:55<29:14,  2.05s/it] 72%|███████▏  | 2232/3086 [1:15:58<29:17,  2.06s/it] 72%|███████▏  | 2233/3086 [1:16:00<29:52,  2.10s/it] 72%|███████▏  | 2234/3086 [1:16:02<28:38,  2.02s/it] 72%|███████▏  | 2235/3086 [1:16:05<33:19,  2.35s/it] 72%|███████▏  | 2236/3086 [1:16:06<30:50,  2.18s/it] 72%|███████▏  | 2237/3086 [1:16:08<30:07,  2.13s/it] 73%|███████▎  | 2238/3086 [1:16:10<27:52,  1.97s/it] 73%|███████▎  | 2239/3086 [1:16:13<31:40,  2.24s/it] 73%|███████▎  | 2240/3086 [1:16:15<30:54,  2.19s/it]                                                     {'loss': 0.201, 'grad_norm': 0.03886071592569351, 'learning_rate': 0.0001644847699287103, 'epoch': 0.73}
 73%|███████▎  | 2240/3086 [1:16:15<30:54,  2.19s/it] 73%|███████▎  | 2241/3086 [1:16:17<30:31,  2.17s/it] 73%|███████▎  | 2242/3086 [1:16:19<27:48,  1.98s/it] 73%|███████▎  | 2243/3086 [1:16:21<30:26,  2.17s/it] 73%|███████▎  | 2244/3086 [1:16:23<28:33,  2.04s/it] 73%|███████▎  | 2245/3086 [1:16:25<29:19,  2.09s/it] 73%|███████▎  | 2246/3086 [1:16:28<30:24,  2.17s/it] 73%|███████▎  | 2247/3086 [1:16:29<28:06,  2.01s/it] 73%|███████▎  | 2248/3086 [1:16:31<28:57,  2.07s/it] 73%|███████▎  | 2249/3086 [1:16:34<29:35,  2.12s/it] 73%|███████▎  | 2250/3086 [1:16:36<28:59,  2.08s/it]                                                     {'loss': 0.1955, 'grad_norm': 0.04119976982474327, 'learning_rate': 0.00016254050550874917, 'epoch': 0.73}
 73%|███████▎  | 2250/3086 [1:16:36<28:59,  2.08s/it] 73%|███████▎  | 2251/3086 [1:16:38<28:05,  2.02s/it] 73%|███████▎  | 2252/3086 [1:16:40<30:11,  2.17s/it] 73%|███████▎  | 2253/3086 [1:16:42<28:40,  2.06s/it] 73%|███████▎  | 2254/3086 [1:16:45<31:53,  2.30s/it] 73%|███████▎  | 2255/3086 [1:16:47<31:39,  2.29s/it] 73%|███████▎  | 2256/3086 [1:16:49<29:43,  2.15s/it] 73%|███████▎  | 2257/3086 [1:16:51<29:36,  2.14s/it] 73%|███████▎  | 2258/3086 [1:16:53<30:38,  2.22s/it] 73%|███████▎  | 2259/3086 [1:16:56<32:06,  2.33s/it] 73%|███████▎  | 2260/3086 [1:16:58<28:54,  2.10s/it]                                                     {'loss': 0.1964, 'grad_norm': 0.04023805633187294, 'learning_rate': 0.00016059624108878806, 'epoch': 0.73}
 73%|███████▎  | 2260/3086 [1:16:58<28:54,  2.10s/it] 73%|███████▎  | 2261/3086 [1:17:00<28:29,  2.07s/it] 73%|███████▎  | 2262/3086 [1:17:02<30:47,  2.24s/it] 73%|███████▎  | 2263/3086 [1:17:04<29:35,  2.16s/it] 73%|███████▎  | 2264/3086 [1:17:06<29:08,  2.13s/it] 73%|███████▎  | 2265/3086 [1:17:09<32:45,  2.39s/it] 73%|███████▎  | 2266/3086 [1:17:11<30:30,  2.23s/it] 73%|███████▎  | 2267/3086 [1:17:13<28:49,  2.11s/it] 73%|███████▎  | 2268/3086 [1:17:15<26:48,  1.97s/it] 74%|███████▎  | 2269/3086 [1:17:16<26:36,  1.95s/it] 74%|███████▎  | 2270/3086 [1:17:18<25:33,  1.88s/it]                                                     {'loss': 0.2012, 'grad_norm': 0.04779024422168732, 'learning_rate': 0.00015865197666882695, 'epoch': 0.74}
 74%|███████▎  | 2270/3086 [1:17:18<25:33,  1.88s/it] 74%|███████▎  | 2271/3086 [1:17:20<25:39,  1.89s/it] 74%|███████▎  | 2272/3086 [1:17:22<26:13,  1.93s/it] 74%|███████▎  | 2273/3086 [1:17:24<25:57,  1.92s/it] 74%|███████▎  | 2274/3086 [1:17:25<24:21,  1.80s/it] 74%|███████▎  | 2275/3086 [1:17:28<25:15,  1.87s/it] 74%|███████▍  | 2276/3086 [1:17:29<24:06,  1.79s/it] 74%|███████▍  | 2277/3086 [1:17:31<23:03,  1.71s/it] 74%|███████▍  | 2278/3086 [1:17:32<22:55,  1.70s/it] 74%|███████▍  | 2279/3086 [1:17:34<24:24,  1.82s/it] 74%|███████▍  | 2280/3086 [1:17:36<24:21,  1.81s/it]                                                     {'loss': 0.1977, 'grad_norm': 0.040576305240392685, 'learning_rate': 0.00015670771224886586, 'epoch': 0.74}
 74%|███████▍  | 2280/3086 [1:17:36<24:21,  1.81s/it] 74%|███████▍  | 2281/3086 [1:17:38<26:04,  1.94s/it] 74%|███████▍  | 2282/3086 [1:17:40<25:57,  1.94s/it] 74%|███████▍  | 2283/3086 [1:17:42<25:29,  1.91s/it] 74%|███████▍  | 2284/3086 [1:17:44<26:10,  1.96s/it] 74%|███████▍  | 2285/3086 [1:17:46<26:09,  1.96s/it] 74%|███████▍  | 2286/3086 [1:17:48<24:58,  1.87s/it] 74%|███████▍  | 2287/3086 [1:17:50<24:06,  1.81s/it] 74%|███████▍  | 2288/3086 [1:17:52<25:18,  1.90s/it] 74%|███████▍  | 2289/3086 [1:17:53<24:27,  1.84s/it] 74%|███████▍  | 2290/3086 [1:17:55<24:40,  1.86s/it]                                                     {'loss': 0.1967, 'grad_norm': 0.04313487187027931, 'learning_rate': 0.00015476344782890472, 'epoch': 0.74}
 74%|███████▍  | 2290/3086 [1:17:55<24:40,  1.86s/it] 74%|███████▍  | 2291/3086 [1:17:57<24:06,  1.82s/it] 74%|███████▍  | 2292/3086 [1:17:59<25:00,  1.89s/it] 74%|███████▍  | 2293/3086 [1:18:01<24:37,  1.86s/it] 74%|███████▍  | 2294/3086 [1:18:03<25:51,  1.96s/it] 74%|███████▍  | 2295/3086 [1:18:05<23:56,  1.82s/it] 74%|███████▍  | 2296/3086 [1:18:07<24:42,  1.88s/it] 74%|███████▍  | 2297/3086 [1:18:09<26:14,  2.00s/it] 74%|███████▍  | 2298/3086 [1:18:11<27:30,  2.09s/it] 74%|███████▍  | 2299/3086 [1:18:13<28:05,  2.14s/it] 75%|███████▍  | 2300/3086 [1:18:15<26:39,  2.03s/it]                                                     {'loss': 0.1999, 'grad_norm': 0.0446772426366806, 'learning_rate': 0.0001528191834089436, 'epoch': 0.75}
 75%|███████▍  | 2300/3086 [1:18:15<26:39,  2.03s/it] 75%|███████▍  | 2301/3086 [1:18:17<25:18,  1.93s/it] 75%|███████▍  | 2302/3086 [1:18:19<24:30,  1.88s/it] 75%|███████▍  | 2303/3086 [1:18:21<25:24,  1.95s/it] 75%|███████▍  | 2304/3086 [1:18:23<25:22,  1.95s/it] 75%|███████▍  | 2305/3086 [1:18:24<24:40,  1.90s/it] 75%|███████▍  | 2306/3086 [1:18:27<25:53,  1.99s/it] 75%|███████▍  | 2307/3086 [1:18:29<25:25,  1.96s/it] 75%|███████▍  | 2308/3086 [1:18:30<24:21,  1.88s/it] 75%|███████▍  | 2309/3086 [1:18:32<24:57,  1.93s/it] 75%|███████▍  | 2310/3086 [1:18:34<24:22,  1.88s/it]                                                     {'loss': 0.1971, 'grad_norm': 0.04691051319241524, 'learning_rate': 0.0001508749189889825, 'epoch': 0.75}
 75%|███████▍  | 2310/3086 [1:18:34<24:22,  1.88s/it] 75%|███████▍  | 2311/3086 [1:18:36<23:50,  1.85s/it] 75%|███████▍  | 2312/3086 [1:18:38<23:13,  1.80s/it] 75%|███████▍  | 2313/3086 [1:18:40<24:14,  1.88s/it] 75%|███████▍  | 2314/3086 [1:18:42<24:28,  1.90s/it] 75%|███████▌  | 2315/3086 [1:18:43<24:05,  1.87s/it] 75%|███████▌  | 2316/3086 [1:18:45<23:55,  1.86s/it] 75%|███████▌  | 2317/3086 [1:18:47<23:09,  1.81s/it] 75%|███████▌  | 2318/3086 [1:18:49<23:25,  1.83s/it] 75%|███████▌  | 2319/3086 [1:18:51<25:30,  1.99s/it] 75%|███████▌  | 2320/3086 [1:18:53<24:45,  1.94s/it]                                                     {'loss': 0.1992, 'grad_norm': 0.04239296913146973, 'learning_rate': 0.00014893065456902138, 'epoch': 0.75}
 75%|███████▌  | 2320/3086 [1:18:53<24:45,  1.94s/it] 75%|███████▌  | 2321/3086 [1:18:55<25:00,  1.96s/it] 75%|███████▌  | 2322/3086 [1:18:57<25:12,  1.98s/it] 75%|███████▌  | 2323/3086 [1:18:59<23:39,  1.86s/it] 75%|███████▌  | 2324/3086 [1:19:00<23:30,  1.85s/it] 75%|███████▌  | 2325/3086 [1:19:03<26:15,  2.07s/it] 75%|███████▌  | 2326/3086 [1:19:05<26:09,  2.07s/it] 75%|███████▌  | 2327/3086 [1:19:07<27:17,  2.16s/it] 75%|███████▌  | 2328/3086 [1:19:10<27:12,  2.15s/it] 75%|███████▌  | 2329/3086 [1:19:12<27:02,  2.14s/it] 76%|███████▌  | 2330/3086 [1:19:14<25:55,  2.06s/it]                                                     {'loss': 0.1976, 'grad_norm': 0.04328874126076698, 'learning_rate': 0.00014698639014906024, 'epoch': 0.76}
 76%|███████▌  | 2330/3086 [1:19:14<25:55,  2.06s/it] 76%|███████▌  | 2331/3086 [1:19:16<27:10,  2.16s/it] 76%|███████▌  | 2332/3086 [1:19:18<26:41,  2.12s/it] 76%|███████▌  | 2333/3086 [1:19:20<26:30,  2.11s/it] 76%|███████▌  | 2334/3086 [1:19:22<27:04,  2.16s/it] 76%|███████▌  | 2335/3086 [1:19:24<26:11,  2.09s/it] 76%|███████▌  | 2336/3086 [1:19:26<24:17,  1.94s/it] 76%|███████▌  | 2337/3086 [1:19:28<25:23,  2.03s/it] 76%|███████▌  | 2338/3086 [1:19:30<24:40,  1.98s/it] 76%|███████▌  | 2339/3086 [1:19:32<24:37,  1.98s/it] 76%|███████▌  | 2340/3086 [1:19:34<24:28,  1.97s/it]                                                     {'loss': 0.1979, 'grad_norm': 0.041229162365198135, 'learning_rate': 0.00014504212572909915, 'epoch': 0.76}
 76%|███████▌  | 2340/3086 [1:19:34<24:28,  1.97s/it] 76%|███████▌  | 2341/3086 [1:19:36<24:10,  1.95s/it] 76%|███████▌  | 2342/3086 [1:19:38<25:34,  2.06s/it] 76%|███████▌  | 2343/3086 [1:19:40<25:05,  2.03s/it] 76%|███████▌  | 2344/3086 [1:19:42<25:42,  2.08s/it] 76%|███████▌  | 2345/3086 [1:19:44<25:17,  2.05s/it] 76%|███████▌  | 2346/3086 [1:19:46<24:15,  1.97s/it] 76%|███████▌  | 2347/3086 [1:19:48<25:56,  2.11s/it] 76%|███████▌  | 2348/3086 [1:19:51<27:00,  2.20s/it] 76%|███████▌  | 2349/3086 [1:19:53<27:26,  2.23s/it] 76%|███████▌  | 2350/3086 [1:19:55<26:31,  2.16s/it]                                                     {'loss': 0.1962, 'grad_norm': 0.039864927530288696, 'learning_rate': 0.00014309786130913801, 'epoch': 0.76}
 76%|███████▌  | 2350/3086 [1:19:55<26:31,  2.16s/it] 76%|███████▌  | 2351/3086 [1:19:57<24:53,  2.03s/it] 76%|███████▌  | 2352/3086 [1:19:59<24:05,  1.97s/it] 76%|███████▌  | 2353/3086 [1:20:01<23:57,  1.96s/it] 76%|███████▋  | 2354/3086 [1:20:02<23:03,  1.89s/it] 76%|███████▋  | 2355/3086 [1:20:04<23:48,  1.95s/it] 76%|███████▋  | 2356/3086 [1:20:07<25:20,  2.08s/it] 76%|███████▋  | 2357/3086 [1:20:09<23:44,  1.95s/it] 76%|███████▋  | 2358/3086 [1:20:10<23:28,  1.94s/it] 76%|███████▋  | 2359/3086 [1:20:12<23:58,  1.98s/it] 76%|███████▋  | 2360/3086 [1:20:16<28:25,  2.35s/it]                                                     {'loss': 0.1961, 'grad_norm': 0.0400424562394619, 'learning_rate': 0.00014115359688917693, 'epoch': 0.76}
 76%|███████▋  | 2360/3086 [1:20:16<28:25,  2.35s/it] 77%|███████▋  | 2361/3086 [1:20:17<26:15,  2.17s/it] 77%|███████▋  | 2362/3086 [1:20:20<25:55,  2.15s/it] 77%|███████▋  | 2363/3086 [1:20:22<25:47,  2.14s/it] 77%|███████▋  | 2364/3086 [1:20:24<24:49,  2.06s/it] 77%|███████▋  | 2365/3086 [1:20:25<24:10,  2.01s/it] 77%|███████▋  | 2366/3086 [1:20:28<24:35,  2.05s/it] 77%|███████▋  | 2367/3086 [1:20:30<24:35,  2.05s/it] 77%|███████▋  | 2368/3086 [1:20:32<25:04,  2.10s/it] 77%|███████▋  | 2369/3086 [1:20:34<24:00,  2.01s/it] 77%|███████▋  | 2370/3086 [1:20:36<23:45,  1.99s/it]                                                     {'loss': 0.1912, 'grad_norm': 0.03809479996562004, 'learning_rate': 0.0001392093324692158, 'epoch': 0.77}
 77%|███████▋  | 2370/3086 [1:20:36<23:45,  1.99s/it] 77%|███████▋  | 2371/3086 [1:20:39<27:34,  2.31s/it] 77%|███████▋  | 2372/3086 [1:20:41<26:01,  2.19s/it] 77%|███████▋  | 2373/3086 [1:20:43<25:25,  2.14s/it] 77%|███████▋  | 2374/3086 [1:20:45<26:01,  2.19s/it] 77%|███████▋  | 2375/3086 [1:20:47<25:24,  2.14s/it] 77%|███████▋  | 2376/3086 [1:20:49<25:50,  2.18s/it] 77%|███████▋  | 2377/3086 [1:20:51<25:36,  2.17s/it] 77%|███████▋  | 2378/3086 [1:20:53<25:09,  2.13s/it] 77%|███████▋  | 2379/3086 [1:20:56<25:05,  2.13s/it] 77%|███████▋  | 2380/3086 [1:20:57<24:29,  2.08s/it]                                                     {'loss': 0.1975, 'grad_norm': 0.04212237894535065, 'learning_rate': 0.0001372650680492547, 'epoch': 0.77}
 77%|███████▋  | 2380/3086 [1:20:57<24:29,  2.08s/it] 77%|███████▋  | 2381/3086 [1:20:59<23:33,  2.00s/it] 77%|███████▋  | 2382/3086 [1:21:02<26:02,  2.22s/it] 77%|███████▋  | 2383/3086 [1:21:04<24:40,  2.11s/it] 77%|███████▋  | 2384/3086 [1:21:06<23:06,  1.98s/it] 77%|███████▋  | 2385/3086 [1:21:07<22:43,  1.94s/it] 77%|███████▋  | 2386/3086 [1:21:09<22:42,  1.95s/it] 77%|███████▋  | 2387/3086 [1:21:11<22:26,  1.93s/it] 77%|███████▋  | 2388/3086 [1:21:13<21:50,  1.88s/it] 77%|███████▋  | 2389/3086 [1:21:15<21:37,  1.86s/it] 77%|███████▋  | 2390/3086 [1:21:16<20:26,  1.76s/it]                                                     {'loss': 0.1922, 'grad_norm': 0.044351886957883835, 'learning_rate': 0.00013532080362929356, 'epoch': 0.77}
 77%|███████▋  | 2390/3086 [1:21:16<20:26,  1.76s/it] 77%|███████▋  | 2391/3086 [1:21:19<21:57,  1.90s/it] 78%|███████▊  | 2392/3086 [1:21:21<23:34,  2.04s/it] 78%|███████▊  | 2393/3086 [1:21:23<22:28,  1.95s/it] 78%|███████▊  | 2394/3086 [1:21:25<22:12,  1.93s/it] 78%|███████▊  | 2395/3086 [1:21:26<22:12,  1.93s/it] 78%|███████▊  | 2396/3086 [1:21:29<24:04,  2.09s/it] 78%|███████▊  | 2397/3086 [1:21:31<23:02,  2.01s/it] 78%|███████▊  | 2398/3086 [1:21:33<24:32,  2.14s/it] 78%|███████▊  | 2399/3086 [1:21:35<24:22,  2.13s/it] 78%|███████▊  | 2400/3086 [1:21:37<22:55,  2.00s/it]                                                     {'loss': 0.1985, 'grad_norm': 0.04773150011897087, 'learning_rate': 0.00013337653920933245, 'epoch': 0.78}
 78%|███████▊  | 2400/3086 [1:21:37<22:55,  2.00s/it] 78%|███████▊  | 2401/3086 [1:21:39<21:38,  1.90s/it] 78%|███████▊  | 2402/3086 [1:21:41<21:39,  1.90s/it] 78%|███████▊  | 2403/3086 [1:21:43<21:58,  1.93s/it] 78%|███████▊  | 2404/3086 [1:21:44<21:47,  1.92s/it] 78%|███████▊  | 2405/3086 [1:21:46<21:04,  1.86s/it] 78%|███████▊  | 2406/3086 [1:21:48<20:07,  1.78s/it] 78%|███████▊  | 2407/3086 [1:21:50<20:17,  1.79s/it] 78%|███████▊  | 2408/3086 [1:21:51<20:09,  1.78s/it] 78%|███████▊  | 2409/3086 [1:21:55<25:13,  2.24s/it] 78%|███████▊  | 2410/3086 [1:21:56<23:13,  2.06s/it]                                                     {'loss': 0.1919, 'grad_norm': 0.04186856374144554, 'learning_rate': 0.00013143227478937134, 'epoch': 0.78}
 78%|███████▊  | 2410/3086 [1:21:56<23:13,  2.06s/it] 78%|███████▊  | 2411/3086 [1:21:59<24:35,  2.19s/it] 78%|███████▊  | 2412/3086 [1:22:02<26:21,  2.35s/it] 78%|███████▊  | 2413/3086 [1:22:04<26:03,  2.32s/it] 78%|███████▊  | 2414/3086 [1:22:06<24:34,  2.19s/it] 78%|███████▊  | 2415/3086 [1:22:07<23:01,  2.06s/it] 78%|███████▊  | 2416/3086 [1:22:09<22:12,  1.99s/it] 78%|███████▊  | 2417/3086 [1:22:11<21:06,  1.89s/it] 78%|███████▊  | 2418/3086 [1:22:12<19:51,  1.78s/it] 78%|███████▊  | 2419/3086 [1:22:14<20:36,  1.85s/it] 78%|███████▊  | 2420/3086 [1:22:16<20:23,  1.84s/it]                                                     {'loss': 0.1978, 'grad_norm': 0.04131416231393814, 'learning_rate': 0.00012948801036941022, 'epoch': 0.78}
 78%|███████▊  | 2420/3086 [1:22:16<20:23,  1.84s/it] 78%|███████▊  | 2421/3086 [1:22:18<20:25,  1.84s/it] 78%|███████▊  | 2422/3086 [1:22:20<20:15,  1.83s/it] 79%|███████▊  | 2423/3086 [1:22:22<20:22,  1.84s/it] 79%|███████▊  | 2424/3086 [1:22:24<20:40,  1.87s/it] 79%|███████▊  | 2425/3086 [1:22:26<22:15,  2.02s/it] 79%|███████▊  | 2426/3086 [1:22:28<22:40,  2.06s/it] 79%|███████▊  | 2427/3086 [1:22:30<21:36,  1.97s/it] 79%|███████▊  | 2428/3086 [1:22:32<21:03,  1.92s/it] 79%|███████▊  | 2429/3086 [1:22:34<21:07,  1.93s/it] 79%|███████▊  | 2430/3086 [1:22:36<20:31,  1.88s/it]                                                     {'loss': 0.1952, 'grad_norm': 0.05110371485352516, 'learning_rate': 0.0001275437459494491, 'epoch': 0.79}
 79%|███████▊  | 2430/3086 [1:22:36<20:31,  1.88s/it] 79%|███████▉  | 2431/3086 [1:22:37<20:04,  1.84s/it] 79%|███████▉  | 2432/3086 [1:22:40<22:57,  2.11s/it] 79%|███████▉  | 2433/3086 [1:22:42<23:49,  2.19s/it] 79%|███████▉  | 2434/3086 [1:22:44<23:22,  2.15s/it] 79%|███████▉  | 2435/3086 [1:22:47<25:31,  2.35s/it] 79%|███████▉  | 2436/3086 [1:22:49<23:48,  2.20s/it] 79%|███████▉  | 2437/3086 [1:22:51<22:37,  2.09s/it] 79%|███████▉  | 2438/3086 [1:22:53<21:42,  2.01s/it] 79%|███████▉  | 2439/3086 [1:22:55<20:55,  1.94s/it] 79%|███████▉  | 2440/3086 [1:22:56<20:52,  1.94s/it]                                                     {'loss': 0.1971, 'grad_norm': 0.04038797318935394, 'learning_rate': 0.000125599481529488, 'epoch': 0.79}
 79%|███████▉  | 2440/3086 [1:22:56<20:52,  1.94s/it] 79%|███████▉  | 2441/3086 [1:22:59<23:40,  2.20s/it] 79%|███████▉  | 2442/3086 [1:23:01<21:25,  2.00s/it] 79%|███████▉  | 2443/3086 [1:23:03<21:16,  1.98s/it] 79%|███████▉  | 2444/3086 [1:23:05<22:11,  2.07s/it] 79%|███████▉  | 2445/3086 [1:23:07<22:18,  2.09s/it] 79%|███████▉  | 2446/3086 [1:23:10<24:06,  2.26s/it] 79%|███████▉  | 2447/3086 [1:23:12<24:43,  2.32s/it] 79%|███████▉  | 2448/3086 [1:23:14<23:50,  2.24s/it] 79%|███████▉  | 2449/3086 [1:23:16<21:45,  2.05s/it] 79%|███████▉  | 2450/3086 [1:23:18<20:47,  1.96s/it]                                                     {'loss': 0.1982, 'grad_norm': 0.04646964743733406, 'learning_rate': 0.00012365521710952688, 'epoch': 0.79}
 79%|███████▉  | 2450/3086 [1:23:18<20:47,  1.96s/it] 79%|███████▉  | 2451/3086 [1:23:19<19:31,  1.85s/it] 79%|███████▉  | 2452/3086 [1:23:22<21:11,  2.01s/it] 79%|███████▉  | 2453/3086 [1:23:24<21:09,  2.00s/it] 80%|███████▉  | 2454/3086 [1:23:26<23:29,  2.23s/it] 80%|███████▉  | 2455/3086 [1:23:29<23:09,  2.20s/it] 80%|███████▉  | 2456/3086 [1:23:31<24:34,  2.34s/it] 80%|███████▉  | 2457/3086 [1:23:33<24:05,  2.30s/it] 80%|███████▉  | 2458/3086 [1:23:35<22:03,  2.11s/it] 80%|███████▉  | 2459/3086 [1:23:37<22:45,  2.18s/it] 80%|███████▉  | 2460/3086 [1:23:39<21:54,  2.10s/it]                                                     {'loss': 0.1964, 'grad_norm': 0.039075449109077454, 'learning_rate': 0.00012171095268956577, 'epoch': 0.8}
 80%|███████▉  | 2460/3086 [1:23:39<21:54,  2.10s/it] 80%|███████▉  | 2461/3086 [1:23:42<23:21,  2.24s/it] 80%|███████▉  | 2462/3086 [1:23:44<24:02,  2.31s/it] 80%|███████▉  | 2463/3086 [1:23:46<22:09,  2.13s/it] 80%|███████▉  | 2464/3086 [1:23:48<21:12,  2.05s/it] 80%|███████▉  | 2465/3086 [1:23:50<20:13,  1.95s/it] 80%|███████▉  | 2466/3086 [1:23:51<19:20,  1.87s/it] 80%|███████▉  | 2467/3086 [1:23:53<19:18,  1.87s/it] 80%|███████▉  | 2468/3086 [1:23:55<20:23,  1.98s/it] 80%|████████  | 2469/3086 [1:23:58<21:18,  2.07s/it] 80%|████████  | 2470/3086 [1:24:00<21:10,  2.06s/it]                                                     {'loss': 0.201, 'grad_norm': 0.0467960387468338, 'learning_rate': 0.00011976668826960466, 'epoch': 0.8}
 80%|████████  | 2470/3086 [1:24:00<21:10,  2.06s/it] 80%|████████  | 2471/3086 [1:24:02<21:33,  2.10s/it] 80%|████████  | 2472/3086 [1:24:04<22:01,  2.15s/it] 80%|████████  | 2473/3086 [1:24:06<22:03,  2.16s/it] 80%|████████  | 2474/3086 [1:24:09<21:50,  2.14s/it] 80%|████████  | 2475/3086 [1:24:10<20:16,  1.99s/it] 80%|████████  | 2476/3086 [1:24:12<19:39,  1.93s/it] 80%|████████  | 2477/3086 [1:24:14<20:15,  2.00s/it] 80%|████████  | 2478/3086 [1:24:16<19:22,  1.91s/it] 80%|████████  | 2479/3086 [1:24:18<19:08,  1.89s/it] 80%|████████  | 2480/3086 [1:24:20<19:27,  1.93s/it]                                                     {'loss': 0.193, 'grad_norm': 0.03648265823721886, 'learning_rate': 0.00011782242384964355, 'epoch': 0.8}
 80%|████████  | 2480/3086 [1:24:20<19:27,  1.93s/it] 80%|████████  | 2481/3086 [1:24:22<20:58,  2.08s/it] 80%|████████  | 2482/3086 [1:24:24<21:01,  2.09s/it] 80%|████████  | 2483/3086 [1:24:26<20:43,  2.06s/it] 80%|████████  | 2484/3086 [1:24:28<20:32,  2.05s/it] 81%|████████  | 2485/3086 [1:24:31<21:31,  2.15s/it] 81%|████████  | 2486/3086 [1:24:33<21:29,  2.15s/it] 81%|████████  | 2487/3086 [1:24:35<21:21,  2.14s/it] 81%|████████  | 2488/3086 [1:24:37<21:18,  2.14s/it] 81%|████████  | 2489/3086 [1:24:40<22:19,  2.24s/it] 81%|████████  | 2490/3086 [1:24:41<21:05,  2.12s/it]                                                     {'loss': 0.1931, 'grad_norm': 0.03605157881975174, 'learning_rate': 0.00011587815942968243, 'epoch': 0.81}
 81%|████████  | 2490/3086 [1:24:41<21:05,  2.12s/it] 81%|████████  | 2491/3086 [1:24:44<21:30,  2.17s/it] 81%|████████  | 2492/3086 [1:24:46<20:45,  2.10s/it] 81%|████████  | 2493/3086 [1:24:47<19:53,  2.01s/it] 81%|████████  | 2494/3086 [1:24:50<20:32,  2.08s/it] 81%|████████  | 2495/3086 [1:24:52<20:36,  2.09s/it] 81%|████████  | 2496/3086 [1:24:54<20:41,  2.10s/it] 81%|████████  | 2497/3086 [1:24:56<20:34,  2.10s/it] 81%|████████  | 2498/3086 [1:24:58<19:59,  2.04s/it] 81%|████████  | 2499/3086 [1:25:00<18:47,  1.92s/it] 81%|████████  | 2500/3086 [1:25:01<18:11,  1.86s/it]                                                     {'loss': 0.1903, 'grad_norm': 0.041780736297369, 'learning_rate': 0.0001139338950097213, 'epoch': 0.81}
 81%|████████  | 2500/3086 [1:25:01<18:11,  1.86s/it] 81%|████████  | 2501/3086 [1:25:04<19:33,  2.01s/it] 81%|████████  | 2502/3086 [1:25:06<19:38,  2.02s/it] 81%|████████  | 2503/3086 [1:25:08<19:33,  2.01s/it] 81%|████████  | 2504/3086 [1:25:09<18:55,  1.95s/it] 81%|████████  | 2505/3086 [1:25:11<18:50,  1.95s/it] 81%|████████  | 2506/3086 [1:25:13<19:16,  1.99s/it] 81%|████████  | 2507/3086 [1:25:16<20:11,  2.09s/it] 81%|████████▏ | 2508/3086 [1:25:18<21:31,  2.24s/it] 81%|████████▏ | 2509/3086 [1:25:20<20:30,  2.13s/it] 81%|████████▏ | 2510/3086 [1:25:22<20:37,  2.15s/it]                                                     {'loss': 0.1921, 'grad_norm': 0.03797370195388794, 'learning_rate': 0.00011198963058976019, 'epoch': 0.81}
 81%|████████▏ | 2510/3086 [1:25:22<20:37,  2.15s/it] 81%|████████▏ | 2511/3086 [1:25:25<21:30,  2.24s/it] 81%|████████▏ | 2512/3086 [1:25:27<20:38,  2.16s/it] 81%|████████▏ | 2513/3086 [1:25:28<19:05,  2.00s/it] 81%|████████▏ | 2514/3086 [1:25:30<17:59,  1.89s/it] 81%|████████▏ | 2515/3086 [1:25:32<17:24,  1.83s/it] 82%|████████▏ | 2516/3086 [1:25:34<18:59,  2.00s/it] 82%|████████▏ | 2517/3086 [1:25:36<19:34,  2.06s/it] 82%|████████▏ | 2518/3086 [1:25:38<18:44,  1.98s/it] 82%|████████▏ | 2519/3086 [1:25:40<17:33,  1.86s/it] 82%|████████▏ | 2520/3086 [1:25:41<16:52,  1.79s/it]                                                     {'loss': 0.1915, 'grad_norm': 0.04139266535639763, 'learning_rate': 0.00011004536616979908, 'epoch': 0.82}
 82%|████████▏ | 2520/3086 [1:25:41<16:52,  1.79s/it] 82%|████████▏ | 2521/3086 [1:25:43<16:42,  1.77s/it] 82%|████████▏ | 2522/3086 [1:25:45<17:44,  1.89s/it] 82%|████████▏ | 2523/3086 [1:25:47<18:22,  1.96s/it] 82%|████████▏ | 2524/3086 [1:25:49<18:15,  1.95s/it] 82%|████████▏ | 2525/3086 [1:25:51<18:41,  2.00s/it] 82%|████████▏ | 2526/3086 [1:25:53<18:11,  1.95s/it] 82%|████████▏ | 2527/3086 [1:25:56<19:18,  2.07s/it] 82%|████████▏ | 2528/3086 [1:25:58<21:20,  2.29s/it] 82%|████████▏ | 2529/3086 [1:26:01<21:24,  2.31s/it] 82%|████████▏ | 2530/3086 [1:26:03<21:09,  2.28s/it]                                                     {'loss': 0.1937, 'grad_norm': 0.04297924414277077, 'learning_rate': 0.00010810110174983797, 'epoch': 0.82}
 82%|████████▏ | 2530/3086 [1:26:03<21:09,  2.28s/it] 82%|████████▏ | 2531/3086 [1:26:05<20:04,  2.17s/it] 82%|████████▏ | 2532/3086 [1:26:07<18:44,  2.03s/it] 82%|████████▏ | 2533/3086 [1:26:08<17:46,  1.93s/it] 82%|████████▏ | 2534/3086 [1:26:10<18:07,  1.97s/it] 82%|████████▏ | 2535/3086 [1:26:13<19:19,  2.11s/it] 82%|████████▏ | 2536/3086 [1:26:15<19:30,  2.13s/it] 82%|████████▏ | 2537/3086 [1:26:17<19:32,  2.14s/it] 82%|████████▏ | 2538/3086 [1:26:19<18:54,  2.07s/it] 82%|████████▏ | 2539/3086 [1:26:21<17:49,  1.96s/it] 82%|████████▏ | 2540/3086 [1:26:23<19:02,  2.09s/it]                                                     {'loss': 0.1953, 'grad_norm': 0.04140112176537514, 'learning_rate': 0.00010615683732987685, 'epoch': 0.82}
 82%|████████▏ | 2540/3086 [1:26:23<19:02,  2.09s/it] 82%|████████▏ | 2541/3086 [1:26:25<17:36,  1.94s/it] 82%|████████▏ | 2542/3086 [1:26:27<18:14,  2.01s/it] 82%|████████▏ | 2543/3086 [1:26:29<17:15,  1.91s/it] 82%|████████▏ | 2544/3086 [1:26:31<18:07,  2.01s/it] 82%|████████▏ | 2545/3086 [1:26:32<17:04,  1.89s/it] 83%|████████▎ | 2546/3086 [1:26:35<17:26,  1.94s/it] 83%|████████▎ | 2547/3086 [1:26:37<18:05,  2.01s/it] 83%|████████▎ | 2548/3086 [1:26:39<19:57,  2.23s/it] 83%|████████▎ | 2549/3086 [1:26:41<18:55,  2.11s/it] 83%|████████▎ | 2550/3086 [1:26:43<17:45,  1.99s/it]                                                     {'loss': 0.1982, 'grad_norm': 0.0424715019762516, 'learning_rate': 0.00010421257290991574, 'epoch': 0.83}
 83%|████████▎ | 2550/3086 [1:26:43<17:45,  1.99s/it] 83%|████████▎ | 2551/3086 [1:26:45<17:21,  1.95s/it] 83%|████████▎ | 2552/3086 [1:26:47<16:40,  1.87s/it] 83%|████████▎ | 2553/3086 [1:26:48<16:24,  1.85s/it] 83%|████████▎ | 2554/3086 [1:26:51<18:25,  2.08s/it] 83%|████████▎ | 2555/3086 [1:26:53<19:14,  2.17s/it] 83%|████████▎ | 2556/3086 [1:26:56<19:25,  2.20s/it] 83%|████████▎ | 2557/3086 [1:26:57<18:35,  2.11s/it] 83%|████████▎ | 2558/3086 [1:27:00<18:48,  2.14s/it] 83%|████████▎ | 2559/3086 [1:27:02<17:59,  2.05s/it] 83%|████████▎ | 2560/3086 [1:27:03<17:38,  2.01s/it]                                                     {'loss': 0.1954, 'grad_norm': 0.03916003555059433, 'learning_rate': 0.00010226830848995461, 'epoch': 0.83}
 83%|████████▎ | 2560/3086 [1:27:03<17:38,  2.01s/it] 83%|████████▎ | 2561/3086 [1:27:05<17:17,  1.98s/it] 83%|████████▎ | 2562/3086 [1:27:07<16:45,  1.92s/it] 83%|████████▎ | 2563/3086 [1:27:09<16:40,  1.91s/it] 83%|████████▎ | 2564/3086 [1:27:11<16:05,  1.85s/it] 83%|████████▎ | 2565/3086 [1:27:13<16:30,  1.90s/it] 83%|████████▎ | 2566/3086 [1:27:14<15:39,  1.81s/it] 83%|████████▎ | 2567/3086 [1:27:17<17:37,  2.04s/it] 83%|████████▎ | 2568/3086 [1:27:19<17:37,  2.04s/it] 83%|████████▎ | 2569/3086 [1:27:21<17:03,  1.98s/it] 83%|████████▎ | 2570/3086 [1:27:22<16:16,  1.89s/it]                                                     {'loss': 0.1913, 'grad_norm': 0.04803137481212616, 'learning_rate': 0.00010032404406999351, 'epoch': 0.83}
 83%|████████▎ | 2570/3086 [1:27:22<16:16,  1.89s/it] 83%|████████▎ | 2571/3086 [1:27:24<16:31,  1.92s/it] 83%|████████▎ | 2572/3086 [1:27:26<16:04,  1.88s/it] 83%|████████▎ | 2573/3086 [1:27:28<15:58,  1.87s/it] 83%|████████▎ | 2574/3086 [1:27:30<15:45,  1.85s/it] 83%|████████▎ | 2575/3086 [1:27:32<17:37,  2.07s/it] 83%|████████▎ | 2576/3086 [1:27:36<20:01,  2.36s/it] 84%|████████▎ | 2577/3086 [1:27:37<19:00,  2.24s/it] 84%|████████▎ | 2578/3086 [1:27:40<19:30,  2.30s/it] 84%|████████▎ | 2579/3086 [1:27:42<18:12,  2.15s/it] 84%|████████▎ | 2580/3086 [1:27:43<16:31,  1.96s/it]                                                     {'loss': 0.2009, 'grad_norm': 0.05495022237300873, 'learning_rate': 9.837977965003239e-05, 'epoch': 0.84}
 84%|████████▎ | 2580/3086 [1:27:43<16:31,  1.96s/it] 84%|████████▎ | 2581/3086 [1:27:45<15:26,  1.83s/it] 84%|████████▎ | 2582/3086 [1:27:47<15:37,  1.86s/it] 84%|████████▎ | 2583/3086 [1:27:49<16:28,  1.97s/it] 84%|████████▎ | 2584/3086 [1:27:51<16:46,  2.00s/it] 84%|████████▍ | 2585/3086 [1:27:54<18:04,  2.16s/it] 84%|████████▍ | 2586/3086 [1:27:56<18:13,  2.19s/it] 84%|████████▍ | 2587/3086 [1:27:58<17:37,  2.12s/it] 84%|████████▍ | 2588/3086 [1:27:59<15:53,  1.92s/it] 84%|████████▍ | 2589/3086 [1:28:02<17:25,  2.10s/it] 84%|████████▍ | 2590/3086 [1:28:04<16:37,  2.01s/it]                                                     {'loss': 0.1943, 'grad_norm': 0.04762973636388779, 'learning_rate': 9.643551523007129e-05, 'epoch': 0.84}
 84%|████████▍ | 2590/3086 [1:28:04<16:37,  2.01s/it] 84%|████████▍ | 2591/3086 [1:28:05<16:13,  1.97s/it] 84%|████████▍ | 2592/3086 [1:28:07<15:39,  1.90s/it] 84%|████████▍ | 2593/3086 [1:28:09<14:48,  1.80s/it] 84%|████████▍ | 2594/3086 [1:28:11<15:48,  1.93s/it] 84%|████████▍ | 2595/3086 [1:28:14<17:25,  2.13s/it] 84%|████████▍ | 2596/3086 [1:28:16<17:12,  2.11s/it] 84%|████████▍ | 2597/3086 [1:28:18<17:41,  2.17s/it] 84%|████████▍ | 2598/3086 [1:28:20<16:25,  2.02s/it] 84%|████████▍ | 2599/3086 [1:28:21<15:47,  1.95s/it] 84%|████████▍ | 2600/3086 [1:28:23<15:07,  1.87s/it]                                                     {'loss': 0.1942, 'grad_norm': 0.0422794483602047, 'learning_rate': 9.449125081011016e-05, 'epoch': 0.84}
 84%|████████▍ | 2600/3086 [1:28:23<15:07,  1.87s/it] 84%|████████▍ | 2601/3086 [1:28:25<16:16,  2.01s/it] 84%|████████▍ | 2602/3086 [1:28:27<15:56,  1.98s/it] 84%|████████▍ | 2603/3086 [1:28:29<14:48,  1.84s/it] 84%|████████▍ | 2604/3086 [1:28:31<15:26,  1.92s/it] 84%|████████▍ | 2605/3086 [1:28:33<16:23,  2.04s/it] 84%|████████▍ | 2606/3086 [1:28:35<16:27,  2.06s/it] 84%|████████▍ | 2607/3086 [1:28:38<17:28,  2.19s/it] 85%|████████▍ | 2608/3086 [1:28:40<17:22,  2.18s/it] 85%|████████▍ | 2609/3086 [1:28:42<16:34,  2.09s/it] 85%|████████▍ | 2610/3086 [1:28:43<15:11,  1.91s/it]                                                     {'loss': 0.1949, 'grad_norm': 0.0437595508992672, 'learning_rate': 9.254698639014906e-05, 'epoch': 0.85}
 85%|████████▍ | 2610/3086 [1:28:43<15:11,  1.91s/it] 85%|████████▍ | 2611/3086 [1:28:46<16:41,  2.11s/it] 85%|████████▍ | 2612/3086 [1:28:48<17:28,  2.21s/it] 85%|████████▍ | 2613/3086 [1:28:50<16:23,  2.08s/it] 85%|████████▍ | 2614/3086 [1:28:52<15:47,  2.01s/it] 85%|████████▍ | 2615/3086 [1:28:54<15:24,  1.96s/it] 85%|████████▍ | 2616/3086 [1:28:56<15:13,  1.94s/it] 85%|████████▍ | 2617/3086 [1:28:58<14:54,  1.91s/it] 85%|████████▍ | 2618/3086 [1:29:00<15:14,  1.95s/it] 85%|████████▍ | 2619/3086 [1:29:02<15:09,  1.95s/it] 85%|████████▍ | 2620/3086 [1:29:04<15:52,  2.05s/it]                                                     {'loss': 0.1985, 'grad_norm': 0.03774068504571915, 'learning_rate': 9.060272197018794e-05, 'epoch': 0.85}
 85%|████████▍ | 2620/3086 [1:29:04<15:52,  2.05s/it] 85%|████████▍ | 2621/3086 [1:29:05<14:53,  1.92s/it] 85%|████████▍ | 2622/3086 [1:29:07<14:16,  1.85s/it] 85%|████████▍ | 2623/3086 [1:29:09<15:11,  1.97s/it] 85%|████████▌ | 2624/3086 [1:29:11<14:36,  1.90s/it] 85%|████████▌ | 2625/3086 [1:29:14<15:56,  2.08s/it] 85%|████████▌ | 2626/3086 [1:29:16<15:35,  2.03s/it] 85%|████████▌ | 2627/3086 [1:29:18<16:15,  2.12s/it] 85%|████████▌ | 2628/3086 [1:29:20<15:02,  1.97s/it] 85%|████████▌ | 2629/3086 [1:29:22<16:32,  2.17s/it] 85%|████████▌ | 2630/3086 [1:29:24<15:35,  2.05s/it]                                                     {'loss': 0.1912, 'grad_norm': 0.044335972517728806, 'learning_rate': 8.865845755022684e-05, 'epoch': 0.85}
 85%|████████▌ | 2630/3086 [1:29:24<15:35,  2.05s/it] 85%|████████▌ | 2631/3086 [1:29:26<14:30,  1.91s/it] 85%|████████▌ | 2632/3086 [1:29:28<14:48,  1.96s/it] 85%|████████▌ | 2633/3086 [1:29:29<14:05,  1.87s/it] 85%|████████▌ | 2634/3086 [1:29:32<15:03,  2.00s/it] 85%|████████▌ | 2635/3086 [1:29:34<15:35,  2.07s/it] 85%|████████▌ | 2636/3086 [1:29:36<15:15,  2.03s/it] 85%|████████▌ | 2637/3086 [1:29:38<14:55,  1.99s/it] 85%|████████▌ | 2638/3086 [1:29:39<14:23,  1.93s/it] 86%|████████▌ | 2639/3086 [1:29:41<14:21,  1.93s/it] 86%|████████▌ | 2640/3086 [1:29:44<15:16,  2.05s/it]                                                     {'loss': 0.1954, 'grad_norm': 0.04451761394739151, 'learning_rate': 8.671419313026571e-05, 'epoch': 0.86}
 86%|████████▌ | 2640/3086 [1:29:44<15:16,  2.05s/it] 86%|████████▌ | 2641/3086 [1:29:46<14:53,  2.01s/it] 86%|████████▌ | 2642/3086 [1:29:48<15:59,  2.16s/it] 86%|████████▌ | 2643/3086 [1:29:50<14:28,  1.96s/it] 86%|████████▌ | 2644/3086 [1:29:52<14:24,  1.96s/it] 86%|████████▌ | 2645/3086 [1:29:53<14:06,  1.92s/it] 86%|████████▌ | 2646/3086 [1:29:55<13:47,  1.88s/it] 86%|████████▌ | 2647/3086 [1:29:57<13:31,  1.85s/it] 86%|████████▌ | 2648/3086 [1:29:59<13:41,  1.87s/it] 86%|████████▌ | 2649/3086 [1:30:01<13:44,  1.89s/it] 86%|████████▌ | 2650/3086 [1:30:03<15:26,  2.12s/it]                                                     {'loss': 0.1952, 'grad_norm': 0.041421543806791306, 'learning_rate': 8.47699287103046e-05, 'epoch': 0.86}
 86%|████████▌ | 2650/3086 [1:30:03<15:26,  2.12s/it] 86%|████████▌ | 2651/3086 [1:30:06<15:27,  2.13s/it] 86%|████████▌ | 2652/3086 [1:30:07<14:43,  2.04s/it] 86%|████████▌ | 2653/3086 [1:30:10<14:53,  2.06s/it] 86%|████████▌ | 2654/3086 [1:30:12<14:42,  2.04s/it] 86%|████████▌ | 2655/3086 [1:30:14<16:20,  2.28s/it] 86%|████████▌ | 2656/3086 [1:30:16<15:36,  2.18s/it] 86%|████████▌ | 2657/3086 [1:30:19<15:41,  2.19s/it] 86%|████████▌ | 2658/3086 [1:30:21<16:16,  2.28s/it] 86%|████████▌ | 2659/3086 [1:30:24<17:05,  2.40s/it] 86%|████████▌ | 2660/3086 [1:30:26<15:49,  2.23s/it]                                                     {'loss': 0.1986, 'grad_norm': 0.04382132738828659, 'learning_rate': 8.282566429034348e-05, 'epoch': 0.86}
 86%|████████▌ | 2660/3086 [1:30:26<15:49,  2.23s/it] 86%|████████▌ | 2661/3086 [1:30:27<14:46,  2.09s/it] 86%|████████▋ | 2662/3086 [1:30:30<15:19,  2.17s/it] 86%|████████▋ | 2663/3086 [1:30:32<15:07,  2.15s/it] 86%|████████▋ | 2664/3086 [1:30:33<14:14,  2.03s/it] 86%|████████▋ | 2665/3086 [1:30:35<13:17,  1.90s/it] 86%|████████▋ | 2666/3086 [1:30:37<12:54,  1.84s/it] 86%|████████▋ | 2667/3086 [1:30:39<13:33,  1.94s/it] 86%|████████▋ | 2668/3086 [1:30:41<13:36,  1.95s/it] 86%|████████▋ | 2669/3086 [1:30:43<13:33,  1.95s/it] 87%|████████▋ | 2670/3086 [1:30:45<14:05,  2.03s/it]                                                     {'loss': 0.1958, 'grad_norm': 0.041400156915187836, 'learning_rate': 8.088139987038236e-05, 'epoch': 0.87}
 87%|████████▋ | 2670/3086 [1:30:45<14:05,  2.03s/it] 87%|████████▋ | 2671/3086 [1:30:48<15:23,  2.23s/it] 87%|████████▋ | 2672/3086 [1:30:50<14:33,  2.11s/it] 87%|████████▋ | 2673/3086 [1:30:52<14:14,  2.07s/it] 87%|████████▋ | 2674/3086 [1:30:54<14:07,  2.06s/it] 87%|████████▋ | 2675/3086 [1:30:56<15:01,  2.19s/it] 87%|████████▋ | 2676/3086 [1:30:58<14:06,  2.07s/it] 87%|████████▋ | 2677/3086 [1:31:00<13:31,  1.98s/it] 87%|████████▋ | 2678/3086 [1:31:02<14:37,  2.15s/it] 87%|████████▋ | 2679/3086 [1:31:04<13:37,  2.01s/it] 87%|████████▋ | 2680/3086 [1:31:06<13:52,  2.05s/it]                                                     {'loss': 0.195, 'grad_norm': 0.04630884900689125, 'learning_rate': 7.893713545042126e-05, 'epoch': 0.87}
 87%|████████▋ | 2680/3086 [1:31:06<13:52,  2.05s/it] 87%|████████▋ | 2681/3086 [1:31:09<15:11,  2.25s/it] 87%|████████▋ | 2682/3086 [1:31:11<14:16,  2.12s/it] 87%|████████▋ | 2683/3086 [1:31:14<15:55,  2.37s/it] 87%|████████▋ | 2684/3086 [1:31:15<14:52,  2.22s/it] 87%|████████▋ | 2685/3086 [1:31:17<13:40,  2.05s/it] 87%|████████▋ | 2686/3086 [1:31:19<13:17,  1.99s/it] 87%|████████▋ | 2687/3086 [1:31:21<13:41,  2.06s/it] 87%|████████▋ | 2688/3086 [1:31:23<13:37,  2.06s/it] 87%|████████▋ | 2689/3086 [1:31:25<12:38,  1.91s/it] 87%|████████▋ | 2690/3086 [1:31:27<12:44,  1.93s/it]                                                     {'loss': 0.1886, 'grad_norm': 0.03570741415023804, 'learning_rate': 7.699287103046013e-05, 'epoch': 0.87}
 87%|████████▋ | 2690/3086 [1:31:27<12:44,  1.93s/it] 87%|████████▋ | 2691/3086 [1:31:28<12:08,  1.84s/it] 87%|████████▋ | 2692/3086 [1:31:31<12:49,  1.95s/it] 87%|████████▋ | 2693/3086 [1:31:33<13:12,  2.02s/it] 87%|████████▋ | 2694/3086 [1:31:35<13:46,  2.11s/it] 87%|████████▋ | 2695/3086 [1:31:37<13:28,  2.07s/it] 87%|████████▋ | 2696/3086 [1:31:39<12:58,  2.00s/it] 87%|████████▋ | 2697/3086 [1:31:41<12:44,  1.96s/it] 87%|████████▋ | 2698/3086 [1:31:43<12:37,  1.95s/it] 87%|████████▋ | 2699/3086 [1:31:45<12:31,  1.94s/it] 87%|████████▋ | 2700/3086 [1:31:46<12:16,  1.91s/it]                                                     {'loss': 0.2001, 'grad_norm': 0.046803005039691925, 'learning_rate': 7.504860661049902e-05, 'epoch': 0.87}
 87%|████████▋ | 2700/3086 [1:31:46<12:16,  1.91s/it] 88%|████████▊ | 2701/3086 [1:31:48<12:16,  1.91s/it] 88%|████████▊ | 2702/3086 [1:31:50<12:10,  1.90s/it] 88%|████████▊ | 2703/3086 [1:31:52<11:44,  1.84s/it] 88%|████████▊ | 2704/3086 [1:31:54<11:55,  1.87s/it] 88%|████████▊ | 2705/3086 [1:31:56<12:14,  1.93s/it] 88%|████████▊ | 2706/3086 [1:31:58<12:16,  1.94s/it] 88%|████████▊ | 2707/3086 [1:32:00<13:21,  2.11s/it] 88%|████████▊ | 2708/3086 [1:32:02<12:19,  1.96s/it] 88%|████████▊ | 2709/3086 [1:32:04<12:00,  1.91s/it] 88%|████████▊ | 2710/3086 [1:32:05<11:23,  1.82s/it]                                                     {'loss': 0.195, 'grad_norm': 0.051399026066064835, 'learning_rate': 7.31043421905379e-05, 'epoch': 0.88}
 88%|████████▊ | 2710/3086 [1:32:05<11:23,  1.82s/it] 88%|████████▊ | 2711/3086 [1:32:08<11:51,  1.90s/it] 88%|████████▊ | 2712/3086 [1:32:09<11:41,  1.88s/it] 88%|████████▊ | 2713/3086 [1:32:11<11:55,  1.92s/it] 88%|████████▊ | 2714/3086 [1:32:13<11:44,  1.89s/it] 88%|████████▊ | 2715/3086 [1:32:15<11:08,  1.80s/it] 88%|████████▊ | 2716/3086 [1:32:17<11:06,  1.80s/it] 88%|████████▊ | 2717/3086 [1:32:19<11:45,  1.91s/it] 88%|████████▊ | 2718/3086 [1:32:20<11:14,  1.83s/it] 88%|████████▊ | 2719/3086 [1:32:22<11:31,  1.88s/it] 88%|████████▊ | 2720/3086 [1:32:24<11:24,  1.87s/it]                                                     {'loss': 0.1955, 'grad_norm': 0.04368547722697258, 'learning_rate': 7.116007777057679e-05, 'epoch': 0.88}
 88%|████████▊ | 2720/3086 [1:32:24<11:24,  1.87s/it] 88%|████████▊ | 2721/3086 [1:32:26<11:27,  1.88s/it] 88%|████████▊ | 2722/3086 [1:32:28<11:03,  1.82s/it] 88%|████████▊ | 2723/3086 [1:32:30<11:12,  1.85s/it] 88%|████████▊ | 2724/3086 [1:32:32<11:09,  1.85s/it] 88%|████████▊ | 2725/3086 [1:32:34<11:26,  1.90s/it] 88%|████████▊ | 2726/3086 [1:32:35<11:00,  1.83s/it] 88%|████████▊ | 2727/3086 [1:32:37<11:28,  1.92s/it] 88%|████████▊ | 2728/3086 [1:32:39<10:52,  1.82s/it] 88%|████████▊ | 2729/3086 [1:32:41<11:02,  1.86s/it] 88%|████████▊ | 2730/3086 [1:32:43<11:40,  1.97s/it]                                                     {'loss': 0.1964, 'grad_norm': 0.040434569120407104, 'learning_rate': 6.921581335061568e-05, 'epoch': 0.88}
 88%|████████▊ | 2730/3086 [1:32:43<11:40,  1.97s/it] 88%|████████▊ | 2731/3086 [1:32:45<11:32,  1.95s/it] 89%|████████▊ | 2732/3086 [1:32:48<12:19,  2.09s/it] 89%|████████▊ | 2733/3086 [1:32:49<11:57,  2.03s/it] 89%|████████▊ | 2734/3086 [1:32:51<11:20,  1.93s/it] 89%|████████▊ | 2735/3086 [1:32:53<11:50,  2.02s/it] 89%|████████▊ | 2736/3086 [1:32:56<13:12,  2.26s/it] 89%|████████▊ | 2737/3086 [1:32:58<12:14,  2.10s/it] 89%|████████▊ | 2738/3086 [1:33:01<13:11,  2.27s/it] 89%|████████▉ | 2739/3086 [1:33:03<13:25,  2.32s/it] 89%|████████▉ | 2740/3086 [1:33:05<13:02,  2.26s/it]                                                     {'loss': 0.1957, 'grad_norm': 0.044260405004024506, 'learning_rate': 6.727154893065455e-05, 'epoch': 0.89}
 89%|████████▉ | 2740/3086 [1:33:05<13:02,  2.26s/it] 89%|████████▉ | 2741/3086 [1:33:08<13:13,  2.30s/it] 89%|████████▉ | 2742/3086 [1:33:09<12:01,  2.10s/it] 89%|████████▉ | 2743/3086 [1:33:11<12:20,  2.16s/it] 89%|████████▉ | 2744/3086 [1:33:14<12:14,  2.15s/it] 89%|████████▉ | 2745/3086 [1:33:15<11:04,  1.95s/it] 89%|████████▉ | 2746/3086 [1:33:17<10:56,  1.93s/it] 89%|████████▉ | 2747/3086 [1:33:19<11:19,  2.00s/it] 89%|████████▉ | 2748/3086 [1:33:21<10:47,  1.92s/it] 89%|████████▉ | 2749/3086 [1:33:23<11:17,  2.01s/it] 89%|████████▉ | 2750/3086 [1:33:25<11:07,  1.99s/it]                                                     {'loss': 0.1934, 'grad_norm': 0.04042642563581467, 'learning_rate': 6.532728451069344e-05, 'epoch': 0.89}
 89%|████████▉ | 2750/3086 [1:33:25<11:07,  1.99s/it] 89%|████████▉ | 2751/3086 [1:33:27<10:43,  1.92s/it] 89%|████████▉ | 2752/3086 [1:33:29<10:44,  1.93s/it] 89%|████████▉ | 2753/3086 [1:33:30<10:24,  1.87s/it] 89%|████████▉ | 2754/3086 [1:33:33<10:44,  1.94s/it] 89%|████████▉ | 2755/3086 [1:33:35<11:07,  2.02s/it] 89%|████████▉ | 2756/3086 [1:33:37<11:31,  2.10s/it] 89%|████████▉ | 2757/3086 [1:33:39<11:41,  2.13s/it] 89%|████████▉ | 2758/3086 [1:33:41<11:32,  2.11s/it] 89%|████████▉ | 2759/3086 [1:33:43<11:15,  2.07s/it] 89%|████████▉ | 2760/3086 [1:33:45<11:11,  2.06s/it]                                                     {'loss': 0.1917, 'grad_norm': 0.03970865532755852, 'learning_rate': 6.338302009073233e-05, 'epoch': 0.89}
 89%|████████▉ | 2760/3086 [1:33:45<11:11,  2.06s/it] 89%|████████▉ | 2761/3086 [1:33:47<10:58,  2.03s/it] 90%|████████▉ | 2762/3086 [1:33:49<10:38,  1.97s/it] 90%|████████▉ | 2763/3086 [1:33:51<10:15,  1.91s/it] 90%|████████▉ | 2764/3086 [1:33:53<09:50,  1.83s/it] 90%|████████▉ | 2765/3086 [1:33:54<09:57,  1.86s/it] 90%|████████▉ | 2766/3086 [1:33:57<11:39,  2.19s/it] 90%|████████▉ | 2767/3086 [1:34:00<11:34,  2.18s/it] 90%|████████▉ | 2768/3086 [1:34:02<11:45,  2.22s/it] 90%|████████▉ | 2769/3086 [1:34:04<10:57,  2.07s/it] 90%|████████▉ | 2770/3086 [1:34:05<10:18,  1.96s/it]                                                     {'loss': 0.1977, 'grad_norm': 0.0420820377767086, 'learning_rate': 6.143875567077121e-05, 'epoch': 0.9}
 90%|████████▉ | 2770/3086 [1:34:05<10:18,  1.96s/it] 90%|████████▉ | 2771/3086 [1:34:07<10:28,  2.00s/it] 90%|████████▉ | 2772/3086 [1:34:09<10:09,  1.94s/it] 90%|████████▉ | 2773/3086 [1:34:11<09:34,  1.84s/it] 90%|████████▉ | 2774/3086 [1:34:13<09:33,  1.84s/it] 90%|████████▉ | 2775/3086 [1:34:15<09:56,  1.92s/it] 90%|████████▉ | 2776/3086 [1:34:16<09:27,  1.83s/it] 90%|████████▉ | 2777/3086 [1:34:19<10:16,  2.00s/it] 90%|█████████ | 2778/3086 [1:34:21<10:16,  2.00s/it] 90%|█████████ | 2779/3086 [1:34:23<10:14,  2.00s/it] 90%|█████████ | 2780/3086 [1:34:25<10:11,  2.00s/it]                                                     {'loss': 0.1949, 'grad_norm': 0.043528132140636444, 'learning_rate': 5.94944912508101e-05, 'epoch': 0.9}
 90%|█████████ | 2780/3086 [1:34:25<10:11,  2.00s/it] 90%|█████████ | 2781/3086 [1:34:26<09:27,  1.86s/it] 90%|█████████ | 2782/3086 [1:34:28<09:10,  1.81s/it] 90%|█████████ | 2783/3086 [1:34:30<09:27,  1.87s/it] 90%|█████████ | 2784/3086 [1:34:32<09:30,  1.89s/it] 90%|█████████ | 2785/3086 [1:34:34<09:16,  1.85s/it] 90%|█████████ | 2786/3086 [1:34:36<09:19,  1.87s/it] 90%|█████████ | 2787/3086 [1:34:38<09:33,  1.92s/it] 90%|█████████ | 2788/3086 [1:34:39<09:25,  1.90s/it] 90%|█████████ | 2789/3086 [1:34:42<09:42,  1.96s/it] 90%|█████████ | 2790/3086 [1:34:44<09:51,  2.00s/it]                                                     {'loss': 0.1901, 'grad_norm': 0.04821304604411125, 'learning_rate': 5.755022683084899e-05, 'epoch': 0.9}
 90%|█████████ | 2790/3086 [1:34:44<09:51,  2.00s/it] 90%|█████████ | 2791/3086 [1:34:46<09:58,  2.03s/it] 90%|█████████ | 2792/3086 [1:34:48<09:34,  1.95s/it] 91%|█████████ | 2793/3086 [1:34:50<09:41,  1.98s/it] 91%|█████████ | 2794/3086 [1:34:51<09:15,  1.90s/it] 91%|█████████ | 2795/3086 [1:34:53<09:26,  1.95s/it] 91%|█████████ | 2796/3086 [1:34:55<09:23,  1.94s/it] 91%|█████████ | 2797/3086 [1:34:58<09:56,  2.06s/it] 91%|█████████ | 2798/3086 [1:35:00<10:13,  2.13s/it] 91%|█████████ | 2799/3086 [1:35:02<09:54,  2.07s/it] 91%|█████████ | 2800/3086 [1:35:04<09:31,  2.00s/it]                                                     {'loss': 0.1894, 'grad_norm': 0.04073069244623184, 'learning_rate': 5.5605962410887875e-05, 'epoch': 0.91}
 91%|█████████ | 2800/3086 [1:35:04<09:31,  2.00s/it] 91%|█████████ | 2801/3086 [1:35:05<08:55,  1.88s/it] 91%|█████████ | 2802/3086 [1:35:07<08:37,  1.82s/it] 91%|█████████ | 2803/3086 [1:35:09<08:22,  1.78s/it] 91%|█████████ | 2804/3086 [1:35:11<08:31,  1.81s/it] 91%|█████████ | 2805/3086 [1:35:13<09:27,  2.02s/it] 91%|█████████ | 2806/3086 [1:35:15<09:50,  2.11s/it] 91%|█████████ | 2807/3086 [1:35:18<10:15,  2.20s/it] 91%|█████████ | 2808/3086 [1:35:20<10:11,  2.20s/it] 91%|█████████ | 2809/3086 [1:35:22<09:42,  2.10s/it] 91%|█████████ | 2810/3086 [1:35:24<09:17,  2.02s/it]                                                     {'loss': 0.1934, 'grad_norm': 0.048673711717128754, 'learning_rate': 5.366169799092676e-05, 'epoch': 0.91}
 91%|█████████ | 2810/3086 [1:35:24<09:17,  2.02s/it] 91%|█████████ | 2811/3086 [1:35:25<08:44,  1.91s/it] 91%|█████████ | 2812/3086 [1:35:27<08:46,  1.92s/it] 91%|█████████ | 2813/3086 [1:35:30<09:09,  2.01s/it] 91%|█████████ | 2814/3086 [1:35:31<08:41,  1.92s/it] 91%|█████████ | 2815/3086 [1:35:33<08:43,  1.93s/it] 91%|█████████▏| 2816/3086 [1:35:35<08:37,  1.92s/it] 91%|█████████▏| 2817/3086 [1:35:37<08:49,  1.97s/it] 91%|█████████▏| 2818/3086 [1:35:39<09:08,  2.05s/it] 91%|█████████▏| 2819/3086 [1:35:41<09:08,  2.06s/it] 91%|█████████▏| 2820/3086 [1:35:43<08:58,  2.02s/it]                                                     {'loss': 0.2, 'grad_norm': 0.03825550153851509, 'learning_rate': 5.171743357096565e-05, 'epoch': 0.91}
 91%|█████████▏| 2820/3086 [1:35:43<08:58,  2.02s/it] 91%|█████████▏| 2821/3086 [1:35:45<08:54,  2.02s/it] 91%|█████████▏| 2822/3086 [1:35:47<08:53,  2.02s/it] 91%|█████████▏| 2823/3086 [1:35:50<09:02,  2.06s/it] 92%|█████████▏| 2824/3086 [1:35:52<08:54,  2.04s/it] 92%|█████████▏| 2825/3086 [1:35:53<08:42,  2.00s/it] 92%|█████████▏| 2826/3086 [1:35:55<08:10,  1.88s/it] 92%|█████████▏| 2827/3086 [1:35:57<08:27,  1.96s/it] 92%|█████████▏| 2828/3086 [1:35:59<08:02,  1.87s/it] 92%|█████████▏| 2829/3086 [1:36:01<07:58,  1.86s/it] 92%|█████████▏| 2830/3086 [1:36:02<07:36,  1.78s/it]                                                     {'loss': 0.1924, 'grad_norm': 0.043933331966400146, 'learning_rate': 4.9773169151004536e-05, 'epoch': 0.92}
 92%|█████████▏| 2830/3086 [1:36:02<07:36,  1.78s/it] 92%|█████████▏| 2831/3086 [1:36:05<08:37,  2.03s/it] 92%|█████████▏| 2832/3086 [1:36:07<08:21,  1.98s/it] 92%|█████████▏| 2833/3086 [1:36:09<08:22,  1.99s/it] 92%|█████████▏| 2834/3086 [1:36:11<08:51,  2.11s/it] 92%|█████████▏| 2835/3086 [1:36:13<08:24,  2.01s/it] 92%|█████████▏| 2836/3086 [1:36:15<08:49,  2.12s/it] 92%|█████████▏| 2837/3086 [1:36:17<08:05,  1.95s/it] 92%|█████████▏| 2838/3086 [1:36:19<08:42,  2.11s/it] 92%|█████████▏| 2839/3086 [1:36:21<08:39,  2.10s/it] 92%|█████████▏| 2840/3086 [1:36:24<08:35,  2.09s/it]                                                     {'loss': 0.1924, 'grad_norm': 0.04192105680704117, 'learning_rate': 4.7828904731043416e-05, 'epoch': 0.92}
 92%|█████████▏| 2840/3086 [1:36:24<08:35,  2.09s/it] 92%|█████████▏| 2841/3086 [1:36:26<08:24,  2.06s/it] 92%|█████████▏| 2842/3086 [1:36:27<08:14,  2.03s/it] 92%|█████████▏| 2843/3086 [1:36:30<08:21,  2.06s/it] 92%|█████████▏| 2844/3086 [1:36:32<08:45,  2.17s/it] 92%|█████████▏| 2845/3086 [1:36:34<08:32,  2.13s/it] 92%|█████████▏| 2846/3086 [1:36:36<08:40,  2.17s/it] 92%|█████████▏| 2847/3086 [1:36:38<08:05,  2.03s/it] 92%|█████████▏| 2848/3086 [1:36:40<07:51,  1.98s/it] 92%|█████████▏| 2849/3086 [1:36:42<07:32,  1.91s/it] 92%|█████████▏| 2850/3086 [1:36:43<07:05,  1.80s/it]                                                     {'loss': 0.1944, 'grad_norm': 0.04127825051546097, 'learning_rate': 4.58846403110823e-05, 'epoch': 0.92}
 92%|█████████▏| 2850/3086 [1:36:43<07:05,  1.80s/it] 92%|█████████▏| 2851/3086 [1:36:45<07:05,  1.81s/it] 92%|█████████▏| 2852/3086 [1:36:47<07:27,  1.91s/it] 92%|█████████▏| 2853/3086 [1:36:49<07:41,  1.98s/it] 92%|█████████▏| 2854/3086 [1:36:52<08:40,  2.25s/it] 93%|█████████▎| 2855/3086 [1:36:54<08:03,  2.09s/it] 93%|█████████▎| 2856/3086 [1:36:57<08:52,  2.32s/it] 93%|█████████▎| 2857/3086 [1:36:59<08:27,  2.22s/it] 93%|█████████▎| 2858/3086 [1:37:01<07:55,  2.08s/it] 93%|█████████▎| 2859/3086 [1:37:02<07:14,  1.91s/it] 93%|█████████▎| 2860/3086 [1:37:04<07:31,  2.00s/it]                                                     {'loss': 0.1954, 'grad_norm': 0.03938659280538559, 'learning_rate': 4.394037589112119e-05, 'epoch': 0.93}
 93%|█████████▎| 2860/3086 [1:37:04<07:31,  2.00s/it] 93%|█████████▎| 2861/3086 [1:37:06<07:18,  1.95s/it] 93%|█████████▎| 2862/3086 [1:37:08<07:30,  2.01s/it] 93%|█████████▎| 2863/3086 [1:37:10<07:36,  2.05s/it] 93%|█████████▎| 2864/3086 [1:37:13<07:46,  2.10s/it] 93%|█████████▎| 2865/3086 [1:37:14<07:28,  2.03s/it] 93%|█████████▎| 2866/3086 [1:37:16<07:14,  1.97s/it] 93%|█████████▎| 2867/3086 [1:37:18<07:12,  1.98s/it] 93%|█████████▎| 2868/3086 [1:37:20<07:02,  1.94s/it] 93%|█████████▎| 2869/3086 [1:37:23<07:33,  2.09s/it] 93%|█████████▎| 2870/3086 [1:37:25<08:02,  2.24s/it]                                                     {'loss': 0.1908, 'grad_norm': 0.04157136753201485, 'learning_rate': 4.199611147116008e-05, 'epoch': 0.93}
 93%|█████████▎| 2870/3086 [1:37:25<08:02,  2.24s/it] 93%|█████████▎| 2871/3086 [1:37:27<07:54,  2.21s/it] 93%|█████████▎| 2872/3086 [1:37:30<08:05,  2.27s/it] 93%|█████████▎| 2873/3086 [1:37:32<07:52,  2.22s/it] 93%|█████████▎| 2874/3086 [1:37:34<07:51,  2.23s/it] 93%|█████████▎| 2875/3086 [1:37:36<07:30,  2.13s/it] 93%|█████████▎| 2876/3086 [1:37:38<07:07,  2.03s/it] 93%|█████████▎| 2877/3086 [1:37:40<07:02,  2.02s/it] 93%|█████████▎| 2878/3086 [1:37:42<07:04,  2.04s/it] 93%|█████████▎| 2879/3086 [1:37:44<06:51,  1.99s/it] 93%|█████████▎| 2880/3086 [1:37:45<06:37,  1.93s/it]                                                     {'loss': 0.1942, 'grad_norm': 0.03671734035015106, 'learning_rate': 4.0051847051198964e-05, 'epoch': 0.93}
 93%|█████████▎| 2880/3086 [1:37:45<06:37,  1.93s/it] 93%|█████████▎| 2881/3086 [1:37:48<06:42,  1.96s/it] 93%|█████████▎| 2882/3086 [1:37:50<06:42,  1.97s/it] 93%|█████████▎| 2883/3086 [1:37:51<06:06,  1.80s/it] 93%|█████████▎| 2884/3086 [1:37:53<06:46,  2.01s/it] 93%|█████████▎| 2885/3086 [1:37:55<06:47,  2.03s/it] 94%|█████████▎| 2886/3086 [1:37:58<06:53,  2.07s/it] 94%|█████████▎| 2887/3086 [1:38:00<06:58,  2.11s/it] 94%|█████████▎| 2888/3086 [1:38:02<06:46,  2.05s/it] 94%|█████████▎| 2889/3086 [1:38:04<06:59,  2.13s/it] 94%|█████████▎| 2890/3086 [1:38:06<06:54,  2.11s/it]                                                     {'loss': 0.1973, 'grad_norm': 0.042859673500061035, 'learning_rate': 3.810758263123785e-05, 'epoch': 0.94}
 94%|█████████▎| 2890/3086 [1:38:06<06:54,  2.11s/it] 94%|█████████▎| 2891/3086 [1:38:08<06:43,  2.07s/it] 94%|█████████▎| 2892/3086 [1:38:10<06:47,  2.10s/it] 94%|█████████▎| 2893/3086 [1:38:12<06:37,  2.06s/it] 94%|█████████▍| 2894/3086 [1:38:14<06:36,  2.06s/it] 94%|█████████▍| 2895/3086 [1:38:17<06:52,  2.16s/it] 94%|█████████▍| 2896/3086 [1:38:19<07:17,  2.30s/it] 94%|█████████▍| 2897/3086 [1:38:21<06:40,  2.12s/it] 94%|█████████▍| 2898/3086 [1:38:24<07:01,  2.24s/it] 94%|█████████▍| 2899/3086 [1:38:26<07:17,  2.34s/it] 94%|█████████▍| 2900/3086 [1:38:28<06:39,  2.15s/it]                                                     {'loss': 0.1935, 'grad_norm': 0.0421341210603714, 'learning_rate': 3.616331821127673e-05, 'epoch': 0.94}
 94%|█████████▍| 2900/3086 [1:38:28<06:39,  2.15s/it] 94%|█████████▍| 2901/3086 [1:38:30<06:21,  2.06s/it] 94%|█████████▍| 2902/3086 [1:38:32<06:28,  2.11s/it] 94%|█████████▍| 2903/3086 [1:38:34<06:26,  2.11s/it] 94%|█████████▍| 2904/3086 [1:38:35<05:48,  1.91s/it] 94%|█████████▍| 2905/3086 [1:38:38<05:52,  1.95s/it] 94%|█████████▍| 2906/3086 [1:38:40<06:03,  2.02s/it] 94%|█████████▍| 2907/3086 [1:38:41<05:43,  1.92s/it] 94%|█████████▍| 2908/3086 [1:38:44<06:04,  2.05s/it] 94%|█████████▍| 2909/3086 [1:38:46<05:56,  2.01s/it] 94%|█████████▍| 2910/3086 [1:38:48<06:16,  2.14s/it]                                                     {'loss': 0.1898, 'grad_norm': 0.03735170513391495, 'learning_rate': 3.421905379131562e-05, 'epoch': 0.94}
 94%|█████████▍| 2910/3086 [1:38:48<06:16,  2.14s/it] 94%|█████████▍| 2911/3086 [1:38:50<05:49,  2.00s/it] 94%|█████████▍| 2912/3086 [1:38:52<06:12,  2.14s/it] 94%|█████████▍| 2913/3086 [1:38:54<05:42,  1.98s/it] 94%|█████████▍| 2914/3086 [1:38:56<06:04,  2.12s/it] 94%|█████████▍| 2915/3086 [1:38:58<05:53,  2.07s/it] 94%|█████████▍| 2916/3086 [1:39:01<06:10,  2.18s/it] 95%|█████████▍| 2917/3086 [1:39:02<05:48,  2.06s/it] 95%|█████████▍| 2918/3086 [1:39:05<05:49,  2.08s/it] 95%|█████████▍| 2919/3086 [1:39:06<05:30,  1.98s/it] 95%|█████████▍| 2920/3086 [1:39:08<05:19,  1.92s/it]                                                     {'loss': 0.1919, 'grad_norm': 0.04000651836395264, 'learning_rate': 3.2274789371354506e-05, 'epoch': 0.95}
 95%|█████████▍| 2920/3086 [1:39:08<05:19,  1.92s/it] 95%|█████████▍| 2921/3086 [1:39:10<05:21,  1.95s/it] 95%|█████████▍| 2922/3086 [1:39:12<05:12,  1.90s/it] 95%|█████████▍| 2923/3086 [1:39:14<05:40,  2.09s/it] 95%|█████████▍| 2924/3086 [1:39:16<05:31,  2.05s/it] 95%|█████████▍| 2925/3086 [1:39:19<05:32,  2.06s/it] 95%|█████████▍| 2926/3086 [1:39:20<05:17,  1.99s/it] 95%|█████████▍| 2927/3086 [1:39:23<05:27,  2.06s/it] 95%|█████████▍| 2928/3086 [1:39:24<05:12,  1.98s/it] 95%|█████████▍| 2929/3086 [1:39:26<05:09,  1.97s/it] 95%|█████████▍| 2930/3086 [1:39:28<05:18,  2.04s/it]                                                     {'loss': 0.1965, 'grad_norm': 0.04373295232653618, 'learning_rate': 3.033052495139339e-05, 'epoch': 0.95}
 95%|█████████▍| 2930/3086 [1:39:28<05:18,  2.04s/it] 95%|█████████▍| 2931/3086 [1:39:31<05:16,  2.04s/it] 95%|█████████▌| 2932/3086 [1:39:33<05:19,  2.08s/it] 95%|█████████▌| 2933/3086 [1:39:34<04:55,  1.93s/it] 95%|█████████▌| 2934/3086 [1:39:36<04:46,  1.88s/it] 95%|█████████▌| 2935/3086 [1:39:38<04:51,  1.93s/it] 95%|█████████▌| 2936/3086 [1:39:40<04:41,  1.88s/it] 95%|█████████▌| 2937/3086 [1:39:42<04:35,  1.85s/it] 95%|█████████▌| 2938/3086 [1:39:44<04:34,  1.85s/it] 95%|█████████▌| 2939/3086 [1:39:46<04:49,  1.97s/it] 95%|█████████▌| 2940/3086 [1:39:48<04:46,  1.96s/it]                                                     {'loss': 0.1985, 'grad_norm': 0.03978319838643074, 'learning_rate': 2.8386260531432273e-05, 'epoch': 0.95}
 95%|█████████▌| 2940/3086 [1:39:48<04:46,  1.96s/it] 95%|█████████▌| 2941/3086 [1:39:50<04:43,  1.96s/it] 95%|█████████▌| 2942/3086 [1:39:52<05:08,  2.14s/it] 95%|█████████▌| 2943/3086 [1:39:54<04:48,  2.02s/it] 95%|█████████▌| 2944/3086 [1:39:56<04:32,  1.92s/it] 95%|█████████▌| 2945/3086 [1:39:58<04:38,  1.97s/it] 95%|█████████▌| 2946/3086 [1:39:59<04:22,  1.88s/it] 95%|█████████▌| 2947/3086 [1:40:02<04:41,  2.02s/it] 96%|█████████▌| 2948/3086 [1:40:04<04:38,  2.02s/it] 96%|█████████▌| 2949/3086 [1:40:05<04:24,  1.93s/it] 96%|█████████▌| 2950/3086 [1:40:07<04:22,  1.93s/it]                                                     {'loss': 0.1932, 'grad_norm': 0.04724661260843277, 'learning_rate': 2.6441996111471156e-05, 'epoch': 0.96}
 96%|█████████▌| 2950/3086 [1:40:07<04:22,  1.93s/it] 96%|█████████▌| 2951/3086 [1:40:10<04:28,  1.99s/it] 96%|█████████▌| 2952/3086 [1:40:12<04:25,  1.98s/it] 96%|█████████▌| 2953/3086 [1:40:13<04:15,  1.92s/it] 96%|█████████▌| 2954/3086 [1:40:16<04:25,  2.01s/it] 96%|█████████▌| 2955/3086 [1:40:18<04:38,  2.13s/it] 96%|█████████▌| 2956/3086 [1:40:20<04:28,  2.07s/it] 96%|█████████▌| 2957/3086 [1:40:22<04:24,  2.05s/it] 96%|█████████▌| 2958/3086 [1:40:24<04:16,  2.01s/it] 96%|█████████▌| 2959/3086 [1:40:26<04:25,  2.09s/it] 96%|█████████▌| 2960/3086 [1:40:28<04:22,  2.08s/it]                                                     {'loss': 0.1884, 'grad_norm': 0.039394013583660126, 'learning_rate': 2.4497731691510043e-05, 'epoch': 0.96}
 96%|█████████▌| 2960/3086 [1:40:28<04:22,  2.08s/it] 96%|█████████▌| 2961/3086 [1:40:30<04:11,  2.01s/it] 96%|█████████▌| 2962/3086 [1:40:32<04:09,  2.02s/it] 96%|█████████▌| 2963/3086 [1:40:35<04:31,  2.20s/it] 96%|█████████▌| 2964/3086 [1:40:37<04:18,  2.12s/it] 96%|█████████▌| 2965/3086 [1:40:38<04:03,  2.01s/it] 96%|█████████▌| 2966/3086 [1:40:40<03:51,  1.93s/it] 96%|█████████▌| 2967/3086 [1:40:42<03:37,  1.83s/it] 96%|█████████▌| 2968/3086 [1:40:44<03:40,  1.87s/it] 96%|█████████▌| 2969/3086 [1:40:45<03:29,  1.79s/it] 96%|█████████▌| 2970/3086 [1:40:48<03:49,  1.98s/it]                                                     {'loss': 0.193, 'grad_norm': 0.04793674498796463, 'learning_rate': 2.2553467271548927e-05, 'epoch': 0.96}
 96%|█████████▌| 2970/3086 [1:40:48<03:49,  1.98s/it] 96%|█████████▋| 2971/3086 [1:40:50<03:51,  2.01s/it] 96%|█████████▋| 2972/3086 [1:40:52<03:54,  2.06s/it] 96%|█████████▋| 2973/3086 [1:40:54<03:41,  1.96s/it] 96%|█████████▋| 2974/3086 [1:40:56<03:54,  2.09s/it] 96%|█████████▋| 2975/3086 [1:40:58<03:44,  2.02s/it] 96%|█████████▋| 2976/3086 [1:41:00<03:40,  2.01s/it] 96%|█████████▋| 2977/3086 [1:41:02<03:49,  2.11s/it] 97%|█████████▋| 2978/3086 [1:41:04<03:46,  2.10s/it] 97%|█████████▋| 2979/3086 [1:41:07<03:54,  2.19s/it] 97%|█████████▋| 2980/3086 [1:41:08<03:37,  2.05s/it]                                                     {'loss': 0.1904, 'grad_norm': 0.039360031485557556, 'learning_rate': 2.0609202851587814e-05, 'epoch': 0.97}
 97%|█████████▋| 2980/3086 [1:41:08<03:37,  2.05s/it] 97%|█████████▋| 2981/3086 [1:41:10<03:24,  1.95s/it] 97%|█████████▋| 2982/3086 [1:41:12<03:36,  2.08s/it] 97%|█████████▋| 2983/3086 [1:41:15<03:37,  2.11s/it] 97%|█████████▋| 2984/3086 [1:41:17<03:33,  2.09s/it] 97%|█████████▋| 2985/3086 [1:41:19<03:22,  2.01s/it] 97%|█████████▋| 2986/3086 [1:41:22<04:11,  2.51s/it] 97%|█████████▋| 2987/3086 [1:41:24<03:42,  2.25s/it] 97%|█████████▋| 2988/3086 [1:41:26<03:29,  2.13s/it] 97%|█████████▋| 2989/3086 [1:41:28<03:35,  2.22s/it] 97%|█████████▋| 2990/3086 [1:41:30<03:21,  2.10s/it]                                                     {'loss': 0.1957, 'grad_norm': 0.04171238839626312, 'learning_rate': 1.86649384316267e-05, 'epoch': 0.97}
 97%|█████████▋| 2990/3086 [1:41:30<03:21,  2.10s/it] 97%|█████████▋| 2991/3086 [1:41:32<03:06,  1.96s/it] 97%|█████████▋| 2992/3086 [1:41:33<03:01,  1.93s/it] 97%|█████████▋| 2993/3086 [1:41:36<03:07,  2.02s/it] 97%|█████████▋| 2994/3086 [1:41:38<03:19,  2.16s/it] 97%|█████████▋| 2995/3086 [1:41:41<03:22,  2.23s/it] 97%|█████████▋| 2996/3086 [1:41:43<03:20,  2.22s/it] 97%|█████████▋| 2997/3086 [1:41:45<03:13,  2.18s/it] 97%|█████████▋| 2998/3086 [1:41:47<03:07,  2.14s/it] 97%|█████████▋| 2999/3086 [1:41:49<03:00,  2.07s/it] 97%|█████████▋| 3000/3086 [1:41:51<02:57,  2.06s/it]                                                     {'loss': 0.193, 'grad_norm': 0.047828543931245804, 'learning_rate': 1.6720674011665585e-05, 'epoch': 0.97}
 97%|█████████▋| 3000/3086 [1:41:51<02:57,  2.06s/it] 97%|█████████▋| 3001/3086 [1:41:53<02:53,  2.04s/it] 97%|█████████▋| 3002/3086 [1:41:55<03:03,  2.18s/it] 97%|█████████▋| 3003/3086 [1:41:57<03:00,  2.17s/it] 97%|█████████▋| 3004/3086 [1:42:00<02:58,  2.18s/it] 97%|█████████▋| 3005/3086 [1:42:02<02:52,  2.13s/it] 97%|█████████▋| 3006/3086 [1:42:03<02:39,  1.99s/it] 97%|█████████▋| 3007/3086 [1:42:05<02:29,  1.89s/it] 97%|█████████▋| 3008/3086 [1:42:08<02:48,  2.16s/it] 98%|█████████▊| 3009/3086 [1:42:10<02:42,  2.11s/it] 98%|█████████▊| 3010/3086 [1:42:13<02:58,  2.34s/it]                                                     {'loss': 0.1894, 'grad_norm': 0.0364893302321434, 'learning_rate': 1.477640959170447e-05, 'epoch': 0.98}
 98%|█████████▊| 3010/3086 [1:42:13<02:58,  2.34s/it] 98%|█████████▊| 3011/3086 [1:42:15<02:46,  2.22s/it] 98%|█████████▊| 3012/3086 [1:42:17<02:39,  2.16s/it] 98%|█████████▊| 3013/3086 [1:42:19<02:41,  2.21s/it] 98%|█████████▊| 3014/3086 [1:42:21<02:32,  2.12s/it] 98%|█████████▊| 3015/3086 [1:42:23<02:34,  2.18s/it] 98%|█████████▊| 3016/3086 [1:42:25<02:23,  2.05s/it] 98%|█████████▊| 3017/3086 [1:42:26<02:10,  1.88s/it] 98%|█████████▊| 3018/3086 [1:42:28<02:08,  1.90s/it] 98%|█████████▊| 3019/3086 [1:42:30<02:08,  1.91s/it] 98%|█████████▊| 3020/3086 [1:42:32<02:11,  1.99s/it]                                                     {'loss': 0.1907, 'grad_norm': 0.04635123535990715, 'learning_rate': 1.2832145171743355e-05, 'epoch': 0.98}
 98%|█████████▊| 3020/3086 [1:42:32<02:11,  1.99s/it] 98%|█████████▊| 3021/3086 [1:42:34<02:07,  1.96s/it] 98%|█████████▊| 3022/3086 [1:42:36<02:04,  1.95s/it] 98%|█████████▊| 3023/3086 [1:42:38<01:59,  1.90s/it] 98%|█████████▊| 3024/3086 [1:42:40<02:00,  1.95s/it] 98%|█████████▊| 3025/3086 [1:42:42<02:01,  1.99s/it] 98%|█████████▊| 3026/3086 [1:42:44<01:53,  1.90s/it] 98%|█████████▊| 3027/3086 [1:42:46<01:59,  2.03s/it] 98%|█████████▊| 3028/3086 [1:42:48<01:54,  1.98s/it] 98%|█████████▊| 3029/3086 [1:42:50<01:52,  1.98s/it] 98%|█████████▊| 3030/3086 [1:42:52<01:49,  1.96s/it]                                                     {'loss': 0.1924, 'grad_norm': 0.040178924798965454, 'learning_rate': 1.0887880751782242e-05, 'epoch': 0.98}
 98%|█████████▊| 3030/3086 [1:42:52<01:49,  1.96s/it] 98%|█████████▊| 3031/3086 [1:42:55<02:11,  2.39s/it] 98%|█████████▊| 3032/3086 [1:42:58<02:11,  2.44s/it] 98%|█████████▊| 3033/3086 [1:43:00<02:03,  2.32s/it] 98%|█████████▊| 3034/3086 [1:43:02<01:54,  2.20s/it] 98%|█████████▊| 3035/3086 [1:43:04<01:47,  2.11s/it] 98%|█████████▊| 3036/3086 [1:43:06<01:40,  2.01s/it] 98%|█████████▊| 3037/3086 [1:43:08<01:38,  2.02s/it] 98%|█████████▊| 3038/3086 [1:43:09<01:33,  1.96s/it] 98%|█████████▊| 3039/3086 [1:43:11<01:32,  1.96s/it] 99%|█████████▊| 3040/3086 [1:43:13<01:30,  1.97s/it]                                                     {'loss': 0.1922, 'grad_norm': 0.04647098854184151, 'learning_rate': 8.943616331821126e-06, 'epoch': 0.99}
 99%|█████████▊| 3040/3086 [1:43:13<01:30,  1.97s/it] 99%|█████████▊| 3041/3086 [1:43:15<01:24,  1.88s/it] 99%|█████████▊| 3042/3086 [1:43:17<01:23,  1.89s/it] 99%|█████████▊| 3043/3086 [1:43:19<01:17,  1.80s/it] 99%|█████████▊| 3044/3086 [1:43:21<01:27,  2.09s/it] 99%|█████████▊| 3045/3086 [1:43:23<01:20,  1.97s/it] 99%|█████████▊| 3046/3086 [1:43:26<01:27,  2.20s/it] 99%|█████████▊| 3047/3086 [1:43:28<01:22,  2.13s/it] 99%|█████████▉| 3048/3086 [1:43:29<01:13,  1.93s/it] 99%|█████████▉| 3049/3086 [1:43:31<01:07,  1.84s/it] 99%|█████████▉| 3050/3086 [1:43:33<01:08,  1.89s/it]                                                     {'loss': 0.1887, 'grad_norm': 0.0375073216855526, 'learning_rate': 6.999351911860012e-06, 'epoch': 0.99}
 99%|█████████▉| 3050/3086 [1:43:33<01:08,  1.89s/it] 99%|█████████▉| 3051/3086 [1:43:35<01:05,  1.87s/it] 99%|█████████▉| 3052/3086 [1:43:36<01:03,  1.86s/it] 99%|█████████▉| 3053/3086 [1:43:38<01:02,  1.89s/it] 99%|█████████▉| 3054/3086 [1:43:40<00:55,  1.74s/it] 99%|█████████▉| 3055/3086 [1:43:42<00:56,  1.82s/it] 99%|█████████▉| 3056/3086 [1:43:44<00:56,  1.88s/it] 99%|█████████▉| 3057/3086 [1:43:46<00:59,  2.05s/it] 99%|█████████▉| 3058/3086 [1:43:48<00:55,  1.97s/it] 99%|█████████▉| 3059/3086 [1:43:50<00:49,  1.83s/it] 99%|█████████▉| 3060/3086 [1:43:52<00:49,  1.92s/it]                                                     {'loss': 0.1883, 'grad_norm': 0.03853929787874222, 'learning_rate': 5.055087491898897e-06, 'epoch': 0.99}
 99%|█████████▉| 3060/3086 [1:43:52<00:49,  1.92s/it] 99%|█████████▉| 3061/3086 [1:43:53<00:46,  1.85s/it] 99%|█████████▉| 3062/3086 [1:43:56<00:46,  1.93s/it] 99%|█████████▉| 3063/3086 [1:43:57<00:41,  1.79s/it] 99%|█████████▉| 3064/3086 [1:43:59<00:38,  1.77s/it] 99%|█████████▉| 3065/3086 [1:44:01<00:41,  1.99s/it] 99%|█████████▉| 3066/3086 [1:44:04<00:41,  2.09s/it] 99%|█████████▉| 3067/3086 [1:44:05<00:37,  1.96s/it] 99%|█████████▉| 3068/3086 [1:44:07<00:35,  1.98s/it] 99%|█████████▉| 3069/3086 [1:44:09<00:31,  1.85s/it] 99%|█████████▉| 3070/3086 [1:44:11<00:30,  1.92s/it]                                                     {'loss': 0.188, 'grad_norm': 0.036415014415979385, 'learning_rate': 3.1108230719377835e-06, 'epoch': 0.99}
 99%|█████████▉| 3070/3086 [1:44:11<00:30,  1.92s/it]100%|█████████▉| 3071/3086 [1:44:13<00:31,  2.09s/it]100%|█████████▉| 3072/3086 [1:44:16<00:29,  2.11s/it]100%|█████████▉| 3073/3086 [1:44:18<00:27,  2.10s/it]100%|█████████▉| 3074/3086 [1:44:20<00:26,  2.22s/it]100%|█████████▉| 3075/3086 [1:44:22<00:24,  2.24s/it]100%|█████████▉| 3076/3086 [1:44:24<00:21,  2.13s/it]100%|█████████▉| 3077/3086 [1:44:26<00:19,  2.12s/it]100%|█████████▉| 3078/3086 [1:44:29<00:18,  2.28s/it]100%|█████████▉| 3079/3086 [1:44:31<00:15,  2.16s/it]100%|█████████▉| 3080/3086 [1:44:33<00:12,  2.02s/it]                                                     {'loss': 0.1932, 'grad_norm': 0.043882448226213455, 'learning_rate': 1.1665586519766688e-06, 'epoch': 1.0}
100%|█████████▉| 3080/3086 [1:44:33<00:12,  2.02s/it]100%|█████████▉| 3081/3086 [1:44:35<00:10,  2.00s/it]100%|█████████▉| 3082/3086 [1:44:37<00:08,  2.17s/it]100%|█████████▉| 3083/3086 [1:44:39<00:06,  2.19s/it]100%|█████████▉| 3084/3086 [1:44:41<00:04,  2.11s/it]100%|█████████▉| 3085/3086 [1:44:43<00:02,  2.12s/it]100%|██████████| 3086/3086 [1:44:45<00:00,  1.97s/it][INFO|trainer.py:3503] 2024-11-14 03:16:40,022 >> Saving model checkpoint to /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5xl-aug-qwe2.5-math7b-metamath/checkpoint-3086
[INFO|configuration_utils.py:472] 2024-11-14 03:16:40,032 >> Configuration saved in /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5xl-aug-qwe2.5-math7b-metamath/checkpoint-3086/config.json
[INFO|tokenization_utils_base.py:2684] 2024-11-14 03:16:40,192 >> tokenizer config file saved in /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5xl-aug-qwe2.5-math7b-metamath/checkpoint-3086/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2024-11-14 03:16:40,194 >> Special tokens file saved in /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5xl-aug-qwe2.5-math7b-metamath/checkpoint-3086/special_tokens_map.json
[INFO|trainer.py:2394] 2024-11-14 03:16:40,930 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                     {'train_runtime': 6296.9255, 'train_samples_per_second': 62.729, 'train_steps_per_second': 0.49, 'train_loss': 0.3274416996145588, 'epoch': 1.0}
100%|██████████| 3086/3086 [1:44:56<00:00,  1.97s/it]100%|██████████| 3086/3086 [1:44:56<00:00,  2.04s/it]
[INFO|trainer.py:3503] 2024-11-14 03:16:49,203 >> Saving model checkpoint to /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5xl-aug-qwe2.5-math7b-metamath
[INFO|configuration_utils.py:472] 2024-11-14 03:16:49,207 >> Configuration saved in /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5xl-aug-qwe2.5-math7b-metamath/config.json
[INFO|tokenization_utils_base.py:2684] 2024-11-14 03:16:49,392 >> tokenizer config file saved in /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5xl-aug-qwe2.5-math7b-metamath/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2024-11-14 03:16:49,394 >> Special tokens file saved in /root/work/huangxin/nanda/ImplicitTransBridge-master/checkpoints/mt5xl-aug-qwe2.5-math7b-metamath/special_tokens_map.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
***** train metrics *****
  epoch                    =        1.0
  total_flos               =        0GF
  train_loss               =     0.3274
  train_runtime            = 1:44:56.92
  train_samples            =     395000
  train_samples_per_second =     62.729
  train_steps_per_second   =       0.49
[INFO|modelcard.py:449] 2024-11-14 03:16:50,332 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'dataset': {'name': '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json', 'type': '/root/work/huangxin/nanda/QAlign-master/data/metamath/MetaMathQA-395K.json'}}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[2024-11-14 03:16:52,977] [INFO] [launch.py:351:main] Process 251 exits successfully.
[2024-11-14 03:16:52,977] [INFO] [launch.py:351:main] Process 249 exits successfully.
[2024-11-14 03:16:54,979] [INFO] [launch.py:351:main] Process 248 exits successfully.
[2024-11-14 03:16:54,979] [INFO] [launch.py:351:main] Process 250 exits successfully.
[2024-11-14 03:16:54,979] [INFO] [launch.py:351:main] Process 246 exits successfully.
[2024-11-14 03:16:55,980] [INFO] [launch.py:351:main] Process 247 exits successfully.
[2024-11-14 03:16:56,982] [INFO] [launch.py:351:main] Process 245 exits successfully.
[2024-11-14 03:16:56,982] [INFO] [launch.py:351:main] Process 252 exits successfully.
